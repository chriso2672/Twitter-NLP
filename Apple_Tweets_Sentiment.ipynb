{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Sentiment Analysis\n",
    "---\n",
    "**Authors:** [Chris O'Malley](https://github.com/chriso2672), [Ted Brandon](https://github.com/theobigdog), [Kelsey Lane](https://github.com/kelsklane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "---\n",
    "This project uses data scrapped from [Twitter by CrowdFlower](https://data.world/crowdflower/brands-and-product-emotions) that contains positive, neutral, and negative sentiment towards Apple and Google products. These tweets were scrapped from various #sxsw hashtags on August 30, 2013. We aim to help Apple's product team get a better sense of how consumers reacted to their presentation to better understand the market and how they could improve their product. We did this by using sentiment analysis to gauge the reaction to their presentations and created a model that can assign sentiment to new tweets to help Apple understand how their products are recieved on the course to launch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Understanding\n",
    "---\n",
    "In this notebook we used only the tweets scrapped from the [CrowdFlower Twitter dataset](https://data.world/crowdflower/brands-and-product-emotions) that pretained to Apple, as that is the company we are targeting in our analysis. After loading in the data with pandas, that left us with 5,428 tweets and their predetermined sentiment. Using regex we cleaned the tweets of any odd characters and leftover artifacts from scrapping like links. After using the Tweet tokenizer from nltk we kept the text in uppercase and kept stopwords, as this combination resulted in the best fit for out model. Any non-relevant punctuation was then removed and the final tokens were lemmatized using the required libraries from nltk. From sklearn we used both a DecisionTreeClassifier as well as MultinomialNB and ComplementNB to model the data, as Bayes models work well with text data and a decision tree was another good simpler model that would give us a comparison. Furthermore, we felt opting for simpler model options and avoiding more computationally exhaustive ones like XGBoost would be a good call, as we have limited computation power and if this was to theoretically scale, a simpler model would be better. For the decision tree we tuned the depth of the tree, the minimum number of samples needed to split, and the criterion used. As for multinomial Bayes we compared CountVectorizer with the Tfidfvectorizer and adjusted the threshold for the max number of features vectorized, as well as the percentage cutoff for the maximum of the data and number of features returned for both vectorizers. Our final model that performed the best was the Complement Naive Bayes model with a Tfidfvectorizer that lopped off the top 50% most frequent words. Of the models it was the least overfit when checking with cross validation and still performed well. The final accurracy of the model on the testing set was 62%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELETE THIS: Just for reference for now\n",
    "The notebook should include a summary at the beginning that briefly and accurately describes your process. The summary should be approximately 250 words -- about the size of a research paper abstract.\n",
    "\n",
    "Summary elements:\n",
    "\n",
    "Business and data understanding:\n",
    "- what kind of data are you using, and what makes it well-suited for the business problem?\n",
    "- You do not need to include any data visualizations in your summary, but consider including relevant descriptive statistics\n",
    "\n",
    "Data preparation:\n",
    "- why did you choose the data preparation steps that you did, and what was the result?\n",
    "- This should be specific to the kind of data you are working with. For example, if you are doing an NLP project, what did you decide to do with stopwords?\n",
    "- Be sure to list the packages/libraries used to prepare the data, and why\n",
    "\n",
    "Modeling: \n",
    "- what modeling package(s) did you use, which model(s) within the package(s), and what tuning steps did you take?\n",
    "- For some projects there may be only one applicable package; you should still briefly explain why this was the appropriate choice\n",
    "\n",
    "Evaluation: \n",
    "- how well did your final model perform?\n",
    "- Include one or more relevant metrics\n",
    "- Be sure to briefly describe your validation approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buisness Understanding\n",
    "---\n",
    "Humans struggle with assigning sentiment to text, especially with more casual language like is used on Twitter. For example, [researchers found](https://arxiv.org/abs/1602.07563) that it is very rare for people to completly agree on the sentiment of a tweet, with annotaters agreeing with another about a tweet anywhere from 12 to 67% of the time. Therefore, sentiment can be something very hard to assign, but can be super helpful to companies for gauging their consumers reactions to new products. \n",
    "\n",
    "Therefore, we aim to help Apple's product team gain better insight into the consumer's reactions to their SXSW announcement to be able to get a better understanding of how their product was recieved and, going forward, people's reactions to their new announcements. As a result, they can use this information to understand how well recieved and new products are, as well as be able to target consumers who are neutral to their products to be able to convert them to buyers and positive points. As a result, Apple's product team can make better informed decisions about their products moving forward and can also adjust their marketing strategies to capture a wider audience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "---\n",
    "The dataset used for this project was scrapped by Crowdflower on August 30, 2013 and comes from data.world. Originally the dataset contained tweets pretaining to both Apple and Google's presentation at the 2013 SXSW conference and had 9,093 tweets alongside what sentiment a group of people assigned the tweets and towards what product the sentiment was directed. However, as our project is only interested in looking at reactions to Apple products, we narrowed the dataset down to only tweets the pretained to Apple. This leaves us with the subset of tweets that talk about Apple or Apple products somewhere in them. However, within the column that labels the sentiment of the tweet there are ones that are labeled \"I can't tell\" for the sentiment. As these are unknown and we can't gain any insight from them we opted to drop them. This leaves us with 5,428 tweets to use in the analysis. As the column that describes the target of the sentiment is 60% null values, we opted to drop it and replace it with `what_product`, which we will detail later on. Finally, for legibility we changed the column with the sentiment (`'is_there_an_emotion_directed_at_a_brand_or_product'`) to label to make it easier to read and mapped the targets to 0 for negative, 1 for positive, and 2 for neutral. Notably, there is a class imbalance where neutral tweets represent 53% of the data, while positive tweets account for about 40% of the data and negative tweets under 8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Read in the data\n",
    "tweets = pd.read_csv('data/clean_tweets.csv', encoding = 'iso-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis obviously has some limitations. For one, we sorted out the Google data in order to hone in on Apple, but with this we lose the ability to compare Apple's performance to their competitiors. As the competitions production could impact how Apple's is recieved, this is a nuance that is lost through this analysis. Furthermore, we are limited in the sentiment analysis we have. More granularity to the degree of negative or positive reception could help refine the model more, but that insight is lost given the levels we have. Similarly, the class imbalance of negative tweets makes it difficult to distinguish even with sampling, so this is another feature that limits our project. Finally, this analysis is only based on tweets, but product reception is also talked about on other platforms. Therefore, our analysis would benefit to widening the net to other platforms and building analyzers that work with these text patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "---\n",
    "As mentioned above, we got rid of the column that identifies the specific product the tweet is about due to the large number of nulls and general inaccuracy of the column, as with exploration the labels did not seem to match well with the tweet content. Below, we use the what_product function to replace this column with a new one. This assumes that the tweet does not refer to multiple products, which is possible, but from exploration does not seem to happen. The order the function checks for products is also used to try and filter for certain products before others that might occur in the same tweet. This felt like an important feature to include as certain products may have been recieved better while others recieved a stronger negative reaction. Therefore, this feature is included to account for how these differences may affect sentiment. While around 50% of the tweets seem to be about iPads, 20% are about Apple or the iPhone, while apps account for 10% of the tweets, with a very small percentage of tweets not containing any product-specific language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a string with the product\n",
    "def what_product(tweet):\n",
    "    tweet_check = tweet.lower()\n",
    "    #Checks for apps before the phone as there is likely a high co-occurrance\n",
    "    if ('app ' in tweet_check) or ('apps ' in tweet_check):\n",
    "        return 'App'\n",
    "    #Checks if tweet is about the iphone\n",
    "    if ('iphone' in tweet_check) or ('phone' in tweet_check):\n",
    "        return 'Phone'\n",
    "    #Checks if the tweet is about the ipad\n",
    "    if ('ipad' in tweet_check):\n",
    "        return 'iPad'\n",
    "    #Anything mentioning Apple returned as company\n",
    "    if ('apple' in tweet_check):\n",
    "        return 'Company'\n",
    "    #Any Apple related tweets that dont mention anything specific lumped to general\n",
    "    else:\n",
    "        return 'General'\n",
    "\n",
    "#Adds product column to the tweets\n",
    "tweets['what_product'] = tweets['tweet_text'].apply(what_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature added is one that keeps track of the ratio of capital letters to other characters in the tweet. This feature was included as the case of the letters is a parameter that gets tweaked during various model iterations, but the presence of capital letters could indicate the sentiment of a tweet. For example, someone whose particularly excited may tweet in all caps compared to someone whose more neutral. Therefore, this information is retained and amplified as its own feature, where the quantity of capital letters is computed as a ratio to the other characters in a tweet so that tweet length doesn't inadvertantly bias the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_letter_ratio(tweet):\n",
    "    #Initilizes count variable\n",
    "    capital_count = 0\n",
    "    #Checks each character in the tweet\n",
    "    for c in tweet:\n",
    "        #Increments count if capital letter is present\n",
    "        if c.isupper():\n",
    "            capital_count += 1\n",
    "    return capital_count / len(tweet)\n",
    "\n",
    "#Adds capital letter ratio column to the tweets\n",
    "tweets['capital_letter_ratio'] = tweets['tweet_text'].apply(capital_letter_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, both positive and negative tweets are liable to spam question marks and exclamation points to indicate strong emotion. As punctuation isn't clearly captured in the tokenization process with duplicate marks getting deleted, this function counts up any existing versions of these punctuation marks to retain this information. This is useful as a feature as it can help distinguish tweets with a positive or negative sentiment from a neutral one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exc_que_count(tweet):\n",
    "    #Punctuation to look for\n",
    "    punctuation = '!?'\n",
    "    #Initilizes count variable\n",
    "    count = 0\n",
    "    #Looks for any punctuation in the string and increments count accordingly\n",
    "    for p in punctuation:\n",
    "        count += tweet.count(p)\n",
    "    return count\n",
    "\n",
    "#Adds !? count column to the tweets\n",
    "tweets['exc_que_count'] = tweets['tweet_text'].apply(exc_que_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one tweet in the dataset with a null tweet value, so this lone row is dropped below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the tokenizer is called later during modeling, the function used to clean and process the tweets into tokens is included below first to walk through its execution. The tokenizer cleans up the text by removing any instances of links or the #sxsw hashtag, as the links don't convey any important meaning and these hashtag variations are what was used to scrape for the tweets, so they're common across all tweets. Next, any non-ASCII character is removed to clean up any non-words. The tweets are then tokenized using nltk's tweet tokenizer and these tokens are then cleaned of any unwanted puntuation or 'rt' \"words\" that are artifacts left by Twitter to indicate if a tweet retweets another and thus doesn't convey any information. Finally, the tokens are tagged with their part of speech and lemmatized, as tense and other affixes wouldn't contribute any extra information. Therefore, the tokens are stripped of these to help limit dimentionality. Notably, stopwords are not stripped from the text. Through testing different parameters, we found that including stopwords can actually increase model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to tokenize text\n",
    "import string\n",
    "\n",
    "#Replaces pos tags with lemmatize compatable tags\n",
    "def pos_replace(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "#Makes list of punctuation to exclude, keeps certain symbols\n",
    "punct = list(string.punctuation)\n",
    "keep_punct = ['#', '?', '!', '@']\n",
    "punct = [p for p in punct if p not in keep_punct]\n",
    "\n",
    "#Used to filter rt\n",
    "common_tweet_words = ['rt']\n",
    "\n",
    "#Removes non-ASCII characters\n",
    "def remove_junk(tweet):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in tweet])\n",
    "    \n",
    "def tweet_tokenizer(doc):\n",
    "    #Gets rid of links\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    doc = re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', '', doc)\n",
    "    #Gets rid of #sxsw hashtag variations\n",
    "    doc = re.sub(r'(?i)(#sxsw)\\w*', '', doc)\n",
    "    #Gets rid of conversions made during scrapping\n",
    "    doc = re.sub(r'{link}', '', doc)\n",
    "    doc = re.sub(r'\\[video\\]', '', doc)\n",
    "    #Gets rid of weird characters\n",
    "    doc = remove_junk(doc)\n",
    "    #Tokenizes using NLTK Twitter Tokenizer\n",
    "    tweet_token = TweetTokenizer(strip_handles = True)\n",
    "    doc = tweet_token.tokenize(doc)\n",
    "    #Gets rid of any tokens that represent if the tweet was retweeted\n",
    "    doc = [w for w in doc if w.lower() not in common_tweet_words]\n",
    "    #Gets rid  of any punctuation that we don't want to keep\n",
    "    doc = [w for w in doc if w not in punct]\n",
    "    #Lemmatizes tokens\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(w[0], pos_replace(w[1])) for w in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we opted to keep retweets in the dataset despite removing the 'rt' marker. While this may artifically increase certain words so they become overrepresented, we feel that the magnification of sentiment of these tweets was still important information. Of the dataset, 750 tweets included end up being retweeted by other people to account for 1,485 total retweets in the dataset. Therefore, while it may impact the representation of some words in the data, we think the amplified sentiment the retweets supply can help model performance more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Simple Model\n",
    "---\n",
    "The first simple model we looked at was a decision tree that only gets fed the tokenized tweets without any additional columns. This was just to get a sense of how a very simple model would work when not supplied with any additional information to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.4431343650208794\n",
      "Validation Score:0.44387000497429946\n"
     ]
    }
   ],
   "source": [
    "y = tweets['label']\n",
    "X = tweets['tweet_text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "baseline = imbpipeline(steps=[\n",
    "    ('preproc', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213, max_depth = 5))\n",
    "])\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "print(\"Training Score:\", baseline.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(baseline, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Bayes Models\n",
    "---\n",
    "The first model type we wanted to test out was Multinomial Bayes, as Bayes models tend to do well with text data and would likely show an improvement over a simple decision tree. As we are not doing binary classification, we opted for Multinomial Bayes and we started with the CountVectorizer as it wasn't clear if weighting the words across different tweets using TF_IDF would yield any additional help to the model. We also included the features created above in order to help improve model performance. Finally, as we have class imbalance affecting tweets with negative sentiment, we opted to SMOTE the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8290346352247605\n",
      "Validation Score:0.6236776653954569\n"
     ]
    }
   ],
   "source": [
    "#Creates features and target then performs train test split\n",
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "#Creates preprocessing step\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "#Creates pipeline for model\n",
    "apple_cv = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state = 213)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#Fits model and prints training score\n",
    "apple_cv.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_cv.score(X_train, y_train))\n",
    "#Cross validates model and prints average result\n",
    "scores = np.mean(cross_val_score(apple_cv, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above model improves on the first simple model, it overfits the data as there is a gap of .20 between training and validation scores. To see if we can bring this overfitting down, another Multinomaial Bayes model is run with TF_IDF as the vectorizer to see how that changes the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7379022353230165\n",
      "Validation Score:0.5662009918451636\n"
     ]
    }
   ],
   "source": [
    "#Creates preprocessing step for pipeline\n",
    "tfidf_vec = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "#Creates pipeline for model\n",
    "apple_tfidf = imbpipeline(steps=[\n",
    "    ('preproc', tfidf_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state = 213)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#Fits model and prints training and cross validation scores\n",
    "apple_tfidf.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_tfidf.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_tfidf, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the training score goes down, this model overfits less compared to the first, and therefore is the version we proceed with. In order to tune it and try and get an improvement on validation scores, we run this pipeline through a gridserach to try and figure out what parameters in the vectorizer could potentially help reduce the overfitting currently present. We test max features alongside max_df to see if limiting the more and less common words could improve model fit, as well as looking at the inclusion of bigrams to see if they add any additional information and testing case to see its influence on classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preproc__tfidf__lowercase': True,\n",
       " 'preproc__tfidf__max_df': 0.75,\n",
       " 'preproc__tfidf__max_features': 4000,\n",
       " 'preproc__tfidf__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__tfidf__max_features': [1000, 4000, 8000, None],\n",
    "    'preproc__tfidf__max_df': [.75, .85, .95],\n",
    "    'preproc__tfidf__lowercase': [False, True],\n",
    "    'preproc__tfidf__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(apple_tfidf, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like ignoring terms in the top 25% of frequency, using all lowercase words, and limiting the vectorizer to only 5,000 words while sticking with only unigrams outputs the best performance. While the adjusted model below shows only slight changes in the score, there is still a reduction in overfitting. The training score has also gone down again, but this is still an overall improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7160402849422747\n",
      "Validation Score:0.5689045989659486\n"
     ]
    }
   ],
   "source": [
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "tfidf_vec = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = True, tokenizer = tweet_tokenizer, \n",
    "                              max_df = .75, max_features = 4000), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "apple_tuned = imbpipeline(steps=[\n",
    "    ('preproc', tfidf_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state = 213)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "apple_tuned.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_tuned.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_tuned, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final Bayes model we ran was a Complement Naive Bayes model. This model is built to deal with imbalanced data, so since the negative sentiment tweets make up so few of our data points, we opted to try it out. We went through a similar process to the Multinomial model for tuning the results, and the final model from that is used below. This is ultimately our best model. While there is still a significant amount of overfitting, it is a comparable amount to the final Multinomial Bayes model. Therefore, as the accuracy scores are better this is the final model we decided to go with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7966101694915254\n",
      "Validation Score:0.6362077749807811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "tfidf_vec = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                             max_df = .5, max_features = 4000), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "final = imbpipeline(steps=[\n",
    "    ('preproc', tfidf_vec),\n",
    "    ('mnb', ComplementNB())\n",
    "])\n",
    "\n",
    "final.fit(X_train, y_train)\n",
    "print(\"Training Score:\", final.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(final, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we thought Multinomial Bayes would give us the best result, we also wanted to test out a different classifier to see if we could get any improvement in score. While a simpler decision tree was run above with only tweets, we ran another one with the added features and no hyperparameters to see what the change in score would be. As the TfidfVectorizer performed better with Bayes above, we opted to keep it as the vectorizer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9992630803242447\n",
      "Validation Score:0.5725846761429584\n"
     ]
    }
   ],
   "source": [
    "#Establishes preprocessing for the decision tree\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "#Creates decision tree pipeline\n",
    "apple_dt = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state = 213)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213))\n",
    "])\n",
    "\n",
    "#Fits the new model on the data and prints the score for training and cross validation\n",
    "apple_dt.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_dt.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_dt, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fantastic training score, the tree is clearly overfitting. In order to tune the tree and hopefully reduce the overfitting, we ran another gridsearch for the decision tree to tune the hypterparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtc__criterion': 'entropy',\n",
       " 'dtc__max_depth': 30,\n",
       " 'dtc__min_samples_split': 2}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'dtc__max_depth': [None, 20, 30],\n",
    "    'dtc__criterion': ['gini', 'entropy'],\n",
    "    'dtc__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(apple_dt, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tuned parameters were then fed into the model below. While the overfitting does go down, it's still pretty substantial. Therefore, the final model we opted to go with was the tuned Complement Naive Bayes one above, as it had the least amount of overfitting while still retaining a decent amount of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8658806190125277\n",
      "Validation Score:0.5779915889118343\n"
     ]
    }
   ],
   "source": [
    "#Adjusts the model in the pipeline to include new parameters\n",
    "apple_dt_tuned = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state = 213)),\n",
    "    ('dtc', DecisionTreeClassifier(max_depth = 30, random_state = 213))\n",
    "])\n",
    "\n",
    "#Fits the new model and prints out training and validation scores\n",
    "apple_dt_tuned.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_dt_tuned.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_dt_tuned, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "---\n",
    "As mentioned earlier, the final model we went with was the tuned Complement Naive Bayes model that used the Tfidf vectorizer. We opted for this model over the others as it had the highest accuracy score while maintaining the least amount of differente between training and validation sets, indicating it was the model that overfit the least. The final score of the model based on the test data is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6219602063375093"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the final accuracy score of the model is 62%. As for why we have been using accuracy as the metric, it's because in terms of false positives and false negatives for sentiment there's not any one we want to avoid more. As we are trying to get a gauge of customer interest across three different sentiment types, we did not want to avoid one misclassification over the other, as we do not care more for any one particular sentiment. Therefore, we opted for accuracy as our metric to find the model that would perform the best overall. For curiosities sake, the confusion matric for the model is printed below just to see its overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "y_hat = final.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "cmdis = ConfusionMatrixDisplay(cm)\n",
    "cmdis.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the final model's accuracy has gone up from around 45% to 63%, there is still improvements to be made and also a bit of overfitting still present in the model. As seen in the confusion matrix, the model still has a hard time identifying positive and negative sentiment as non-neutral. Therefore, while the current model may be faster than having someone determine the sentiment of a tweet on their own, there is still more work to be done before the model could be deployed and prove truly useful to Apple's product team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---\n",
    "Overall though, we'd recommend using the model as it still is fairly accurrate in distinguishing tweet sentiment, especially when comapred to doing so by hand. As a result, Apple's product team can use this sentiment analyzer to target neutral consumers and convert them to buyers. While the model improved from the first pass, it still doesn't distinguish non-neutral tweets from neutral ones very well. Therefore, this model might not fully solve the business problem. Furthermore, this sentiment analysis only looked at tweets, but consumers likely post about Apple on other platforms as well. That's why, going forward, it would be interesting to try and implement sentiment analysis for different platforms to try and gauge consumers on other platforms. On top of that, making the sentiment labels themselves more fine grained would provide more insight into how strongly consumers feel, so incorporating that into the project in the future may yield better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
