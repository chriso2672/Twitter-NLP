{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Sentiment Analysis\n",
    "---\n",
    "**Authors:** [Chris O'Malley](https://github.com/chriso2672), [Ted Brandon](https://github.com/theobigdog), [Kelsey Lane](https://github.com/kelsklane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "---\n",
    "This project uses data scrapped from [Twitter by CrowdFlower](https://data.world/crowdflower/brands-and-product-emotions) that contains positive, neutral, and negative sentiment towards Apple and Google products. These tweets were scrapped from various #sxsw hashtags on August 30, 2013. We aim to help Apple's product team get a better sense of how consumer's reacted to their presentation to better understand the market and how they could improve. We did this by using sentiment analysis to gage the reaction to their presentation and created a model **results and recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Understanding\n",
    "---\n",
    "In this notebook we used only the tweets scrapped from the [CrowdFlower Twitter dataset](https://data.world/crowdflower/brands-and-product-emotions) that pretained to Apple, as that is the company we are targeting in our analysis. After loading in the data with pandas, that left us with 5,428 tweets and their predetermined sentiment. We cleaned the tweets of any odd characters and leftover artifacts from scrapping like links using regex. After using the Tweet tokenizer from nltk, we kept the text in uppercase and kept stopwords, as this resulted in the best fit for out model. Any non-relevant punctuation was then removed and the final tokens were lemmatized using the required libraries from nltk. From sklearn we used both a DecisionTreeClassifier and MultinomialNB to model the three different levels, as MultinomialNB works well with text data and a decision tree was another good simpler model that would give us a comparison. Furthermore, we felt opting for simpler model options and avoiding ones like different ensembles was a good call, as we have limited computation power and if this was to theoretically scale, a simpler model would take up less computational power. For the decision tree we tuned the depth of the tree, the minimum number of samples needed to split, and the criterion used. As for multinomial Bayes we compared CountVectorizer with the tfidfvectorizer and adjusted the threshold for the max number of features vectorized, as well as the percentage cutoff for the maximum and minimum of the data for both. Our final model that performed the best was the Multinomial Bayes with a tfidfvectorizer that lopped off the top 20% of the data. Of the models it was the least overfit when checking with cross validation and still performed well. The final accurracy of the testing set was 62%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook should include a summary at the beginning that briefly and accurately describes your process. The summary should be approximately 250 words -- about the size of a research paper abstract.\n",
    "\n",
    "Summary elements:\n",
    "\n",
    "Business and data understanding:\n",
    "- what kind of data are you using, and what makes it well-suited for the business problem?\n",
    "- You do not need to include any data visualizations in your summary, but consider including relevant descriptive statistics\n",
    "\n",
    "Data preparation:\n",
    "- why did you choose the data preparation steps that you did, and what was the result?\n",
    "- This should be specific to the kind of data you are working with. For example, if you are doing an NLP project, what did you decide to do with stopwords?\n",
    "- Be sure to list the packages/libraries used to prepare the data, and why\n",
    "\n",
    "Modeling: \n",
    "- what modeling package(s) did you use, which model(s) within the package(s), and what tuning steps did you take?\n",
    "- For some projects there may be only one applicable package; you should still briefly explain why this was the appropriate choice\n",
    "\n",
    "Evaluation: \n",
    "- how well did your final model perform?\n",
    "- Include one or more relevant metrics\n",
    "- Be sure to briefly describe your validation approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buisness Understanding\n",
    "---\n",
    "Apple product team as stakeholder. Want to gain insight into consumer reaction to announcement at SXSW conference and provide further action. Going forward, use sentiment analysis to gauge consumer reaction to new products on Twitter to help identify percentage of people that recieve news well vs. those that dislike it. Furthermore, model can pick up on neutral consumers to be able to hone in on them and convert them to buyers/positive in the future. Implication is that able to improve their marketing strategy and product based on real-time feedback from tweets to make a better product.\n",
    "\n",
    "- Explain real world problem\n",
    "- Identify stakeholder and how use\n",
    "- Implication of project for real-wolrd problem and stakeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "---\n",
    "The data used for this project was scrapped by Crowdflower on August 30, 2013 and comes from data.world. Originally the dataset contained tweets pretaining to both Apple and Google's presentation at the 2013 SXSW conference and had 9,093 tweets alongside their human-judged sentiment and towards what product the sentiment was directed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Read in the data\n",
    "df = pd.read_csv('data/tweets.csv', encoding = 'iso-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as our project is only interested in looking at reactions to Apple products, we narrowed the dataset down to only tweets the pretained to Apple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabs any tweets already labeled with Apple products/company\n",
    "ipad = df[df['emotion_in_tweet_is_directed_at'] == 'iPad']\n",
    "apple = df[df['emotion_in_tweet_is_directed_at'] == 'Apple']\n",
    "mix = df[df['emotion_in_tweet_is_directed_at'] == 'iPad or iPhone App']\n",
    "iphone = df[df['emotion_in_tweet_is_directed_at'] == 'iPhone']\n",
    "apps = df[df['emotion_in_tweet_is_directed_at'] == 'Other Apple product or service']\n",
    "\n",
    "#Grabs tweets with no company label\n",
    "no_labels = df[df['emotion_in_tweet_is_directed_at'].isna()]\n",
    "no_labels = no_labels.dropna(subset = ['tweet_text'])\n",
    "\n",
    "#Checks non-labeled tweets for mentions of apple products\n",
    "tweet_list = []\n",
    "for tweet in no_labels['tweet_text']:\n",
    "    #Lowers the tweet text and checks for relevant product names\n",
    "    tweet_check = tweet.lower()\n",
    "    if ('iphone' in tweet_check) or ('ipad' in tweet_check) or ('apple' in tweet_check):\n",
    "        tweet_list.append(tweet)\n",
    "        \n",
    "#Grabs the actual rows with Apple tweets, drops duplicates and merges to one datset\n",
    "unlabeled_apple = df[df['tweet_text'].isin(tweet_list)]\n",
    "unlabeled_apple = unlabeled_apple.drop_duplicates(subset = 'tweet_text')\n",
    "final_df = pd.concat([ipad, apple, mix, iphone, apps, unlabeled_apple], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with the subset of tweets that talk about Apple or Apple products somewhere in them. However, within the column that labels the sentiment of the tweet there are ones that are labeled as \"I can't tell\" for the sentiment. As these are unknown and we can't gain any insight from them we opted to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filters out any rows where the sentiment is unknown\n",
    "tweets = final_df[final_df['is_there_an_emotion_directed_at_a_brand_or_product'] != 'I can\\'t tell']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with 5,428 tweets to use in the analysis. As the column that describes the target of the sentiment is 60% null values, we opted to drop it and replace it with a more refined feature later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops column\n",
    "tweets = tweets.drop('emotion_in_tweet_is_directed_at', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for legibility we changed the column with the sentiment ('is_there_an_emotion_directed_at_a_brand_or_product') to label to make it easier to read and mapped the targets to 0 for negative, 1 for positive, and 2 for neutral. Noteably, there is a class imbalance where neutral tweets represent 50% of the data, while positive tweets account for about 40% of the data and negative tweets under 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves values to new columm and drops the old one\n",
    "tweets['label'] = tweets['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "tweets = tweets.drop('is_there_an_emotion_directed_at_a_brand_or_product', axis = 1)\n",
    "\n",
    "#Reassigns the sentiment labels to numbers\n",
    "tweets.label = tweets.label.map({'Negative emotion' : 0, 'Positive emotion': 1, \n",
    "                                 'No emotion toward brand or product': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis obviously has some limitations. For one, we sorted out the Google data in order to hone in on Apple, but with this we lose the ability to compare Apple's performance to their competitiors. As the competitions production could impact how Apple's is recieved, this is a nuance that is lost through this analysis. Furthermore, we are limited in the sentiment analysis we have. More granularity to the degree of negative or positive reception could help refine the model more, but that insight is lost given the levels we have. Finally, this analysis is only based on tweets, but product reception is also talked about on other platforms. Therefore, our analysis would benefit to widening the net to other platforms and building analyzers that work with these text patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "---\n",
    "As mentioned above, we got rid of the column that identifies the specific product the tweet is about due to the large number of nulls and general inaccuracy of the column, as with exploration the labels did not seem to match well with the tweet content. Below, we use the what_product function to replace this column with a new one. This assumes that the tweet does not refer to multiple products, which is possible but from exploration does not seem to happen. The order is also used to try and filter for certain products before others that might occur in the same tweet. This felt like an important feature to include as certain products may have been recieved better while others recieved a stronger negative reaction. Therefore, this feature is included to account for how these differences may affect sentiment. While around 50% of the tweets seem to be about the iPad, 20% are about Apple or the iPhone, while apps account for 10% of the tweets, with a very small percentage of tweets not containing any product-specific language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a string with the product\n",
    "def what_product(tweet):\n",
    "    tweet_check = tweet.lower()\n",
    "    #Checks for apps before the phone as there is likely a high co-occurrance\n",
    "    if ('app ' in tweet_check) or ('apps ' in tweet_check):\n",
    "        return 'App'\n",
    "    #Checks if tweet is about the iphone\n",
    "    if ('iphone' in tweet_check) or ('phone' in tweet_check):\n",
    "        return 'Phone'\n",
    "    #Checks if the tweet is about the ipad\n",
    "    if ('ipad' in tweet_check):\n",
    "        return 'iPad'\n",
    "    #Anything mentioning Apple returned as company\n",
    "    if ('apple' in tweet_check):\n",
    "        return 'Company'\n",
    "    #Any Apple related tweets that dont mention anything specific lumped to general\n",
    "    else:\n",
    "        return 'General'\n",
    "\n",
    "#Adds product column to the tweets\n",
    "tweets['what_product'] = tweets['tweet_text'].apply(what_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature added is one that keeps track of the ratio of capital letters to other characters in the tweet. This feature was included as the case of the letters is a parameter that gets tweaked during various model iterations, but the presence of capital letters could indicate the sentiment of a tweet. For example, someone whose particularly excited may tweet in all caps compared to someone whose more neutral. Therefore, this information is retained and highlighted as its own feature, where the quantity of capital letters is computed as a ratio to the other characters in a tweet so that tweet length doesn't inadvertantly bias the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_letter_ratio(tweet):\n",
    "    #Initilizes count variable\n",
    "    capital_count = 0\n",
    "    #Checks each character in the tweet\n",
    "    for c in tweet:\n",
    "        #Increments count if capital letter is present\n",
    "        if c.isupper():\n",
    "            capital_count += 1\n",
    "    return capital_count / len(tweet)\n",
    "\n",
    "#Adds capital letter ratio column to the tweets\n",
    "tweets['capital_letter_ratio'] = tweets['tweet_text'].apply(capital_letter_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, both positive and negative tweets are liable to spam question marks and exclamation points to indicate strong emotion. As punctuation isn't clearly captured in the tokenization process with duplicate marks getting deleted, this function counts up any existing versions of these punctuation marks to retain this information. This is useful as a featuer as it can help distinguish tweets with a positive or negative sentiment from a neutral one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exc_que_count(tweet):\n",
    "    #Punctuation to look for\n",
    "    punctuation = '!?'\n",
    "    #Initilizes count variable\n",
    "    count = 0\n",
    "    #Looks for any punctuation in the string and increments count accordingly\n",
    "    for p in punctuation:\n",
    "        count += tweet.count(p)\n",
    "    return count\n",
    "\n",
    "#Adds !? count column to the tweets\n",
    "tweets['capital_letter_ratio'] = tweets['tweet_text'].apply(capital_letter_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one tweet in the dataset with a null tweet value, so this lone row is dropped below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the tokenizer is called later during modeling, the function used to clean and process the tweets into tokens is included below first to walk through its execution. The tokenizer cleans up the text by removing any instances of links or the #sxsw hashtag, as the links don't convey any important meaning and these hashtag variations are what was used to scrape for the tweets, so they're common across all tweets. Next, any non-ASCII character is removed to clean up any non-words. The tweets are then tokenized using nltk's tweet tokenizer and these tokens are then cleaned of any unwanted puntuation or 'rt' \"words\" that are artifacts left by Twitter to indicate if a tweet retweets another and thus doesn't convey any information. Finally, the tokens are tagged with their part of speech and lemmatized, as tense and other affixes wouldn't contribute any extra information so the tokens are stripped of these to help limit dimentionality. Noteably, stopwords are not stripped from the text. Through testing different parameters, we found that including stopwords can actually increase model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to tokenize text\n",
    "import string\n",
    "\n",
    "#Replaces pos tags with lemmatize compatable tags\n",
    "def pos_replace(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "#Makes list of punctuation to exclude, keeps certain symbols\n",
    "punct = list(string.punctuation)\n",
    "keep_punct = ['#', '?', '!', '@']\n",
    "punct = [p for p in punct if p not in keep_punct]\n",
    "\n",
    "#Used to filter rt\n",
    "common_tweet_words = ['rt']\n",
    "\n",
    "#Removes non-ASCII characters\n",
    "def remove_junk(tweet):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in tweet])\n",
    "    \n",
    "def tweet_tokenizer(doc):\n",
    "    #Gets rid of links\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    doc = re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', '', doc)\n",
    "    #Gets rid of #sxsw hashtag variations\n",
    "    doc = re.sub(r'(?i)(#sxsw)\\w*', '', doc)\n",
    "    #Gets rid of conversions made during scrapping\n",
    "    doc = re.sub(r'{link}', '', doc)\n",
    "    doc = re.sub(r'\\[video\\]', '', doc)\n",
    "    #Gets rid of weird characters\n",
    "    doc = remove_junk(doc)\n",
    "    #Tokenizes using NLTK Twitter Tokenizer\n",
    "    tweet_token = TweetTokenizer(strip_handles = True)\n",
    "    doc = tweet_token.tokenize(doc)\n",
    "    #Gets rid of any tokens that represent if the tweet was retweeted\n",
    "    doc = [w for w in doc if w.lower() not in common_tweet_words]\n",
    "    #Gets rid  of any punctuation that we don't want to keep\n",
    "    doc = [w for w in doc if w not in punct]\n",
    "    #Lemmatizes tokens\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(w[0], pos_replace(w[1])) for w in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we opted to keep retweets in the dataset. While this may artifically increase certain words so they become overrepresented, we feel that the magnification of sentiment of these tweets was still important information. **Add in information here about %RTs vs. non-RT tweets**. Therefore, while it may impact the representation of some words in the data, the amplified sentiment the retweets supply may help model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Simple Model\n",
    "---\n",
    "The first simple model we looked at was a decision tree that only gets fed the tokenized tweets without any additional columns. This was just to get a sense of how a very simple model would work when not supplied with any additional information to establish a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.4591009579955785\n",
      "Validation Score:0.43404531134592494\n"
     ]
    }
   ],
   "source": [
    "y = tweets['label']\n",
    "X = tweets['tweet_text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "baseline = imbpipeline(steps=[\n",
    "    ('preproc', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213, max_depth = 5))\n",
    "])\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "print(\"Training Score:\", baseline.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(baseline, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Bayes Models\n",
    "---\n",
    "The first model type we wanted to test out was Multinomial Bayes, as Bayes models tend to do well with text classification and would likely show an improvement over a simple decision tree. As we are not doing binary classification, we opted for Multinomial Bayes and we started with the CountVectorizer as it wasn't clear if weighting the words across different tweets using TF_IDF would yield any additional help to the model. We also included the features created above in order to help improve model performance. Finally, as Multinomial Bayes accounts for class probabilities, we opted not to SMOTE the minority classes, as even in iterations of the model that included SMOTE it seemed to decrease model performance rather than improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8403340702530091\n",
      "Validation Score:0.6190105666179286\n"
     ]
    }
   ],
   "source": [
    "#Creates features and target then performs train test split\n",
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "#Creates preprocessing step\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "#Creates pipeline for model\n",
    "apple_cv = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#Fits model and prints training score\n",
    "apple_cv.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_cv.score(X_train, y_train))\n",
    "#Cross validates model and prints average result\n",
    "scores = np.mean(cross_val_score(apple_cv, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above model improves on the first simple model, it overfits the data as there is a gap of almost .22 between training and validation scores. To see if we can bring this overfitting down, another Multinomaial Bayes model is run with TF_IDF as the vectorizer to see if this can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7523949889462048\n",
      "Validation Score:0.6192598845359581\n"
     ]
    }
   ],
   "source": [
    "#Creates preprocessing step for pipeline\n",
    "tfidf_vec = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "#Creates pipeline for model\n",
    "apple_tfidf = imbpipeline(steps=[\n",
    "    ('preproc', tfidf_vec),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#Fits model and prints training and cross validation scores\n",
    "apple_tfidf.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_tfidf.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_tfidf, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the training score goes down, this model overfits less compared to the first, and therefore is the version we proceed with. In order to tune it and try and get an improvement on validation scores, we run this pipeline through a gridserach to try and figure out what parameters in the vectorizer could potentially help reduce the overfitting present currently. We test max features alongside max_df to see if limiting the more and less common words could improve model fit, as well as looking at the inclusion of bigrams to see if they add any additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preproc__tfidf__max_df': 0.75,\n",
       " 'preproc__tfidf__max_features': 2000,\n",
       " 'preproc__tfidf__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__tfidf__max_features': [1000, 2000, 4000, 8000],\n",
    "    'preproc__tfidf__max_df': [.75, .85, .95],\n",
    "    'preproc__tfidf__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(apple_tfidf, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like ignoring terms in the top 25% of frequency and limiting the vectorizer to only 2,000 words while sticking with only unigrams outputs the best performance. While the adjusted model below shows only slight changes in the score, there is still a reduction in overfitting. The training score has also gone down again, but this is the best model so far and is the best bet in terms of being able to accurately predict with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7273397199705232\n",
      "Validation Score:0.6327721921586952\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer, \n",
    "                              max_df = .75, max_features = 2000), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "apple_tuned = imbpipeline(steps=[\n",
    "    ('preproc', tfidf_vec),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "apple_tuned.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_tuned.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_tuned, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we thought Multinomial Bayes would give us the best result, we also wanted to test out a different classifier to see if we could get any improvement in score. While a simpler decision tree was run above with only tweets, we ran another one with the added features and no hyperparameters to see what the change in score would be. As the TfidfVectorizer performed better with Bayes above, we opted to keep it as the vectorizer here. Furthermore, as decision trees don't account for class imbalance unlike Bayes, we oversampled the minority class, negative tweets, using SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9992630803242447\n",
      "Validation Score:0.5725846761429584\n"
     ]
    }
   ],
   "source": [
    "#Establishes preprocessing for the decision tree\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "#Creates decision tree pipeline\n",
    "apple_dt = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state = 213)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213))\n",
    "])\n",
    "\n",
    "#Fits the new model on the data and prints the score for training and cross validation\n",
    "apple_dt.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_dt.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_dt, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fantastic training score, the tree is clearly overfitting. In order to tune the tree and hopefully reduce the overfitting, we ran another gridsearch for the decision tree to tune the hypterparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtc__criterion': 'entropy',\n",
       " 'dtc__max_depth': 30,\n",
       " 'dtc__min_samples_split': 2}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'dtc__max_depth': [None, 20, 30],\n",
    "    'dtc__criterion': ['gini', 'entropy'],\n",
    "    'dtc__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(apple_dt, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tuned parameters were then fed into the model below. While the overfitting does go down, it's still pretty substantial. Therefore, the final model we opted to go with was the tuned Multinomal Bayes on above, as it had the least amount of overfitting while still retaining a decent amount of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8658806190125277\n",
      "Validation Score:0.5779915889118343\n"
     ]
    }
   ],
   "source": [
    "#Adjusts the model in the pipeline to include new parameters\n",
    "apple_dt_tuned = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state = 213)),\n",
    "    ('dtc', DecisionTreeClassifier(max_depth = 30, random_state = 213))\n",
    "])\n",
    "\n",
    "#Fits the new model and prints out training and validation scores\n",
    "apple_dt_tuned.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_dt_tuned.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_dt_tuned, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "---\n",
    "As mentioned earlier, the final model we went with was the tuned Multinomial Bayes model that used the Tfidf vectorizer. We opted for this model over the others as it had the highest accuracy score while maintaining the least amount of differente between training and validation sets, indicating it was the model that overfit the least. The final score of the model based on the test data is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6330140014738393"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_tuned.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the final accuracy score of the model is 63%. As for why we have been using accuracy as the metric, it's because in terms of false positives and false negatives for sentiment there's not any one we want to avoid more. As we are trying to get a gauge of customer interest across three different sentiment types, we did not want to avoid one misclassification over the other, as we do not care more for any one particular sentiment. Therefore, we opted for accuracy as our metric to find the model that would perform the best overall. For curiosities sake, the confusion matric for the model is printed below just to see its overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "y_hat = apple_tuned.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "cmdis = ConfusionMatrixDisplay(cm)\n",
    "cmdis.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the final model's accuracy has gone up from around 45% to 63%, there is still improvements to be made and also a bit of overfitting still present in the model. As seen in the confusion matrix, the model still has a hard time identifying positive and negative sentiment as non-neutral. Therefore, while the current model may be faster than having someone determine the sentiment of a tweet on their own, there is still more work to be done before the model could be deployed and prove truly useful to Apple's product team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---\n",
    "**Add recommendations based on model.** While the model improved from the first pass, it still doesn't distinguish non-neutral tweets from neutral ones very well. Therefore, this model might not fully solve the business problem. Furthermore, this sentiment analysis only looked at tweets, but consumers likely post about Apple on other platforms as well. That's why, going forward, it would be interesting to try and implement sentiment analysis for different platforms to try and gauge consumers on other platforms. Furthermore, making the sentiment labels themselves more fine grained would provide more insight into how strongly consumers feel, so incorporating that into the project in the future may yield better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
