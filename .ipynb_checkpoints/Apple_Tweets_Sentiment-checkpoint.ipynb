{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/twitter_bg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Sentiment Analysis\n",
    "---\n",
    "**Authors:** [Chris O'Malley](https://github.com/chriso2672), [Ted Brandon](https://github.com/theobigdog), [Kelsey Lane](https://github.com/kelsklane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "---\n",
    "This project uses data scraped from [Twitter by CrowdFlower](https://data.world/crowdflower/brands-and-product-emotions) that contains positive, neutral, and negative sentiment towards Apple and Google products. These tweets were scraped from various #sxsw hashtags on August 30, 2013. We aim to help Apple's product team get a better sense of how consumers reacted to their presentation, to better understand the market and how they could improve their product. We did this by using sentiment analysis to gauge the reaction to their presentations and created a model that can assign sentiment to new tweets to help Apple understand how their products are received on the course to launch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Understanding\n",
    "---\n",
    "In this notebook, we used only the tweets scraped from the [CrowdFlower Twitter dataset](https://data.world/crowdflower/brands-and-product-emotions) that pertained to Apple, as that is the company we are targeting in our analysis. After loading in the data with pandas, that left us with 5,428 tweets and their predetermined sentiment. Using regex, we cleaned the tweets of any odd characters and leftover artifacts from scraping, such as links. After using the Tweet tokenizer from nltk, we retained any uppercase as well as stopwords, as this combination resulted in the best fit for out model. Any non-relevant punctuation was then removed and the final tokens were lemmatized using the required libraries from nltk. From sklearn, we used a DecisionTreeClassifier as well as MultinomialNB and ComplementNB to model the data. Bayes models work well with text data and a decision tree was another good, simpler model that would give us a comparison. Furthermore, we felt simpler model options as well as avoiding more computationally exhaustive ones like XGBoost, would be a good call, as we have limited computational power. If this were to theoretically scale, a simpler model would be better. For the decision tree, we tuned the depth of the tree, the minimum number of samples needed to split, and the criterion used. As for multinomial Bayes, we compared CountVectorizer with the TfidfVectorizer and adjusted the threshold for the max number of features vectorized, as well as the percentage cutoff for the maximum of the data and number of features returned for both vectorizers. Our final model was the Complement Naive Bayes model with a TfidfVectorizer that lopped off the top 50% most frequent words. Of the models, it was the least overfit, when cross validation was performed. The final accuracy of the model on the testing set was 62%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "---\n",
    "Humans struggle with assigning sentiment to text, especially with more casual language such as what is used on Twitter. For example, [researchers found](https://arxiv.org/abs/1602.07563) that it is very rare for people to completely agree on the sentiment of a tweet, with annotaters agreeing with each other about a tweet anywhere from 12 to 67% of the time. Therefore, sentiment can be something very hard to assign, but can be helpful to companies for gauging their consumers' reactions to new products. \n",
    "\n",
    "With this in mind, we aim to help Apple's product team gain better insight into the consumers' reactions to their SXSW announcement, to be able to get a better understanding of how their product was received and, going forward, people's reactions to their new announcements. As a result, they can use this information to understand how well-received and new products are, as well as be able to target consumers who are neutral to their products, in order to be able to convert them to buyers and positive points. As a result, Apple's product team can make better, informed decisions about their products moving forward and can also adjust their marketing strategies to capture a wider audience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "---\n",
    "The dataset used for this project was scraped by Crowdflower on August 30, 2013 and comes from data.world. The original dataset contained 9,093 tweets pertaining to both Apple and Google's presentations at the 2013 SXSW conference. Alongside the tweets  came sentiment of a group of people assigned the tweets, towards what product the sentiment was directed. However, as our project is only interested in looking at reactions to Apple products, we narrowed the dataset down to only tweets that pertained to Apple. This leaves us with the subset of tweets that talk about Apple or Apple products somewhere in them. Within the column that labels the sentiment of the tweet are included labels of, \"I can't tell\". As these are unknown and we can't gain any insight from them, we opted to drop any entries that included this label. This left us with 5,428 tweets to use in the analysis. As the column that describes the target of the sentiment is 60% null values, we opted to drop it and replace it with `what_product`, of which we will detail later. Finally, for legibility we changed the column with the sentiment `'is_there_an_emotion_directed_at_a_brand_or_product'` to make it easier to read.  Further, we mapped the targets to 0 for negative, 1 for positive, and 2 for neutral. Notably, there is a class imbalance where neutral tweets represent 53% of the data, while positive tweets account for about 40% of the data and negative tweets just under 8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Read in the data\n",
    "tweets = pd.read_csv('data/clean_tweets.csv', encoding = 'iso-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis obviously has some limitations. For one, we sorted out the Google data in order to hone in on Apple, but with this we lose the ability to compare Apple's performance to their competitors. As the competition's production could impact how Apple's was received, this is a nuance that is lost through this analysis. Furthermore, we are limited in the sentiment analysis we have. More granularity to the degree of negative or positive reception could help refine the model more, but that insight is lost given the levels we have. Similarly, the class imbalance of negative tweets makes it difficult to distinguish even with sampling, so this is another feature that limits our project. Finally, this analysis is only based on tweets, but product reception is also talked about on other platforms. Therefore, our analysis would benefit to widening the net to other platforms and building analyzers that work with these text patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "---\n",
    "As mentioned above, we got rid of the column that identifies the specific product the tweet is about, due to the large number of nulls and general inaccuracy of the column, as with exploration the labels did not seem to match well with the tweet content. Below, we use the `what_product` function to replace this column with a new one. This assumes that the tweet does not refer to multiple products, which is possible, but from exploration does not seem to happen. The order the function checks for products is also used to try and filter for certain products before others that might occur in the same tweet. This was an important feature to include as certain products may have been received better while others received a stronger negative reaction. Therefore, this feature is included to account for how these differences may affect sentiment. Around 50% of the tweets seem to be about iPads, 20% are about Apple or the iPhone, while apps account for 10% of the tweets, with a very small percentage of tweets not containing any product-specific language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a string with the product\n",
    "def what_product(tweet):\n",
    "    tweet_check = tweet.lower()\n",
    "    #Checks for apps before the phone as there is likely a high co-occurrance\n",
    "    if ('app ' in tweet_check) or ('apps ' in tweet_check):\n",
    "        return 'App'\n",
    "    #Checks if tweet is about the iphone\n",
    "    if ('iphone' in tweet_check) or ('phone' in tweet_check):\n",
    "        return 'Phone'\n",
    "    #Checks if the tweet is about the ipad\n",
    "    if ('ipad' in tweet_check):\n",
    "        return 'iPad'\n",
    "    #Anything mentioning Apple returned as company\n",
    "    if ('apple' in tweet_check):\n",
    "        return 'Company'\n",
    "    #Any Apple related tweets that dont mention anything specific lumped to general\n",
    "    else:\n",
    "        return 'General'\n",
    "\n",
    "#Adds product column to the tweets\n",
    "tweets['what_product'] = tweets['tweet_text'].apply(what_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature added is one that keeps track of the ratio of capital letters to other characters in the tweet. This feature was included as the case of the letters is a parameter that gets tweaked during various model iterations, but the presence of capital letters could indicate the sentiment of a tweet. For example, someone who is particularly excited may tweet in all caps compared to someone who is more neutral. Therefore, this information is retained and amplified as its own feature, where the quantity of capital letters is computed as a ratio to the other characters in a tweet, so the tweet length doesn't inadvertantly bias the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_letter_ratio(tweet):\n",
    "    #Initilizes count variable\n",
    "    capital_count = 0\n",
    "    #Checks each character in the tweet\n",
    "    for c in tweet:\n",
    "        #Increments count if capital letter is present\n",
    "        if c.isupper():\n",
    "            capital_count += 1\n",
    "    return capital_count / len(tweet)\n",
    "\n",
    "#Adds capital letter ratio column to the tweets\n",
    "tweets['capital_letter_ratio'] = tweets['tweet_text'].apply(capital_letter_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, both positive and negative tweets are liable to spam question marks and exclamation points to indicate strong emotion. As punctuation isn't clearly captured in the tokenization process, with duplicate marks getting deleted, this function counts up any existing versions of these punctuation marks to retain this information. This is useful as a feature as it can help distinguish tweets with a positive or negative sentiment from a neutral one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exc_que_count(tweet):\n",
    "    #Punctuation to look for\n",
    "    punctuation = '!?'\n",
    "    #Initilizes count variable\n",
    "    count = 0\n",
    "    #Looks for any punctuation in the string and increments count accordingly\n",
    "    for p in punctuation:\n",
    "        count += tweet.count(p)\n",
    "    return count\n",
    "\n",
    "#Adds !? count column to the tweets\n",
    "tweets['exc_que_count'] = tweets['tweet_text'].apply(exc_que_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one tweet in the dataset with a null tweet value, so this lone row is dropped below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the tokenizer is called later during modeling, the function used to clean and process the tweets into tokens is included below first, to walk through its execution. The tokenizer cleans up the text by removing any instances of links or the #sxsw hashtag, as the links don't convey any important meaning and these hashtag variations are what was used to scrape for the tweets, so they're common across all tweets. Next, any non-ASCII characters are removed to clean up any non-words. The tweets are then tokenized using nltk's tweet tokenizer and these tokens are then cleaned of any unwanted punctuation or 'rt' (an artifact left by Twitter to indicate if a tweet retweets another and thus doesn't convey any new information). Finally, the tokens are tagged with their part of speech and lemmatized, as tense and other affixes wouldn't contribute any extra information. Therefore, the tokens are stripped of these to help limit dimensionality. Notably, stopwords are not stripped from the text. Through testing different parameters, we found that including stopwords can actually increase model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to tokenize text\n",
    "import string\n",
    "\n",
    "#Replaces pos tags with lemmatize compatable tags\n",
    "def pos_replace(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "#Makes list of punctuation to exclude, keeps certain symbols\n",
    "punct = list(string.punctuation)\n",
    "keep_punct = ['#', '?', '!', '@']\n",
    "punct = [p for p in punct if p not in keep_punct]\n",
    "\n",
    "#Used to filter rt\n",
    "common_tweet_words = ['rt']\n",
    "\n",
    "#Removes non-ASCII characters\n",
    "def remove_junk(tweet):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in tweet])\n",
    "    \n",
    "def tweet_tokenizer(doc):\n",
    "    #Gets rid of links\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    doc = re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', '', doc)\n",
    "    #Gets rid of #sxsw hashtag variations\n",
    "    doc = re.sub(r'(?i)(#sxsw)\\w*', '', doc)\n",
    "    #Gets rid of conversions made during scraping\n",
    "    doc = re.sub(r'{link}', '', doc)\n",
    "    doc = re.sub(r'\\[video\\]', '', doc)\n",
    "    #Gets rid of weird characters\n",
    "    doc = remove_junk(doc)\n",
    "    #Tokenizes using NLTK Twitter Tokenizer\n",
    "    tweet_token = TweetTokenizer(strip_handles = True)\n",
    "    doc = tweet_token.tokenize(doc)\n",
    "    #Gets rid of any tokens that represent if the tweet was retweeted\n",
    "    doc = [w for w in doc if w.lower() not in common_tweet_words]\n",
    "    #Gets rid  of any punctuation that we don't want to keep\n",
    "    doc = [w for w in doc if w not in punct]\n",
    "    #Lemmatizes tokens\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(w[0], pos_replace(w[1])) for w in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we opted to keep retweets in the dataset despite removing the 'rt' marker. While this may have artificially increased certain words, possibly causing overrepresentation, we felt that the magnification of sentiment of these tweets was still important information. Of the dataset, 750 tweets ended up being retweeted, accounting for 1,485 total retweets in the dataset. Therefore, while it may impact the representation of some words in the data, we think the amplified sentiment the retweets supply can help model performance moreso than exclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Simple Model\n",
    "---\n",
    "The first simple model we looked at was a decision tree that only gets fed the tokenized tweets without any additional columns. This was just to get a sense of how a very simple model would establish a baseline, when not supplied with any additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.4431343650208794\n",
      "Validation Score:0.44387000497429946\n"
     ]
    }
   ],
   "source": [
    "y = tweets['label']\n",
    "X = tweets['tweet_text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "baseline = imbpipeline(steps=[\n",
    "    ('preproc', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213, max_depth = 5))\n",
    "])\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "print(\"Training Score:\", baseline.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(baseline, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Bayes Models\n",
    "---\n",
    "The next model type we wanted to test out was Multinomial Bayes, as Bayes models tend to do well with text data and would likely show an improvement over a simple decision tree. As we are not performing binary classification, we opted for Multinomial Bayes and started with the CountVectorizer, as it wasn't clear if weighting the words across different tweets, using TF_IDF, would yield any additional improvement to the model. We also included the features created above in order to help improve model performance. Finally, as we have class imbalance affecting tweets with negative sentiment, we opted to SMOTE the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8295259150085974\n",
      "Validation Score:0.6236779668681509\n"
     ]
    }
   ],
   "source": [
    "#Creates features and target then performs train test split\n",
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "#Creates preprocessing step\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "#Creates pipeline for model\n",
    "apple_cv = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state = 213)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#Fits model and prints training score\n",
    "apple_cv.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_cv.score(X_train, y_train))\n",
    "#Cross validates model and prints average result\n",
    "scores = np.mean(cross_val_score(apple_cv, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above model improves on the first simple model, it overfits the data as there is a gap of 0.2 between training and validation scores. To see if we could bring this overfitting down, another Multinomaial Bayes model was run with TF_IDF as the vectorizer to see how that changed the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7418324735937116\n",
      "Validation Score:0.571851494550881\n"
     ]
    }
   ],
   "source": [
    "#Creates preprocessing step for pipeline\n",
    "tfidf_vec = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "#Creates pipeline for model\n",
    "apple_tfidf = imbpipeline(steps=[\n",
    "    ('preproc', tfidf_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state = 213)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "#Fits model and prints training and cross validation scores\n",
    "apple_tfidf.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_tfidf.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_tfidf, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the training score goes down, this model overfits less compared to the first and therefore is the version we proceeded with. In order to tune and get an improvement on validation scores, we ran this pipeline through a gridsearch to attempt to figure out which parameters in the vectorizer could potentially help reduce the overfitting currently present. We tested max features alongside max_df to see if limiting the more and less common words could improve model fit, additionally enabling us to look at the inclusion of bigrams in order to see if they add any additional information. We also tested case to observe its influence on classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preproc__tfidf__lowercase': True,\n",
       " 'preproc__tfidf__max_df': 0.75,\n",
       " 'preproc__tfidf__max_features': 4000,\n",
       " 'preproc__tfidf__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__tfidf__max_features': [1000, 4000, 8000, None],\n",
    "    'preproc__tfidf__max_df': [.75, .85, .95],\n",
    "    'preproc__tfidf__lowercase': [False, True],\n",
    "    'preproc__tfidf__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(apple_tfidf, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like ignoring terms in the top 25% of frequency, using all lowercase words, and limiting the vectorizer to only 5,000 words, while sticking with only unigrams output the best performance. While the adjusted model below shows only slight changes in the score, there is still a reduction in overfitting. The training score has also gone down again, but this is still an overall improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7234094816998281\n",
      "Validation Score:0.5735735065796416\n"
     ]
    }
   ],
   "source": [
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "tfidf_vec = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = True, tokenizer = tweet_tokenizer, \n",
    "                              max_df = .75, max_features = 4000), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "apple_tuned = imbpipeline(steps=[\n",
    "    ('preproc', tfidf_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state = 213)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "apple_tuned.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_tuned.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_tuned, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final Bayes model we ran was a Complement Naive Bayes model. This model was built to deal with imbalanced data; since the negative sentiment tweets make up so few of our data points, we opted to try it out. We went through a similar process to the Multinomial model for tuning the results, and the final model from that is used below. This was ultimately our best model. While there is still a significant amount of overfitting, it is a comparable amount to the final Multinomial Bayes model. Therefore, as the accuracy scores are better, this is the final model we decided to go with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7966101694915254\n",
      "Validation Score:0.6362077749807811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "tfidf_vec = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                             max_df = .5, max_features = 4000), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "final = imbpipeline(steps=[\n",
    "    ('preproc', tfidf_vec),\n",
    "    ('mnb', ComplementNB())\n",
    "])\n",
    "\n",
    "final.fit(X_train, y_train)\n",
    "print(\"Training Score:\", final.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(final, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we thought Multinomial Bayes would give us the best result, we also wanted to test out a different classifier to see if we could get any improvement in score. While a simpler decision tree was run above with only tweets, we ran another with the added features and no hyperparameters, to see what the change in score would be. As the TfidfVectorizer performed better with Bayes above, we opted to keep it as the vectorizer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9995087202161631\n",
      "Validation Score:0.570378498967456\n"
     ]
    }
   ],
   "source": [
    "#Establishes preprocessing for the decision tree\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "#Creates decision tree pipeline\n",
    "apple_dt = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state = 213)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213))\n",
    "])\n",
    "\n",
    "#Fits the new model on the data and prints the score for training and cross validation\n",
    "apple_dt.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_dt.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_dt, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fantastic training score, the tree is clearly overfitting. In order to tune the tree and hopefully reduce the overfitting, we ran another gridsearch for the decision tree to tune the hypterparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtc__criterion': 'gini', 'dtc__max_depth': 20, 'dtc__min_samples_split': 5}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'dtc__max_depth': [None, 20, 30],\n",
    "    'dtc__criterion': ['gini', 'entropy'],\n",
    "    'dtc__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(apple_dt, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tuned parameters were then fed into the model below. While the overfitting does go down, it's still pretty substantial. Therefore, the final model we opted to go with was the tuned Complement Naive Bayes above, as it had the least amount of overfitting while still retaining acceptable accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8676000982559567\n",
      "Validation Score:0.5775001884204338\n"
     ]
    }
   ],
   "source": [
    "#Adjusts the model in the pipeline to include new parameters\n",
    "apple_dt_tuned = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state = 213)),\n",
    "    ('dtc', DecisionTreeClassifier(max_depth = 30, random_state = 213))\n",
    "])\n",
    "\n",
    "#Fits the new model and prints out training and validation scores\n",
    "apple_dt_tuned.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_dt_tuned.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_dt_tuned, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "---\n",
    "As mentioned earlier, the final model we went with was the tuned Complement Naive Bayes model that used the Tfidf vectorizer. We opted for this model over the others as it had the highest accuracy score while maintaining the least amount of difference between training and validation sets, indicating it was the model that overfit the least. The final score of the model based on the test data is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6219602063375093"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the final accuracy score of the model is 62%. As for why we chose accuracy as the metric, in terms of false positives and false negatives for sentiment, neither did we want to avoid more. As we are trying to get a gauge of customer interest across three different sentiment types, we did not want to avoid one misclassification over the other, as we do not care more for any one particular sentiment. Therefore, we opted for accuracy as our metric to find the model that would perform the best overall. For curiosity's sake, the confusion matrix for the model is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "y_hat = final.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "cmdis = ConfusionMatrixDisplay(cm)\n",
    "cmdis.plot();\n",
    "plt.savefig('images/confusion_matrix.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the final model's accuracy has gone up from around 45% to 63%, there are still improvements to be made and also a bit of overfitting remains in the model. As seen in the confusion matrix, the model still has a hard time identifying positive and negative sentiment as non-neutral. Therefore, while the current model may be faster than having someone determine the sentiment of a tweet on their own, there is still more work to be done before the model could be deployed and prove truly useful to Apple's product team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---\n",
    "Overall, we would recommend using the model as it is fairly accurate in distinguishing tweet sentiments, especially when compared to doing so by hand. As a result, Apple's product team can use this sentiment analyzer to target neutral consumers and convert them to buyers. While the model improved from the first pass, it still doesn't distinguish non-neutral tweets from neutral ones very well. Therefore, this model might not fully solve the business problem. Furthermore, this sentiment analysis only looked at tweets, but consumers likely post about Apple on other platforms as well. That is why, going forward, it would be interesting to try and implement sentiment analysis for some of these other platforms to try and gauge consumers by different means. On top of that, making the sentiment labels themselves more fine grained would provide more insight into how strongly consumers feel, so incorporating that into the project in the future may yield better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
