{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stakeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apple as stakeholder and compare sentiment of its release vs. Google's at SXSW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('../data/tweets.csv', encoding = 'iso-8859-1')\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets rid of company column\n",
    "tweets = df.drop('emotion_in_tweet_is_directed_at', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops rows with unknown sentiment\n",
    "tweets = tweets[tweets['is_there_an_emotion_directed_at_a_brand_or_product'] != 'I can\\'t tell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renames column with sentiments as label and drops single nan row\n",
    "tweets['label'] = tweets['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "tweets = tweets.drop('is_there_an_emotion_directed_at_a_brand_or_product', axis = 1)\n",
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    0.602954\n",
       "Positive emotion                      0.333259\n",
       "Negative emotion                      0.063787\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.label.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.602954\n",
       "1    0.333259\n",
       "0    0.063787\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reassigns label\n",
    "tweets.label = tweets.label.map({'Negative emotion' : 0, 'Positive emotion': 1, \n",
    "                                 'No emotion toward brand or product': 2})\n",
    "\n",
    "tweets.label.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to tokenize text\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Replaces pos tags with lemmatize compatable tags\n",
    "def pos_replace(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "#Makes list of punctuation to exclude, keeps certain symbols\n",
    "punct = list(string.punctuation)\n",
    "keep_punct = ['#', '?', '!', '@']\n",
    "punct = [p for p in punct if p not in keep_punct]\n",
    "\n",
    "#Used to filter out words leftover by twitter scrape\n",
    "common_tweet_words = ['rt']\n",
    "\n",
    "#Removes non-ASCII characters (aka emojis that cant be converted to original symbol)\n",
    "def remove_junk(tweet):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in tweet])\n",
    "    \n",
    "#TRY LOWERING/NOT LEMMATIZING AND SEE WHAT CHANGES\n",
    "def tweet_tokenizer(doc, stop_words = sw):\n",
    "    #Gets rid of links\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    doc = re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', '', doc)\n",
    "    doc = re.sub(r'(?i)(#sxsw)\\w*', '', doc)\n",
    "    #Gets rid of conversions made during scrapping\n",
    "    doc = re.sub(r'{link}', '', doc)\n",
    "    doc = re.sub(r'\\[video\\]', '', doc)\n",
    "    #Gets rid of weird characters\n",
    "    doc = remove_junk(doc)\n",
    "    #Tokenizes using NLTK Twitter Tokenizer\n",
    "    tweet_token = TweetTokenizer(strip_handles = True)\n",
    "    doc = tweet_token.tokenize(doc)\n",
    "    #Gets rid of numbers\n",
    "#     doc_2 = []\n",
    "#     for w in doc:\n",
    "#         if any([c.isdigit() for c in w]):\n",
    "#             pass\n",
    "#         else:\n",
    "#             doc_2.append(w)\n",
    "    #Gets rid of leftover stopwords/punctuation/twitter meta-info\n",
    "    doc = [w for w in doc if w.lower() not in sw]\n",
    "    doc = [w for w in doc if w.lower() not in common_tweet_words]\n",
    "    doc = [w for w in doc if w not in punct]\n",
    "    #Stemmer\n",
    "#     snows = SnowballStemmer('english')\n",
    "#     doc = [snows.stem(w) for w in doc]\n",
    "    #Lemmatizes tokens\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(w[0], pos_replace(w[1])) for w in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@mention  - False Alarm: Google Circles Not Coming NowÛÒand Probably Not Ever? - {link} #Google #Circles #Social #SXSW\n",
      "\n",
      "\n",
      "['False', 'Alarm', 'Google', 'Circles', 'Coming', 'Probably', 'Ever', '?', '#Google', '#Circles', '#Social']\n"
     ]
    }
   ],
   "source": [
    "t = tweets.loc[38, 'tweet_text']\n",
    "test = tweet_tokenizer(t)\n",
    "print(t)\n",
    "print('\\n')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Get rid of     don't delete this\""
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = 'Get rid of #sxsw #SXSW #Sxsw #SXSWedu don\\'t delete this'\n",
    "test = re.sub(r'(?i)(#sxsw)\\w*', '', test)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Features with Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def setup_three_subplots():\n",
    "    \"\"\"\n",
    "    It's hard to make an odd number of graphs pretty with just nrows\n",
    "    and ncols, so we make a custom grid. See example for more details:\n",
    "    https://matplotlib.org/stable/gallery/subplots_axes_and_figures/gridspec_multicolumn.html\n",
    "\n",
    "    We want the graphs to look like this:\n",
    "     [ ] [ ] [ ]\n",
    "       [ ] [ ]\n",
    "\n",
    "    So we make a 2x6 grid with 5 graphs arranged on it. 3 in the\n",
    "    top row, 2 in the second row\n",
    "\n",
    "      0 1 2 3 4 5\n",
    "    0|[|]|[|]|[|]|\n",
    "    1| |[|]|[|]| |\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(15,9))\n",
    "    fig.set_tight_layout(True)\n",
    "    gs = fig.add_gridspec(2, 4)\n",
    "    ax1 = fig.add_subplot(gs[0, :2]) # row 0, cols 0-1\n",
    "    ax2 = fig.add_subplot(gs[0, 2:4])# row 0, cols 2-3\n",
    "    #ax3 = fig.add_subplot(gs[0, 4:]) # row 0, cols 4-5\n",
    "    ax4 = fig.add_subplot(gs[1, 1:3])# row 1, cols 1-2\n",
    "    #ax5 = fig.add_subplot(gs[1, 3:5])# row 1, cols 3-4\n",
    "    return fig, [ax1, ax2, ax4]\n",
    "\n",
    "def plot_distribution_of_column_by_category(column, axes, title=\"Word Frequency for\"):\n",
    "    for index, category in enumerate([0, 1, 2]):\n",
    "        # Calculate frequency distribution for this subset\n",
    "        all_words = tweets[tweets[\"label\"] == index][column].explode()\n",
    "        freq_dist = FreqDist(all_words)\n",
    "        top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "        tokens = top_10[0]\n",
    "        counts = top_10[1]\n",
    "\n",
    "        # Set up plot\n",
    "        ax = axes[index]\n",
    "        ax.hist(tokens, counts)\n",
    "\n",
    "        # Customize plot appearance\n",
    "        ax.set_title(f\"{title} {category}\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.tick_params(axis=\"x\", rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "all_words = []\n",
    "for tweet in tweets['tweet_text']:\n",
    "    all_words.extend(tweet_tokenizer(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cell', 'conference', 'coworkers', 'trophy', '10x', 'announces', 'gut', 'Alamo', 'Droid', 'starbucks', 'Autism', 'hashable', 'desk', '#bemyneighbor', 'thrown', 'God', 'extensive', 'employ', 'late', 'b', 'Digg', 'Reminded', 'bad', 'Z16', '#designingforkids', 'Killers', 'carrie', 'Ps', 'spot', 'copy', 'voluntarily', 'Web', \"i'd\", '#pseudoretweet', 'Quiet', 'touchpad', 'GET', 'mile', 'Feeback', 'instantly', 'CW', '#gssxsw', 'Toronto', 'Dictators', 'Callay', 'Survive', 'Sketching', 'ensure', 'shitty', 'lobby', \"O'Reily\", '#PosterousEvents', 'unequipped', 'lense', '#rebelTV', 'Fluffy', '4th', 'Steampunk', 'peep', 'useful', 'loathe', 'converge', 'execute', 'fundamental', 'doen', '#kickoffparty', '#web3', '5:30', 'Sketchbooks', 'Weekend', \"who've\", '#techenvy', 'Playbook', '#uncertainty', 'bucket', '#GR2L2', 'prop', \"Planet's\", 'money', 'guitar', 'Forrester', '200', 'advertise', '#DOMO', 'Recorder', 'Uncomfortable', 'Cus', 'Hobo', 'trial', '#googledoodle', 'bluetooth', 'poem', 'cable', 'coincides', 'reaction', 'unplug', '#TheInternet', 'Eg', 'study', 'MK', 'pacific', 'banana', 'blogpost', '<title>', 'convention', 'many', 'Sangre', 'popup-store', 'Burner', 'whoooooo', 'Tnx', 'festival', 'margin', 'Arcade', 'giggle', 'crossroad', '#hot', 'cabbie', '#CNNGrill', '#OpenBeta6', 'Magic', 'privacy', 'table', 'Ladies', 'JAPANESE', 'seriously', 'deeply', 'Lightning', 'kitkat', 'colleague', 'Spiderhouse', 'earth', 'capture', 'put', 'Stranger', 'heaven', 'sick', 'KH', 'retire', 'Bless', 'Portlanders', '#justsaying', 'Saw', '___', '6:30', 'Gadget', 'Demo', 'Desperately', 'groupme', 'ATTN', '#infektd', 'upset', 'Marshall', 'holler', 'mail', 'Boi', 'Correct', 'Tim', 'agenda', 'Bernd', 'Finally', '6.25', 'north', 'Nelson', 'selling', '#Voicefeed', 'ubertwitter', 'rockin', 'Entice', 'photobooth', 'change', 'Dionne', 'Berry', '#veryslow', 'FOUND', 'Tracy', 'Smart', 'us', 'monetize', 'inc', 'closing', 'WABI-SABI', 'steam', 'instrument', 'storage', '#iPad_2', 'client', 'rule', 'avro', 'charity', 'Breaking', 'heh', 'possibly', 'compass', 'killing', 'got', 'round', 'gonna', 'Buzz', 'scar', 'Brutal', 'interrupt', 'fawn', '#Hollrback', '#annoying', '#internet', 'tonigh', 'Rise', 'SxSWSex', 'productive', 'pit', 'Sxsw', 'compan', 'integration', 'Complete', 'flat', 'rechargeable', 'Depeche', '#googlemaps', 'OH', 'ubiquity', 'ZMS', 'house', 'BILLION', 'UI', 'telecommuters', 'CloudSight', '#musedchat', 'hood', 'Quantter', '#periscope', 'belly', '11:00', 'Cool', 'prefers', 'Year', 'tkts', 'fourth', '#macallan', '#GoogleLa', 'TechStars', 'ths', '#line', '#gaming', 'dodgeball', '120', 'advertising', 'necessary', 'full-force', 'sense', 'Blog', 'remember', 'Confusion', 'cut', 'Utility', 'cocaine', 'upcoming', '#notevenstartedyet', 'realization', '#garyvee', 'cabby', 'sky', 'reply', '03/15', 'willinclude', 'MashBash', 'admit', 'nose', 'photo', 'wear', 'conversing', 'hoc', 'Tattoo', 'jam', 'leisure', 'doctor', '#zing', '#Events', 'Rainjacket', 'town', 'nothing', 'Phones', '#Mac', '#myegc', '#BlowingUp', 'Computer', 'ironic', 'jcpenney', '#kneejockey', 'quickly', 'Lol', 'sxsw', '#4sq', '#liquidux', 'Boolean', '#prsa', 'prob', 'REALLY', 'loose', 'endorse', 'Publishing', 'lot', 'intuitive', 'acceleration', 'Anyone', 'exploration', 'Farms', \"Kanye's\", '#festivalgenius', 'bed', '11bil', 'Hackathon', 'rock', 'range', 'Lanyard', 'demonstrate', 'pdparticle', '#PLAYSXSW', '#flipboard', 'Fb', '#buzz', 'gadget', 'deaddd', 'Impact', 'Ahead', '#truestory', 'BLACKBERRY', '#applestore', 'Side', 'Wal-Mart', 'idiocy', 'Download', 'inside', \"Today's\", 'sustainability', '#cloud', 'assignment', 'exclusively', 'HLS', 'ballroom', 'accessibility', 'tot', '10mins', 'Planet', 'festivities', 'factor', 'kingdom', 'cinema', 'halfway', 'Larry', 'Optimistic', 'resourceful', 'blackbook', 'meat', 'Insane', 'ay', 'artifact', 'intend', '#Launch', 'Bicycle', 'feedback', 'Macbooks', 'Heat', 'sham', 'Data', '#libraries', 'Brain', 'Packrat', 'AE', '#VideoGames', '12:01', 'Stereo', 'Bueller', '#cellbots', 'notwithstanding', 'FAIL', 'Funny', 'Note', 'obv', 'Physical', 'nite', 'Officer', 'realtime', 't-mobile', 'collaboration', '#ApartMovie', 'CNNMoney', 'concept', 'marissa', 'Queue', 'return', 'Harris', 'statement', '#UXDes', 'twice', 'VatorNews', 'lady', 'Gap', 'Aol', '1000', 'En', '2:45', 'nab', 'evening', 'Layar', 'Imagine', 'Yobongo', 'Pinoy', 'JC', 'Schemas', '4.0', 'psyche', 'Followed', 'Glad', 'buyer', 'streaming', '1:1', 'Yet', 'increase', 'Haiti', 'f', 'rush', 'driver', 'chimp', 'Store.The', 'two-screen', 'Monkey', 'Niran', '#thePlatform', 'Maudies', '15', 'Liz', \"Apple's\", 'informative', \"Let's\", 'Pop', 'mac', 'Consulting', 'howmto', 'choose', 'enablers', 'field', 'Dugan', 'Talks', 'computer', 'Kingston', 'remind', 'Armadillo', 'ittttt', 'grand', 'magically', 'Milyoni', 'present', 'Die', 'ui', '#HootStats', 'Alcoholics', '#futureoftouch', '#CStejas', '1of', 'TechCrunch', 'ZIP', 'Town', 'gift', 'turn-by-turn', '#insanity', '1', 'bugger', 'Conan', 'others', 'lustre', 'Spearheading', 'Sell', 'handset', \"day's\", \"Foursquare's\", 'Post-Mobile', 'Domain', 'Yeah', 'XMAS', 'dawdle', 'IN1102', 'Jeebus', 'trip', 'undivided', 'Motley', '#meetup', 'brawl', '#Foursquare', 'sweepstakes', 'attention', 'Stay', 'Humorous', 'either', 'Left', '#hacknews', 'interesting', 'NOV.US', 'Inc', 'elsewhere', 'antigov', '#TakeOneForTheTeam', 'Jonathan', 'pie', '#hackingwork', 'gap', 'Patterns', 'three-quarters', '#iminfanboycentral', 'Specials', 'B-day', 'counsel', 'Guide', \"why's\", 'export', 'development', 'Apparently', 'severe', '#mobilenews', 'DEAL', '#science', 'query', '#franken', 'Gen', 'succumb', 'I21', 'navigate', 'contraption', 'JAM', '#WhetherApple', 'Gave', 'clutter', 'pledge', 'Things', 'Accelerater', 'Ken', 'Grace', 'kinda', '#team_android', 'addition', 'Damn', 'operate', 'AustinJS', 'Mpls', 'heave', 'teach', 'punch', 'Bummer', 'Walk', 'Zeitgeist', 'juan', 'try', 'Best', '450', 'SWF', '#cool', 'full-swing', 'result', 'vuvuzela', 'TR', '#lazyweb', '#thisiscrazyfun', 'aber', 'hoodie', 'cheeky', 'Related', '9:35', '#fb', 'spreadsheet', 'trauma', 'si', 'Absolutely', 'Unbelievable', 'predictability', 'commercial', 'compile', 'enjoyed', 'painful', 'Sad', 'Macys', 'NU', 'Wurst', '1154', 'HQ', 'wey', 'dude', 'overuse', '5min', '#CO', '#4sqchat', 'convore', 'RIGHT', 'Include', \"I'll\", '#Keitai', 'Everything', 'restaurants-rating', 'Jambox', 'deserve', 'added', 'WRAP-UP', 'bestie', '#ConnectedTV', 'Outside', '#xplat', '#guykawasaki', 'proof', '#caniexpenseanipad', 'imacs', 'cardless', 'Williams', 'Shirt', \"SXSW's\", 'entrepreneurial', '#Tech', 'apologies', 'mother', 'competitor', 'Copyright', '#eeducation', 'Times', 'IPHONE', 'smartphone', 'pas', 'techno', 'ahem', 'think', 'crap', 'Aussies', 'resonant', 'Applications', 'fan', 'rewards-type', 'explorer', 'Bin', '#barrydiller', 'have', 'm.mayer', '96', 'Journalism', 'kick', 'expierence', 'NY', 'curse', 'Content', 'Dang', 'Analytics', 'experiential', \"there's\", 'chrome', 'engaging', 'inspire', 'Short', 'Irish', 'hundred', '22', 'onerous', '#wordnerd', 'Codes', 'Present', 'abound', 'too--prove', 'lick', \"PAC-Man's\", 'BANKING', 'Congrats', '#abacus', 'organization', 'Trust', 'calculation', 'yourmom', 'Waitress', 'Beckley', 'must.resist.Mac.temptation', 'Venue', \"Foodspotting's\", \"he's\", 'Incubator', 'Sorry', 'Compete', 'spring', '#rejection', 'Tour', '3/14', 'Volunteer', 'ur', 'bgr', 'web', 'publicize', 'Red', 'geotagging', 'meetin', 'Grandfather', 'dvrs', 'session', 'chatter', 'Tweet', 'acknowledge', 'former', 'quicker', 'bag', 'Delayed', 'Hear', 'Sucks', '88', 'esque', 'glitch', 'Agencies', 'Dealing', 'connectivity', 'wasnt', 'barcode', 'hv', 'Save', '#13', 'Tandy', 'Headed', 'samsung', '#Recco', 'steal', '#verizonFTW', 'Knife', 'meh', 'vid', '#blackberry', 'Events', 'goddamn', 'keynote', 'finish', '#22sxsw', 'ol', 'cope', 'Alive', 'Regretting', 'poison', 'gen', 'Springing', 'gather', 'info', 'Tracker', \"Flipboard's\", 'band', '#diversity', 'celeb', 'Encounter', 'Onto', 'Dodgeball', 'replicate', 'authoritzed', 'worm', 'atleast', 'w00t', 'backpages', '#GoogleCircles', 'APPLE', 'traffic', 'effen', '--->', 'SCVNGR', '#bmm', 'impossible', '#popupstore', 'I10', 'Thinking', 'pause', 'BAR', \"Can't\", 'stood', 'Love', 'opening', 'invent', '#chargin2diffphonesatonce', '#gsdm', 'Beware', 'beach', 'Murphy', '#evrmobile', 'Wilderness', 'snatch', 'piece', 'Recharge', 'tonite', 'forest', 'Watched', 'frontend', 'focus', 'trusty', 'Last', 'frictionless', '#ux', '#inagist', 'Unloading', 'skateboard', '#googledoodles', 'playback', 'Robots', 'DST', 'mindshare', 'Help', '#AnybodyWantToBuyMeAnIpad2', '#smallbiz', 'CBS', 'Vuitton', 'WAM', '#NPAC', 'Official', '#lbs', 'exist', 'grumble', '#searchengineland', 'resonance', 'licence', \"EVR's\", 'remain', 'Let', 'fist', 'torture', 'patch', 'factory', '#wakeuplaughing', 'La', 'photos', 'suckle', 'useless', 'lan', 'police', '2nite', 'LIGHT', 'networking', 'News', 'packed', 'travel', '#droid', 'couple', 'local', 'etch-a-sketch', '#SocialSearch', 'screening', '#gamelayer', 'consumer-generated', '#msusxsw', 'first', 'chronicle', \"startup's\", 'Diego', 'Taptu', '23', 'built-in', 'sure', 'rather', 'cc', 'Pearson', '80', 'Reliving', 'clearly', 'Q2', '#craigslistkiller', '_llet', '#newapplestoreaustin', 'lifetime', 'C34', 'thrill', 'in-depth', 'strip', 'practical', 'willpower', 'Roddy', 'NEW', 'affect', '7:00', 'Celebrate', 'applause', 'Beta', 'bos', '#artikelen', 'best', 'Browsers', 'support', 'drain', 'Pot', 'Investors', 'Z28', \"Auntie's\", 'surely', 'advise', 'T-mobile', 'refuse', 'incenticize', 'Age', 'replenish', 'well', '2x', 'class', 'Salgado', 'Altimeter', 'tip', '#swag', 'unscientific', 'PS3', 'Stage', 'Power', 'used', 'LUCYS', 'Kelt', 'payment', 'chair', 'abnormal', '#androidcrunch', 'Bubo', 'Phrase', 'devastation', 'Tip', 'cheese', 'excludes', '#liveshows', 'punchout', 'st', '#Latism', 'visigoth', 'unlock', '#TouchingStories', 'private', 'Dating', 'Realizing', 'Another', 'Jony', 'yo', '#startupbus', 'Lennon', 'Light', '#booya', 'tab', '#appleaddiction', 'Mar', 'shakedown', 'Easy', 'Contest', 'Audi', 'vestibule', 'television', 'twtng', '00-11', 'Internet', 'sneaker', 'Chen', 'suppose', 'Missoni', 'FRESH', 'Nonprofit', 'SNS', 'Tribune', 'Open-source', '#austinites', '125', 'Installs', 'tryna', 'lug', 'Coinsidence', 'N22', 'tweetcaster', 'authority', 'Full', 'follower', 'hit', 'Janecek', '#mobile491', 'browsering', 'disappointed', 'ninja', 'Aaron', 'access', 'cupcake', 'bookmark', 'fedoras', 'state', 'starbu', 'co-hosting', '#Illmakeitwork', 'Chavez', 'museum', 'help', 'Lifeless', 'ala', 'Lazy', '#lifelinetotheworld', '#critthink', 'global', 'movement', 'unfortunate', 'Everywhere', 'Ppl', '#Awesome', 'Bojan', '#Founders', 'breath', 'Hall', '#techiesunite', 'Doh', 'unemployed', 'trillion', 'BREAKING', 'Craziness', 'forgotten', 'Macs', 'scenes', 'Chuck', '#lost', 'Unlockable', 'unconvinced', 'taco', 'Drop', '#Speech', 'Meeping', '829', '#UberSocial', 'Latitude', 'Indexed', 'C10', 'Blanc', 'Promise', 'TX', '#Masha', 'interest', 'Narcissism', 'Every', 'Small', 'distrub', 'ahi', '#jobs', 'DL', 'deal', 'Checking-out', 'slick', '#crowley', 'TC', 'shat', 'Twitgle', 'finalist', \"#Apple's\", 'unleash', 'N25', 'follow', 'Director', 'Baby', '#Tapworthy', 'ground', 'house-go', 'Felix', 'preview', 'slim', '512-457-8800', 'Fado', 'Screens', 'Nat', 'Neumann', '59:59', 'Smarty', 'exactly', 'Linkedin', 'orkut', '#blast', 'League', 'Cinema', 'starten', 'electric', 'Spazzmatics', 'USB', 'hypocrisy', '#10', 'enjoy', 'initiate', 'cartoonishly', 'sec', 'Available', '64gig', '#Newsweek', '#iphoneapp', '#uxamandroid', 'Techie', 'yellow', 'accidentally', 'gratification', 'Ocarina', 'promos', 'Festival', 'siliconvalley', 'fear', 'always', 'Queueing', 'compatible', 'Warning', 'typo', 'cocktail', '#film', 'Laptop', 'centric', 'Spam', '#powermattteam', 'sampler', 'leash', 'torturous', 'madness', '#weak', 'hard', 'right-brain', 'seat', '#letushopenot', '#fools', '#banking', 'aight', 'criticism', 'overstock', 'tragedy', 'mm', 'fundraise', 'Ooooo', 'gigantic', 'Anonymity', 'virtual', '7pm', 'LIKE', 'learn', 'dt', 'Stripes', 'could', 'FUN', '800', 'merchandise', 'newspaper', 'Solves', 'Able', 'Must-have', 'vegan', '#ignitesxsw', 'week', '#netflixiphone', '#How_To', '#shice', '#wellplayed', 'JK', 'vend', 'Navigation', 'ACLU', '#Rightnow', 'unveiled', 'Verizon', 'Tsk', 'Slap', '#sheeple', 'bloke', 'WebKit', 'emergency', 'snuggie', 'champ', 'cheap', 'delight', 'ask', '#hipstamatic', 'originally', '#stribpol', '#cnn', '#POURsite', 'inner', 'Chzbrgr', 'non-apple', 'Curious', 'life-changing', 'glow-in-the-dark', '#geeksrule', \"u'r\", '#loveit', 'king', 'Wonders', '#usguys', 'fabulous', 'powerful', '11:30', 'Soundtrax', 'Society', 'Started', 'Headline', 'Global', 'downside', '#grauniad', '17', 'forbidden', 'McCue', 'shelf', '#mkesxsw', '#Top_News', 'following', 'hilarious', 'white', '#in', 'fav', '#ucpress', 'torch', 'auto-correct', 'nearly', 'NicoRiccelli', 'Collecting', 'botch', 'funnel', '#BillMurray', 'writer', 'Somebody', 'aclu', '#heattracker', 'Disaster', '#bynd', 'nasty', 'mobil', 'null', 'Latest', '#futurefifteen', '#developers', 'competition', 'Nieuwe', 'dudes', 'maggie', 'Wish', 'Hipster', 'Rob', '64g', 'Less', '#SXSH', '#rickshaw', 'Chips', 'stabilizer', 'combine', 'Pub', 'LARGER', 'Summary', 'macbook', 'arcade', 'Great', 'drama', 'quit', 'Women', 'jg', 'Space', 'Quick', 'Pah', 'Dying', 'industrial', 'Perhaps', 'rite', 'Next', '#GPS', 'sq', 'iphones', 'Xbox', '#cvdc', '#welivehere', '#Macallan', 'View', '#TheRocksReport', 'Skyfire', 'Estoy', 'Pengen', '#hipster', 'anatomy', 'I-Phone', 'engine', 'political', 'prototype', '#marissagoogle', '#digitalblasphemy', 'graphic.ly', '#iPhone', 'boom', 'shortcut', '#16162', 'iphone', 'hobo', 'witty', 'NFC', 'Result', 'Brooklyn', 'Conf', 'function', 'Yea', \"Fitzgerald's\", 'Opening', '4G', 'nav', 'suggestion', '10,000', 'Austin-area', 'iAnything', 'ATT', 'Snap', '#tech4good', 'visualize', 'flannel', 'AlphaGraphics', 'fucker', 'cheesesteaks', '#musique', '#Canada', 'jetlag', 'jdnya', 'PR', 'UBER', 'toast', 'CONFIRMED', 'key', 'booth', 'mode', 'SmartFriend', 'Unofficial', 'contest', 'Sxxpress', 'thought', 'Cops', 'overview', 'Vinh', 'snacker', 'XWave', '#events', '13', 'today', '#southbysouthwest', 'Feel', 'smudgy', 'sprint', 'proceeds', 'basket', 'fancrazed', 'NetworkCircles', 'Editor', 'dope', 'Disease', 'excuse', 'blahzay', '#Hangover3', 'Downloading', 'Interesting', 'Looks', 'invest', 'Canada', 'discovery', \"ev'ry\", '#mktgmoment', 'corporate', '#disagree', 'Everyone', 'prostate', 'foresight', 'Made', '#sprint', 'cowboy', 'Artificial', 'closely', 'stock', 'cpap', 'PDF', 'DIN', 'morphie', 'smarmcake', 'Singapore', 'FaceTime', 'eve', '#copyright', 'friday', 'litle', \"Google's\", '#beevil', 'elevate', 'j', 'pm-Part', 'Ill', 'concert', 'front', 'meaningful', 'provider', 'Wishful', 'block', '#nfusion', '230', 'amazing', 'Con', 'incl', 'event_IAP', 'frustration', 'reunite', 'Tools', 'medical', 'behave', 'Mystery', 'ignore', 'stumble', 'Devs', '#Endorsers', 'SXSWi', 'prepping', 'Club', 'wanderer', 'printer', 'Tx', 'Z43', 'SM2', 'fulltime', 'relevant', 'mad', 'Nowhere', 'ingredient', 'consciously', 'source', 'Petricone', 'ice', 'WELL', 'iP4', 'sync', 'sensor', 'vuelta', 'Guadalupe', 'Lordy', 'Apaan', 'Jackie', 'celebrate', 'panel', 'DELICIOUSLY', 'folks', 'saver', 'Invades', \"crowd's\", 'UNLV', 'pedicab', 'Emily', 'Angry', '86', '#search', 'employer', 'couch', 'txts', '#leanstartup', 'Woohoo', 'Leases', 'baby', 'Spots', 'liveley', 'Qrank', 'rubber', '3/19', '#Fandango', 'randy', 'cheer', 'Tweeting', '#arabspring', 'Geo', 'flop', '#PNID', 'Sets', 'Cobra', 'Sunday', '#ios', '#progressbar', '#ipad2', 'WIRES', '#crm', 'bump', 'remedied', 'Katrina', 'Make', '#superhappydevhouse', 'Debut', 'Interfaces', '2day', 'prize', 'sur', 'Courtyard', 'urT', 'Idea', 'iterative', '#ironic', \"butt's\", 'instore', 'C40', 'Kit', 'namely', 'Ballroom', '#NCAA', 'start', 'extend', 'optimistic', 'join.me', '214', 'Twitterstream', 'traditional', 'Credit', 'school', 'fruit', '#IE', 'clock', 'Palo', 'Alisa', 'revolt', 'Lounge', '32', \"team's\", 'produce', 'cynical', 'Calendar', 'omfg', 'lulling', 'fodder', '64GB', 'populous', 'link', '600', 'often', 'Heats', 'magazine', 'along', 'Deck', '#cwc2011', 'flight', 'bff', 'slew', 'BRILLIANT', 'taunt', 'World', 'Sunglasses', 'recorder', 'Hell', 'accept', '#morefunpeoplewatching', 'Trailer', 'contemplative', 'ungrateful', 'noon', 'Synching', 'River', '#mobilefail', 'Museums', 'Entry', \"ass's\", 'contradict', 'Rocks', 'skulls', '4Android', 'presenter', '#techrockstar', 'Checking', 'meet-up', 'standard', 'newsworthy', 'Etsy', 'insists', 'acquire', 'banker', 'courtesy', 'Doc', '#ponies', 'stole', 'cerebral', 'Walking', 'overheard', 'Response', 'Free', 'ez', 'Crowley', 'dissapointment', 'high-five', 'tribute', 'Broken', 'cooper', 'grown', '#MusicMonday', 'sending', 'opera', 'multichannel', 'immobile', '#socailmedia', 'FRI', 'soho', 'Meetings', 'Detail', 'Individual', 'Attention', '#toolongforme', '#appcircus', 'everyone', 'skype', '#travellerARlaunch', 'Vacation', 'evangelist', 'tomo', 'newbie', 'Totalitarian', 'signing', 'ppl', '#slp', 'Localmind', 'Website', '#beforetwitter', 'guerrilla', 'Amazingly', 'Kolko', 'Reality', 'insightful', 'incorrect', \"NYT's\", 'soggy', 'orange', 'lovefresh', '#h4cker', 'monster', 'firefighter', 'prayer', 'upon', 'knockout', 'sunday', '18', 'Jaysis', 'Msft', 'weeknd', 'Midway', 'pop-up', '#amazing', \"Macy's\", 'Show-free', 'fatigue', 'buat', 'sole', 'cripple', 'Charger', \"Here's\", '3G', 'LinkedIn', 'apps', 'collection', 'Cat', 'Bar', 'Massive', '16gb', 'winner', 'Waaaaaa', 'hey', 'Audience', '#itsnot', 'Terrific', 'Places', 'Highlight', 'patience', 'happens', 'carriers', 'Government', 'lesson', 'Penney', '779', 'peer', '3/13', '#Stratlandia', 'Berklee', '#SUxSW', 'I-just-shat-my-fan-boy-knickers', 'slaughter', 'Hill', '#UXAmandroid', '500', 'Tab', 'Moonbot', 'align', 'restraunts', 'Classic', 'Working', 'Rose', 'Phil', 'fuel', 'II', 'Vegas', 'WHITE', 'xmas', 'customized', 'havent', 'Zite', 'appreciation', 'hopeful', 'consultation', 'Schiller', 'reconcile', 'MB', 'recreate', 'AKQA', 'UberSocial', 'dear', 'Disliking', 'Googles', 'drumroll', '#iamagameshater', 'SEO', 'melissa', 'e', 'Compelling', 'punctuate', 'culture', 'sn', 'Omgz', 'limited', 'player', 'forward', 'Device', 'day', 'WP', 'suck', 'Ventures', 'Shhh', 'Roughly', 'listen', 'Archive', 'OMG', 'co-worker', 'T-minus', 'PEOPLE', 'Torrent', 'pure', 'stall', 'Friends', 'Joomla', 'gallery', 'closer', 'infact', 'article', 'Much', 'bored', 'Opera', '#qagb', 'stream', 'Share', 'city', 'surpass', 'Sanctuary', 'ape', 'democracy', '#smm', 'havnt', 'paste', 'prepare', 'entertaining', 'continuous', '#ecademy', '#notatSXSW', 'Blogging', 'Intrigued', 'Lake', '#austinwins', \"tip-don't\", 'fairy', 'spoke', 'Apple-fanboyism', 'flashmob', '#puregenius', 'Sheen', 'underway', 'invited', 'coupon', 'magnet', 'conf', '#kindle', 'rally', 'Pandora', 'Shang', 'Complex', 'Even', 'makery', 'intimidate', 'Boom', 'awkward', 'application', 'Thoora', 'ahoy', 'unbearable', 'period', 'Could', 'Dominance', 'Gah', 'stats', 'Awesome', 'synched', 'Juicing', 'shake', 'milyoni', 'tough', 'Lets', 'Freshbooks', 'delete', 'self', 'umbrella', 'around', 'splendor', 'ATMS', '#get_a_lyfe', 'javascript', 'Whoohoo', 'autumn', 'sayeth', 'airplane', 'startup', 'Spazmatic', 'appear', 'PRIVACY', 'Creek', 'ctr', 'Wowwwwww', 'Later', 'reportedly', 'forever', 'Gym', '#usxsw', 'Bros', 'crave', 'Hangover', 'facebook', '2wk', '#yum', 'skill', 'direction', 'Ha.ha', 'arsenal', 'Transparency', 'chumps', '#SXnewworlds', 'agency', 'aggregate', 'cattle', 'droppin', '63', 'half-friends', 'hour', 'compute', 'right', 'Webiste', 'ample', 'Yup', 'Film', 'anxiety', 'omarg', 'whose', 'DAY', 'idk', 'congratulation', 'tweetup', 'kitten', 'Melissa', 'Bluezoom', 'debug', '45', 'sillier', '#XM', 'format', 'Cover', 'Julie', 'Water', 'relay', 'Photos', 'idea', 'Ecko', '#awesometiming', \"queue's\", 'notepad', 'dispenser', 'annoy', 'Sterling', 'life', '#innotribe', 'Beautiful', 'contextual', '#advertising', 'bear', 'Wooooo', 'app-downloading', 'occasionally', 'planet', 'STDs', 'Aron', 'Means', 'electronics', '#FriendsOffer', 'let', 'bullish', 'lotsa', 'likability', '#google', 'eat', '#DiscoTalk', '#harhar', 'updating', 'acquisition', 'Novelty', 'StartupTribe', 'tinker', 'pressie', 'GOOGLE', 'woo', '#SocialNetworking', '#tech', 'introduces', 'cont', \"Consumerist's\", 'Signs', 'ink', 'Iphone', 'horrible', 'tabs', 'laptop', '#hotpot', 'cup', 'Denis', 'Audio', 'Central', 'vibe', 'Convenient', 'corral', '->', '#toocoolforsxswanyway', '#Barry_Diller', 'manhandle', 'Panorama', 'non-iPad', 'H', 'single', 'staff', 'pavement', 'resource', 'VIA', 'RUMOR', 'fuck', '14th', 'Pls', 'Nicholas', 'fragment', 'Talent', 'dev', 'vandaag', 'Partying', 'innovate', '#nightjar', 'inferior', 'grid', 'Surprisingly', 'Interviewing', 'knowledge', 'Jammy', '#TheKills', 'option', 'Tablet', 'carny', 'Bitbop', 'disappoint', 'MMOD', 'show', '#miamibeach', '4.3', 'demonstration', 'app-based', 'telling', '360iDev', 'Thinks', 'pix', 'breakthrough', 'Fear', 'Identity', 'Arrives', 'mid-battery', 'Unexpected', 'greet', 'Death', 'pusher', 'river', 'pi', 'Battery', 'Wearing', 'Iron', \"gov't\", 'salsa', 'tuxedo', '#olderadults', 'Anti-Apple', '#eecms', 'mindstorm', 'Paul', 'v', '#Photos', 'purpose', 'wifi', 'Nerd', 'Fan', 'speedup', 'Kinect', 'eventful', 'suddenly', 'archive', 'shoe', 'Adwords', 'Young', '4-5', '#FAIL', 'Darryl', 'checkout', 'Rockin', 'generation', '#technology', 'reasonable', 'June', 'lecture', 'Delicious', 'ins', 'Hoping', 'Radio', 'yawn', 'palsy', 'pep', 'overwhelming', 'sapo', '#dtsxsw', 'Big', '#marissamayer', '#Mashabl', 'WineLibrary', 'transparently', 'Temperatures', '#unpaid', '=D', 'Ides', '#ipod', 'Awkward', 'per', 'stylish', 'Kia', 'Z8', 'Jo', '#popup', 'much', 'Visit', 'Birds', 'Mention', 'buddy', '#NewYorkCity', 'clone', 'Teaching', 'heal', 'Kenny', 'Google-ACLU', 'extenders', 'G', 'prez', 'flap', 'Real', '#TC', 'Traveling', '#connectedcar', 'existent', 'exhibitor', 'Yen', '#precommerce', 'upbeat', 'southby', 'repair', '<---', 'Wu', 'Running', 'bible', 'advice', 'ILoveaSurprise', 'MiFi', '#doingsocialmediawrong', 'intention', 'Goggles', 'Samsung', 'Trucks', '#uosxsw', 'yall', 'Per', 'Print', '#singularity', 'zzzs', 'Report', '#Lego', 'ARW', '#food', 'Wandering', '#conversation', 'ahold', 'Restored', 'dull', 'clueless', \"Ipad's\", 'horror', 'Invite', 'VZW', 'three', 'KNOW', '#csuitecsourcing', 'lawn', 'martini', 'definition', 'mass', '#laptop', 'luxury', 'backpack', 'United', 'freak', 'dread', 'UberGuide', 'articulate', 'catch', '#UXdes', '1300', 'Pre-order', 'CDR', 'Pretty', '10:30', 'Mom', 'talented', '150M', 'kickass', 'sux', 'Tests', 'needle', 'POV', 'dance', 'Days', '#edchat', 'Bear', 'human', 'partnership', 'app', 'texas', 'British', 'moly', 'Hmmzies', \"AKQA's\", 'superstar', 'grow', 'bully', 'Vanessa', 'FTW', 'sketchy', 'haha', '#inpdx', '#BrianLam', 'Ep5', 'denim', 'Farmer', '1980', 'Diary', 'metric', 'Double', 'outdated', 'quarter', 'dashboard', '2b', '12:30', 'SPOTS', '#geosocial', 'Cab', 'MASSIVE', 'notch', 'Tips', 'Impromptu', '#Party', 'balance', 'Mindstorm', '#tweetignite', 'N00B', 'Agree', 'mophie', \"line's\", 'sim', 'Deal', 'hooked', 'dictator', 'locally', 'tactic', 'attendong', 'Baaah', 'Park', 'favorite', 'cache', '#ImAnAussie', 'achievement', '#AOL', 'Survival', 'Access', 'empire', 'picture', 'POWER', 'file', '#notmycollegesxsw', 'wait', 'Chevy', 'pump', \"mayer's\", 'pop-ups', 'inventory', 'Cause', 'Nav', 'hell', 'york', 'indication', 'pop-store', 'Drinks', 'obvious', 'EtchnSketch', 'unprepared', 'tags', 'signal', 'au', 'bore', 'KXAN', 'Memento', 'curation', 'P7E9EME7KYLJ', 'threat', 'easy', 'Ads', '#kickball', 'expose', 'sneaky', 'depress', '#entry', 'Arduino', 'brocast', 'Dev', 'filter', 'delightful', 'Phillip', 'Doors', 'convert', 'U', 'Innovative', 'sunglasses', \"Roar's\", 'Webmaster', 'denies', 'snag', 'prepaid', ':(', 'Cable', 'Start', '#win', '#wp7dev', 'guidance', 'multi', '#curatedebate', 'Poss', '699', 'wood', 'camp', 'diagram', '#Justsayin', 'iTC', 'enter', 'Initial', 'interview', 'Bannka', 'paradigm', \"y'all\", 'cast', 'NEEDS', 'Loud', 'Adds', 'Seems', 'year', 'Zuckerberg', 'friends', 'Searches', 'nu', 'rout', 'pepsi', 'overlap', 'passerby', 'Tamale', 'Cor', 'Online', 'track', 'Rosso', \"tech's\", '#evo', 'edition', 'Doubt', '#cbus', 'persistent', 'Forgot', '#bawling', 'Got', 'Er', 'Traffic', 'serviceStory', 'slightly', 'fry', 'Calhoun', 'hope', 'marketplace', 'thick', 'shower', 'Professionals', 'priority', 'Receiving', 'night', '#umassjour', 'Talked', '#please', 'Theatre', 'Forget', 'decide', 'large', 'itme', 'thing-a-ma-jig', 'suicide', 'Congo', '#deviantART', 'response', 'attach', 'launching', 'nifty', 'decade', 'push', '#ipadmadness', 'iPad-winning', 'definitely', 'part', 'Sixth', 'untapped', 'priest', '#podcasters', 'yup', 'er', 'acceptable', 'tuck', 'APAC', 'Feeling', '#CMW', 'bulletin', 'BB', '#crowded', 'coverage', '1pm', '1406', 'och', 'Sudden', 'ideas', '#digibiz', 'Theory', 'Denies', '#ATT', '2nd', '#virtualwallet', 'strict', 'giant', 'HootSuite', '#pedicabs', 'thinner', 'Wicked', '#IwantaCameraonmyiPad', 'overwhelm', '#verizon', 'panelist', 'til', 'Rather', 'teleporting', 'scope', 'subject', 'B', 'Mophie', 'Adobe', 'Teams', 'Shhhh', 'firewall', 'iPods', 'colour', '#JapanQuake', 'OS', 'Stream', '5,000-', 'XIPAD', 'PK', 'unbelievable', 'body', 'Silicon', 'AMI', 'steve', 'distant', 'Rollin', '<video>', '#lowtech', '#Powermatteam', 'Mayer', 'preferrably', 'Six', 'pace', 'os', '#pandora', 'assume', 'highlight', '#EarthHour', 'proprietary', 'black', 'set-up', 'Earth', 'Cafe', 'Driskill', 'swoop', '#zomb', \"Smule's\", 'Teacher', 'sessions-nxt', 'rad', 'Uzu', 'pan', '#nerdheaven', 'ridonkulous', '#imthatgood', 'blackberry', '5.22', 'ditch', 'keyboard', 'contrary', 'reunion', '#Gsdm', 'Rivals', 'Tube', 'Wild', '#understandinghumans', '#USA', 'seeker', '#csr', 'unused', 'route-around', 'LonelyPlanet', 'streetview', '#WAM', 'Lemonade', 'epicenter', 'Regency', '95', 'finagles', 'flipboard', '#Beiber', 'hash', 'Cunning', 'positive', 'Hotpot--looks', 'Translate', 'due', 'perhaps', 'Wintel', 'Recipes', '#property', 'analysis', 'Picture', 'Harbor', 'Ounce', 'date', 'everyday', '#tweetanonymous', 'crack', 'faulty', 'randomly', 'iPad-only', 'pesky', 'pic', '6gjmypj', '#pubcamp', 'judge', 'weather', 'Html', 'spitzer', 'Tweets', 'fame', 'exhibit', 'wall', 'serf', '03/11', 'silent', 'ridiculous', 'amazes', '#Sony', 'meeti', 'Release', 'maps', '#PapaSangre', '#digitaldeath', 'MSFT', '#bandcamp', 'neck', 'still', 'basis', 'usual', 'cost', '812', ':]', 'denial', '#rad', 'costume', 'Tub', 'influential', 'BIG', 'Talking', 'sponso', 'wean', '#TronLoungeSXSW', 'permanently', 'Across', \"Insider's\", 'trench', 'billion', '#Zazzle', 'breathe', 'mapping', 'allows', '#gswsxsw', 'Yellow', 'reset', 'exercise', 'Genius', 'repay', 'war', '#iusxsw', 'worship', 'aw', 'Hollrback', 'showcasing', 'thing', 'Timberlake', '#ui', 'Nvidia', 'security', '#strangeproblems', '#LI', 'Wishing', 'Bobby', 'FCC', '#fuckyeah', 'tiff', 'frozen', 'Searchable', 'screw', 'One', '#newmusic', 'danger', 'regular', 'vacuous', 'MacWorld', 'GE', 'Cr', \"they'd\", 'Cameron', 'checklist', 'bust', 'I7', 'belie', \"I'd\", '#mcommerce', 'REDCROSS', 'end', 'brightness', 'monger', 'consequence', 'shrink', '#HTML', 'Chinese', 'Scott', '#fxsw', 'Hang', 'group', 'respect', 'Phone', 'Risk', 'ultimately', 'quality', 'asset', '#ismparty', '#bestappever', 'unlocked', 'anticipate', 'Babalola', 'yeaaa', 'midday', '5000', '#rewards', 'hat', 'require', 'ish', \"ipad's\", '#googlekillsit', 'pacman', 'Wear', 'problem', 'buzz', \"That's\", 'cal', 'HA', 'assist', 'EP', \"O'Reilly\", 'tshirts', 'process', 'foreshadowing', 'YAY', 'Man', '#foodspotting', 'sing', 'Scenes', 'attractive', 'Paywall', 'Giving', '32g', 'bring', 'a-ma-zing', 'cooler', 'symbol', 'Charge', '#doubleloser', 'more.My', 'augment', 'Temporary', '605', 'door', 'prime', 'BACK', '#li', 'statistic', 'FlipBoard', 'readability', 'Panels', 'Strategy', '#MarchMadness', 'bomb', 'image', 'leather', 'model', 'sweater', 'Mint', 'AWESOME', 'Independent.ie', '#Apple_Store', 'godsend', 'urs', 'instal', '#twitter', 'Refrigerator', 'appearance', 'em', 'Taxi', \"month's\", 'Airs', 'impromptu', '#notreally', '#elevate', 'porn', 'Jose', 'discus', '#betainvites', 'volume', '#wwsxsw', 'Saved', 'Unimitated', 'eventually', 'thousand', \"I'm\", 'Station', '3.0', 'memory', 'mill', 'monkey', 'edu', 'handy', 'cellphone', '=)', 'Graph', 'OOPS', 'fluid', 'ran', 'worry', 'Crowdsourced', 'PRX', '#backinbusiness', 'DJ', '#TeamAndroid', 'else', '#essdub', 'Anybody', 'IMHO', 'facial', 'Awaiting', '30-45', 'regel', 'posible', '400', 'Radisson', 'formation', \"smartphone's\", '#latism', 'statuses', 'Solving', 'sexy', 'Talib', 'Mindjet', 'chunk', 'magical', 'Sustainability', 'joint', 'Tomlinson', 'principle', 'city-go-round', 'Shame', 'Via', 'interfaces', 'softball', 'Fantastico', 'coffee', 'snazzy', 'hoy', 'huge', 'TAKEOVER', 'Break', 'condense', '#augcomm', \"fin'ly\", 'hewlitt', 'MILITARY-SCIENTIFIC', '#leanUX', 'promise', 'PowerPad', 'wine', '#StayingAlive', 'Politics', '#tonchidot', 'city-market', 'Hooray', 'Medina', '#realtalk', 'Richard', 'crappy', 'Congratulations', '11', 'alternative', 'Couple', 'Engaging', 'Evri', 'Competition', 'cmon', 'crazyfest', 'NotesPlus', '#doingitwrong', 'Republic', 'Mecca', 'Must-Have', '#1315', 'everybody', 'poll', 'tougher', 'pirate', 'experiment', '#privacybootcamp', '106', 'writeup', 'latter', 'Shopping', \"imo's\", 'accomplish', 'expand', 'Dejavu', 'thisisdare', 'barging', 'American', 'hoo', 'domain', 'Juts', 'hershel', 'biz', 'nailed', 'row', 'consideration', 'F', 'Reading', 'seriousness', 'JOEPIEEE', 'compliment', '#mophie', '#ButFacebook', 'MindTouchers', 'WAZE', 'layer', 'widget', 'yrs', 'universe', '#drunkwalrus', '1406-1408', 'Grille', 'Domo', 'South', 'TONIGHT', 'Meyers', 'Writing', 'common', 'GSDM', '#analog', 'Pumped', 'comms', 'mayhem', '10.5', 'volunteer', 'Aisle', 'tap', 'emotional', 'list', 'bracelet', 'parabolico_bh', 'people', 'claim', 'amiss', 'valuable', '#peace', '2am', 'cross-checkins', 'Lavelle', '#LP', 'AP', ']:', 'pizza', '#EatDrinkTweet', 'PATs', '#tattoo', 'Green', 'carry-on', 'documentary', 'Idle', 'nuanced', 'BEST', 'fall', 'minor', '#personalcloud', 'potential', 'apartment', '#score', 'Length', '#sxflip', '#CoronaSDK', 'GeoLoqi', 'Shea', 'imo', 'Bliss', 'Chilcott', 'banality', '#cmon', 'Better', 'Clover', 'office', 'drop', '#Cloud', 'Probably', '#Naomi', '#SanFrancisco', 'foursquare', 'sx', 'Rel', 'iPHONE', 'square-foot', 'BlackBerry', 'Spotted', 'Disgusted', 'Screenburn', 'PSFK', '#', 'Pensei', '360', 'not-so-secret', 'u', 'mq', 'non-Apple', '#App', '#Jobs_Co', 'Hint', 'gram', 'Lynn', 'Speaks', 'Ogilvy', 'Frazier', 'attendees', 'Cactus', 'Europe', 'Positively', '#TheIndustryParty', 'Wine', 'ar', 'Hootsuite', 'Luster', '#iStoned', 'place', 'nerdier', 'cd-player', 'fixed', 'country', '4:45', 'Sweet', 'gran', 'F1', 'plowed', '#jeopardy', 'wilt', 'consolidate', 'Ave', 'burlesque', 'quake', '6:45', '<=>', 'donkey', 'Sun', 'shall', 'review', 'partial', 'Webinar', 'Bands', 'Chicken', 'survive', 'scarf', 'consume', '4sqwill', 'SDK', \"Deathstarr'd\", 'xbox', 'Texting', 'collective', '#duh', '911', 'splash', 'Ideas', 'SIGN', 'iReport', 'hot', '#Art', 'GORGEOUS', 'Newsweek', 'big', 'trump', 'Enjoying', 'database', 'AUSTIN', 'multi-touch', 'Ensues', 'Jeez', 'speaker', 'weasel', 'apt', '#UOsxsw', 'Ap', 'choir', 'Talk', 'sleeve', 'bar', 'wholistic', 'Class', 'Berkowitz', 'Checkout', 'sadly', 'checkins', 'Among', 'Engadget', '#woops', '#Filmaster', 'rm', '#impactdashboard', 'conversion', 'Mind', 'element', 'iSack', '#overheard', 'logic', 'douche.Put', 'creeper', '#GuyKawasaki', 'look', 'Props', 'Anything', '#couch', 'lonely', '#maps', 'Sounds', 'realize', 'mini', 'calendar', 'Sat', 'schtuff', 'autodial', 'epic', 'Pile', 'apple-breeds', 'good', 'nationwide', 'apple-sets-up-temporary-megastore-at-sxsw', 'V.Interesting', 'Google', 'Realtime', 'god', 'beat', 'Jaqueline', 'microformats', 'Terms', 'geekest', 'validation', 'pagerank', 'Jerk', 'fictitious', 'barbarian', 'becomes', 'LaunchRock', '#biz', 'kilt', 'Shawn', 'west', 'To-Do', 'Natives', '#notsurprised', 'constantly', \"else's\", 'Swiss', 'LiquidSpace', '#necro', 'Scored', 'pecan', 'grows', '#uncategorized', '8', 'updated', 'Starting', 'authorization', '#smtravel', 'juiced', 'wife', 'cult', 'Explorer', 'Wam', 'qualcosa', 'Products', 'pp', ':-\\\\', '#notionink', 'haul', 'Heavy', 'Laura', 'iPhone-in-toilet', 'latte', '169', 'SD', 'Designer', 'Timely', 'Move', 'Gay', 'Police', '#aspengrove', 'portfolio', 'inability', 'grabbed', 'great', 'Coverage', '#blogger', 'Toddlers', 'boyfriend', '420', 'true', 'Crap', 'BUY', 'record', 'match', 'ship', '#enchantment', '#happydance', 'role', 'Food', '#appleATXdt', 'percent', 'Good', 'iPad-made', 'Wings', 'midnight', 'LONG', 'ahead', 'Making', 'Bowl', 'Airport', 'Ipanemas', 'smooshed', '#ewww', 'Inofficial', 'Lacking', 'Adaptive', 'Mullenweg', 'california', '5pm', 'eww', '44', 'OWNS', '#video', 'Behaviour', 'Bizzy', 'omit', 'sort', 'needs', 'Crisis', ':-|', 'loser', 'strong', '#smscifi', 'Rumors', 'ClipCon', 'twilio', 'Monthly', 'Pass', '#offers', 'Blah', 'Eisner', \"#Foursquare's\", 'Sullivan', 'Badge', 'complain', 'strike', '#ranking', '#JobsCo', '#QRCode', 'own', 'lover', 'Gyllenhaal', '#Documentally', 'mashable', 'Crowd', 'autographs', 'kidnap', 'Ride', 'webkit', 'MP3', 'copying', '#TakingBack', 'mess', 'pull', 'taker', 'suffer', 'locker', 'lock', 'chug', \"o'reilly\", '#vCards', 'ten', 'African', 'ZLF', 'Tweeted', 'brilliant', 'Refreshing', 'region', \"it'll\", 'designer', 'Winner', 'sale', 'Clear', 'mount', 'Groupon', 'Generous', 'stillman', \"mae's\", 'redesign', '#digital', 'dancin', 'simultaneously', 'wifi-only', 'grille', '#sm', 'drool', 'Cult', 'Safe', 'noes', 'nut', 'Companies', 'Parakeets', '#googleio', 'Controlled', 'cautiously', 'vote', 'Swisher', 'TRON', 'unusual', 'Creo', 'improvemnt', 'Avoid', '#GitChoCocktailOn', 'route', '#saveustechies', 'background', 'face', 'workshop', 'Nuanced', 'poster', 'emerge', 'helaas', 'Suggestions', '#beluga', 'fragmentation', 'Juwan', 'amuse', 'Grant', '#DeferoMobile', 'Danny', 'newly', 'Stoked', 'explain', 'Missing', 'impressed', 'coolhaus', 'Well', 'race', 'Videos', 'Walmart', 'heute', '#wakeNbake', 'Clearly', 'Internets', 'tea', 'Bloody', 'HTML', '#psychedthough', 'tunehopper', 'prior', 'Whatcha', 'Slides', 'idol', '#downloaded', '#EastSidePies', '#accordion', 'capability', 'yield', 'send', 'Sheer', 'chance', 'Hurricane', '#Austinbusiness', 'Pop-POP', 'Pres', 'Seen', 'gas', '#discotalk', 'detour', 'goal', 'transport', 'price', 'content', 'Christmas', '#tapworthy', '#nerd', 'Amen', 'Macbook', '1k', 'ecodriving', '#bjdproductions', 'convo', 'Search', 'takin', 'Dope', 'Fanbois', 'hate', 'Keeping', 'LEH', '#LBSeverywhere', 'Period', 'Um', \"Shakespeare's\", 'immersive', 'Ughhh', 'borrow', 'clever', 'Special', 'ummm', 'WANW', 'Frankeninterface', '#iQlab', 'Yep', 'omg', 'thats', 'Listening', 'strap', 'Heading', 'Groups', 'crackberry', \"Travolta's\", 'Wired', 'ultralite', 'Research', 'morning', 'penetrates', 'really', 'Connected', 'Reassured', 'trampling', '97', \"iPad's\", 'SnakeHeead', '#fh', 'Software', '#socialart', 'Joins', 'Challenge', '#startups', 'cursor', 'ding', 'ski', 'buffalo', \"O'goody\", 'Interactive', 'opposite', 'NewTrent', 'euphoria', 'aftrnoon', 'Sky', 'Bout', '#flashmob', 'rim', 'pop', 'iron', '12:00-', 'Sooo', '#American_Statesman', '#iwantacr48', 'Chelsea', 'serve', 'correct', 'Tristan', 'would', 'Grab', 'iRelay', 'functionality', 'Say', 'engagement', 'skip', 'timeline', 'EVO', 'cluttering', 'org', 'certificate', '40min', 'zazzle', 'Practical', 'Pearl', '#corporate', 'digi', 'VIDEO', 'Walston', 'moderator', 'english', ':)', 'sock', 'Al', 'woohoo', 'downtown', 'iPhone.workin', 'revolution', 'REKS', 'financially', 'dig', '#uselessobservations', '21', 'republish', 'enthusiast', 'event', '#seo', 'SM', '#zaarly', 'Factor', 'SAMSUNG', 'Woods', 'location-aware', 'Cue', 'trackpads', 'il', \"Apple'd\", '#GoogleMaps', 'selection', 'whiteboarding', 'Cloudkick', 'Verne', 'Sure', 'PureVolume', 'Gram', 'SxSWi', 'Problem', 'Always', 'throwin', 'Plan', 'Wandered', 'Replacement', '#FreeMusic', '#ScreenFuture', 'Offline', 'solar', 'BAD', 'impediment', 'GENIUS', '-->', 'Coyne', 'excel', 'Major', '#Mobile', 'Kindda', 'Literally', 'Harnessing', 'Gecko', 'Techno', 'mini-maglight', 'Sinclair', 'Austin-based', 'yes', 'darker', 'organise', '#videogame', '90', 'Hiring', 'SouthBy', '#iPod', 'Marisa', 'shock', 'likeability', '9-14', '#vizthink', 'ughhhhh', 'team', 'dilemma', 'Video', 'nih', 'Thomas', 'vintage', 'Exciting', '3/15', 'opener', 'fun', 'perk', 'education', 'Wisconsin', 'conquer', 'subsidize', 'task', 'Vegan', 'brother', '#partylikeits1986', '#metricsdrivendesign', 'warmth', \"Coyne's\", '#bolinsxsw', 'Ton', 'Hard', 'Barbarian', 'worst', 'product', 'migrate', 'Knock', 'heart', 'coin', 'usage', 'podcast', 'Begins', 'ringing', 'address', 'Jadi', 'Evil', 'sherry', '45bil', 'klick', '#Gowalla', 'glad', 'gain', 'rag', '#todo', 'inane', 'visual', 'White', '3:59', '#Flipboard', 'synthetic', '#gadgetoverload', '#Ohio', 'lent', 'Donate', 'Khan', 'impact', '#zappos', '#gamesfortv', '#googlecircle', 'SI', 'comp', 'shindig', 'Hah', 'Cuts', 'underestimate', 'pls', 'durable', 'Summit', 'tore', 'Badgeless', 'OUTS', 'reveals', '82', 'fellowship', 'Adfonic', 'headlong', 'Dear', 'SE', 'Way', 'pak', 'autocorrected', 'effing', 'annuncia', 'AdWords', 'Visa', 'BC', 'Person', 'Control', 'felt', 'Buttons', 'head', '#smaroi', 'LAX', 'tournament', '16GB', 'harrymccracken', 'tmrw', 'sxswpopup', 'boxee', 'cart', 'plenty', 'weekend', 'tempting', 'scoop', 'site', 'Project', 'LETS', 'toy', 'Vevo', 'Developers', 'Scepticism', 'serendipitous', 'Resist', 'Elliott', '#cmsxsw', 'higher', 'cntr', 'MDW', 'another', 'Dwnld', 'amoral', 'Rest', '60', '#bettercloud', 'patio', 'friend', 'Reubens', 'fart', 'Refresh', '210', 'Forth', 'Traveller', 'Mekong', 'Afraid', '#Li', 'plane', '#tsunami', 'Give', 'holy', 'TOP', 'daylight', 'J', '#smvis', \"user's\", 'Re-Tweeted', '#AndroidSXSW', 'intern', '#pcma', 'Flava', 'leNewz', '#dorkinout', 'album', 'recognition', 'Used', 'Dropped', 'nfc', '#notpouting', 'Yay', 'augmented', 'jetpacks', 'exists', 'goddesses', 'yeah', 'define', 'sta', 'via', 'besides', 'tech-savvy', 'unit', 'Schmidt', 'announcement', 'Enjoyed', 'Kinda', 'premature', '#jzsxsw', 'discuss', 'Dialy', 'rank', 'Text-To-Speech', 'Sugar', '#aclu', '#BillLee', 'TTYE', 'theme', 'collect', 'Takin', 'Site', 'makin', 'literally', '#whowillrise', '#letshookup', 'rental', 'Offers', 'badass', 'Bday', '#sxsurrogates', 'edit', 'awareness', 'blueray', 'Cpmpetition', 'driving', 'covet', 'delayed', 'Interface', 'Surui', 'frineds', 'non-iPhone', 'bro', 'chng', 'shout', 'Seated', 'Volunteering', 'photograph', 'Collections', 'Easier', '!', 'Confession', 'documented', 'autograph', 'Wed-Mon', 'Structures', 'Conv', '#amhealth', 'appy', 'att', 'survival', 'Wednesday', 'unveil', 'knew', 'grape', 'Hoot', 'Daily', '#thefutureismobile', 'cant', \"#4sq's\", 'Salesperson', '5am', 'Systems', 'bo.lt', 'choice', 'Luckily', 'spam', 'battery', '3g', 'HotPot', 'Hits', '#YourMom', '#BPM', 'Whoops', 'Camera', 'plant', 'wipe', 'Fest', 'stun', 'Helping', 'brick', 'Life', 'essentially', 'disneyland', 'Dashboard', 'Gruber', 'envisions', 'Precautions', 'Squeal', 'db', 'Gone', 'visitor', 'Calvin', 'happy', 'recollection', 'totally', 'abroad', 'Impressed', 'Warner', 'tech', 'induced', '#1422', '#iOS', '#SolLipman', 'usar', 'room', 'funny', 'overall', 'Cases', 'eBay', 'Oscars', 'hair', 'radius', 'Almost', 'biyt.ly', 'TON', 'co', '#BxBQ', '250', '#blogging', '#Curious', 'affirmative', 'train', 'ply', 'salon', \"They've\", 'ware', '#Bill_Murray', 'coolaid', 'Jayden', 'playlist', 'ebooks', 'Sound', 'warn', 'Zagg', 'passing', 'HUZZAH', 'Niceness', 'provide', 'rhubarb', 'rise', '#circles', 'envy', 'equal', 'l8er', '1.6', 'Possible', 'pal', 'bloggable', 'maven', '#toodamnlucky', 'Drink', 'Mobile-First', 'flaw', 'although', 'latin', '#tweethouse', 'RANT', 'Hottest', '#AAPL', '99', 'Location-Based', '10am', 'haz', 'golden', '#Logo', 'Febuary', 'goofy', 'Designed', 'Memolane', 'Wordpress', 'webby', 'software', 'Parties', 'Cocktail', 'diferencia', 'vodka', 'ecosystem', '4chan4eva', 'swift', '#worklifeprogress', 'brightens', 'empower', '#ogilvynotes', 'Tweetcaster', 'hardcore', 'Sam', '#brainwashed', 'DMI', 'GoogleWebTeam', 'barely', 'winelibrary.tv', 'service', 'ChartBeat', 'supersede', 'crafty', '#zazzlsxsw', 'knob', 'way', 'Z', 'amid', 'Annoyed', '#ChrisMessina', 'tradeshow', 'pack', 'timechange', 'acc', '#NoWhammies', 'iconbuffet', '#DgtlTribe', ':-D', 'Barry', 'trace', 'iPhone', '#iphone5', 'anticipation', 'ht', '#dickbar', '#twnp', '#SelectionSunday', 'become', 'hop', 'Eye', 'formula', '#socbiz', 'Need', '#Monster', 'FORM', 'longer', 'living', 'iPad-formatted', 'childhood', 'near', '#iHearNetwork', 'worldwide', 'LEGO', '0', 'Updates', 'Flaming', 'interact', 'Appears', 'childish', 'commuter', 'Announcement', 'marathon', '#agnerd', 'bit', '#snapgoods', 'watch', '#BRK', 'blogger', 'Jeanne', 'Staring', 'hidden', 'specifically', 'X-Ray', 'FORWARD', '59', 'inde', 'brown', 'old', 'Ning', '#zaarlyiscoming', 'anyway', 'publish', 'Lunch', 'sad', 'Dude', '#tmobile', 'Flipboard', 'innacurate', 'studio', '98.5', 'anywhere', 'curious', 'Trajan', 'War', 'Preview', 'Creative', 'anti-theft', 'wankery', '#TWIT', '#sloanxsw', 'gasp', 'Workspace', 'Maybe', 'APP', 'Pro', 'Session', 'Hyatt', 'truth', 'backyard', 'Co-Founder', '#SXFL', 'castle', 'Rapids', 'Apps', 'pariah', 'LDA', 'tmr', 'one', 'email', '#NFC', 'aun', 'habit', 'issue', '#Lame', 'tummy', '#shareable', '?', 'deviantART', 'eff', 'venue', 'buck', '#madmen', '#Japan', 'journalist', '#geekery', 'group-aware', 'handsfree', 'within', 'wknd', '#ToyJoyBuy', 'Reid', 'Clarity', \"Dan's\", 'passenger', 'concern', 'Shuttle', 'asap', 'Reactions', 'Ipads', '#wiiings', 'Mandy', 'Speaking', 'introduce', 'Ubuntu', 'community', 'safari', 'MindTouch', 'Enjoy', 'sweeeet', 'operator', '#fanboy', 'Marissa', 'win-win', 'mary', 'usurp', '#savebrands', 'dong', 'Mapquest', 'Loving', '#GoogleDoodles', '#Kawasaki', 'Presents', 'Arab', 'ups', '#party', 'r', '#gadget', 'Bay', 'GEEK', 'cks', 'Existential', '#local', 'Stumbled', 'parking', '#conservebatterylife', 'Axsure', 'Nook', 'butt', '#VC', '#AccessSXSW', 'onslaught', 'curiosity', 'Ustream', 'selfish', 'usefulness', 'Dali', 'rollout', '#KetchSX', 'past', 'on.mash.to/fHDCfn', 'less', '#hashable', 'Manor', 'force', \"dosen't\", 'OWLE', \"Buy's\", 'mmm', 'peeing', 'object', 'Pulling', '#VIEW512', '#attsxsw', 'IAVA', 'ARwords', 'flyer', 'woman', 'Whitelists', '#saysshewithoutanipad', 'screen', 'dancing', '#androidsxsw', 'generally', 'act', 'Mashable', 'Enchanted', 'radar', 'pip', 'Bots', 'Friend', 'cocky', 'poo-poos', 'MAKE', '#UsGuys', '75-80', 'eBook', 'disagree', 'suggest', 'dah', 'Ferriss', 'Orin', 'influencers', 'shell', 'street', 'Fight', 'Dream', 'Webber', 'doodle', 'FISHING', '#webmobile', 'William', 'adapt', 'strategy', 'snack', '#Cab', 'synergy', 'Happy', 'Argh', 'Hold', '3:45', 'unofficial', 'Demoed', 'atx', 'complement', 'bezel', '64MB', 'frame', 'Guguchu', 'skier', 'story', 'jean', 'Bounds', 'roast', 'tablet', 'Schedule', '#atx', 'guild', 'successful', 'Economy', 'Explains', 'Server', 'worried', 'Theater', 'Max', 'retweet', 'foodie', 'Buyers', 'heard', '#actors', 'view', 'Sometimes', 'waterproof', 'ludicon', 'brace', '#hcsm', 'temptation', 'arrive', 'Mr', 'lookalike', 'Includes', 'VIPorbit', '1500', 'conversation', '#mindjet', 'toptweets', 'YYZ', 'chase', '#HELP', \"Amy's\", 'cough', 'Mrs', '#cbustech', '):', 'customizable', 'Os', 'overflow', 'sunny', 'recomds', 'blogged', \"t's\", '#wireless', 'choreography', 'IP', \"year's\", 'Mater', 'Matt', 'non-Mac', 'normal', '#LiptonSXSW', \"n't\", '#random', 'msn', '#Grindr', 'Adele', 'stinkin', 'Classics', 'knit', 'Ups', 'microsim', 'Yikes', 'leider', 'physical', '#protip', 'monopoly', 'family', '#artallaround', 'achieve', 'para', 'Z20', 'Old', \"Clark's\", 'third', 'LLC', 'brand', 'Piece', 'unique', 'mingle', '#StartupBus', 'presence', 'Kindle', 'Ukraine', 'core', 'flock', 'g', 'Bright', 'carousel', 'Learnt', 'batphone', 'strum', 'hackathon', '1,000+', 'neat', 'icanhas', 'unstable', 'slice', '#imanoutcast', 'Belgiums', 'Chief', 'Snotty', 'smelling', 'blogging', 'unless', 'Hollow', 'Mall', 'immediate', 'four', 'permanent', 'um', 'proposal', 'finally', 'paging', 'Head', 'affiliate', 'Airlines', '#midem', '#pcbuzz', 'ears', '#travel', 'Wolfram', 'Trombone', 'Genentech', 'balcony', '#WS', 'expect', 'settle', 'CSS', 'Packing', 'Khoi', 'fill', 'deliver', 'broadcast', 'pun', 'already-dwindling', 'peddle', 'itinerary', '#tmsxsw', 'salt', '#Clevelandsteamer', 'Southwest', '#DARKNET', 'fetishism', '#sleeves', 'warp', 'Ive', 'Toast', '#Bing', 'frabjous', 'Z17', 'Circus', 'socialmediabum', '#AppleATXdt', 'thoughtful', 'decal', 'Ranking', '3:30', 'programmer', 'await', 'homework', 'ACLUs', '#Productive', 'opportunism', 'faster', 'deep', 'tethering', 'though', 'america', 'joke', 'lift', 'dialect', '#photosharing', 'penasaran', 'aggregator', '#socialgood', 'FRIENDS', 'spent', '#donline', 'network', '#wp7', '#iPAD', 'Hmmm', '3000', 'fee', 'surgery', 'Tokyo', 'WordPress', 'mystery', 'trend', 'lag', 'Customers', 'sum', 'bit.ly/g03MZB', 'dash', 'CHECK', 'NASA', 'eligible', '#Friends_Offer', 'Scan', 'bravo', 'Dutch', 'Teach', 'deck', 'invoke', '#beyondwc', 'www.zaggle.org', 'Hour', 'Howard', 'staircase', 'shinmy', 'resolve', 'Chromeo', '#hashtags', 'tidbit', '#smmnextgen', 'SWAG', 'Gamechanger', '16mins', 'DownloadFREE', 'Notetaker', 'Zynga', 'limit', '#dmad', 'Castle', '#eventseekr', 'Omar', 'Uh', 'Bored', 'BRAND', 'iCNNreport', 'Intel', 'Marc', 'apparent', 'PopUp', 'Voice', 'Totally', 'stack', 'outlet', 'demo', 'ditto', '60SecondCrush', 'Booyah', 'Liebling', 'donation', 'Discuss', 'shorten', 'conv', 'health', '#hotwhat', 'dd', 'bout', '#online', 'familiarize', 'Base', 'construct', \"We'd\", 'Advisory', 'exceed', '#greatergood', 'Lasts', '#pc', 'feedly', 'merchant', 'Conde', 'might', 'bubble', '#photography', 'Fake', 'director', \"everyone's\", 'can', '9ABC', 'Evo', 'left-brain', 'Grilled', 'Z4', 'Interviewed', \"Mayer's\", 'never', 'disruptive', 'term', 'CEOs', '285', 'GTFS', 'Jenny', 'Otherwise', 'tomorrow', 'Nokia', 'Phoenix', '#PhoneGap', 'worse', '#Popplet', 'unadulterated', '#junkies', 'Mission', 'Omg', 'Links', 'Uppward', 'Ck', 'craziness', 'anti-schizophrenic', '#featuredartists', 'Crashes', '#winning', 'Animated', 'cream', 'parachute', 'EVERYWHERE', 'phone', 'glass', '#Cyber', 'esp', 'corrects', 'repressed', 'REAL', '#sleepy', 'scepticism', 'spill', '#wave', 'callback', 'Virgin', '9', 'hyperactivity', 'Pops', 'donut', 'Graphics', '#eidt', 'scoping', 'spotted', 'massage', 'live-tweeting', 'mobile', 'awhile', 'tempt', '#Calyp', 'May', \"Ain't\", 'bandwagon', 'bang', 'Artist', 'suckas', 'Trying', 'Podcast', 'prespective', 'Win', 'Medical', 'otherwise', 'SMS', 'Gee', '#Social', 'Paraormal', 'Fanboy', 'caveat', 'writes', 'Child', 'Live', 'tractor', 'popup', 'Road', 'heck', 'time', 'screenburn', 'fo', '55', 'intuition', 'MeetingWave', 'Scoping', 'Busiest', 'impulsive', 'caramel', '#LeanUX', 'Leaving', 'Poopin', 'Sites', 'Saves', 'Francisco', 'mayer', 'whenever', 'Attending', 'peeps', 'mug', 'Doodle', 'Vaynerchuck', 'Stop', '6thish', \"#Android's\", 'SHOTGUN', 'foot', '#DennisCrowley', 'fat', 'auto', 'Gotta', '#help', 'Banks', '#shame', '#bavcid', 'evolve', 'karaoke', 'Heavenly', 'Andy', 'detach', '#FREE', '#news', 'Witness', 'remote', 'droid', '#Lonely-Planet', '35', '#BetterSearch', \"guy's\", 'presumably', 'Scavenger', 'Hollywood', 'occasional', 'Badass', '#psfk', 'umm', 'Boooo', 'el', 'Blood', '#werdorks', 'MADE', 'Extra', 'vid.camera', 'honor', 'ipadtoday', 'VIP', 'noch', 'Planzai', '#vzw', 'Appolicious', '#GroupMe', '#npr', 'Rainbow', 'Fried', 'whats', 'chk', 'Attendance', 'toolkit', 'btwn', 'rotational', '#uxdes', 'Pre', '#client', 'sesh', 'DailyGrape', 'Dahl', '#overtweeting', 'love', 'LMK', 'Congratulation', 'adapter', 'Rad', 'Throwing', 'Standing', 'inclds', 'brought', 'feeling', '#Discovr', 'PdaNet', 'wire', 'rebel', '4am', '#battlela', 'fly', 'Program', 'battle', 'Contests', 'fisting', 'TextToSpeech', 'Cute', 'Dan', 'Telegraph', 'stop', '#Blackberry', 'Messaging', 'recon', 'Pick', 'z', '2B', 'heavier', 'Ajax', '#lovemyjob', '#philanthropy', 'Tablets', 'nigerian', 'Word', 'et', 'producs', 'SteveCase', 'guilty', 'fact', '#makinggamesbetter', 'Anonymous', 'Pint', \"Mother's\", 'Vast', 'insane', 'peek', 'Swarm', 'Discovery', \"Internet's\", 'earths', '#indieauthors', 'Strip', 'concierge', 'HollerGram', 'Govt', '#TheThankYouEconomy', '#gadgetenvy', 'read', 'measurement', 'NC', '#inspiredbyprint', '#eduVC', '#sped', '#AUSxSW', 'quotable', 'scout', 'Strangers', 'able', '930a', 'ton', 'alphabetically', 'Charging', 'fyi', '#netneutrality', 'demand', '#HTXSW', 'Do-It-Yourself', 'Exhibit', 'stringer', 'everytime', 'Extended', 'brushstroke', 'Personal', 'wish', '#Smmnextgen', 'Rofl', 'fr', 'away', 'Yahoo', 'Card', 'high', '#googlesxsw', 'witness', '#HOTSHEET', 'Bracket', 'machine', 'Wikitude', 'age', 'dropbox', \"Organic's\", '#timoreilly', 'communicate', \"Park's\", '#pr', 'alpha', 'OpenBeta', 'panic', 'Go', '#AUS', 'Exogear', ':-)', 'bajillions', 'fellow', 'Augmented', 'explore', 'LCD', 'Submitting', '#yelp', 'THIN', 'Fiona', 'X', '#musiek', 'Sheep', 'transparent', 'malady', '#confusion', 'Shoes', 'conflict', 'pawn', 'ass', 'Simplicity', '#FML', '#BallinOnABudget', 'Christian', 'magic', 'entertain', 'partying', '1415', 'ball', '#Games', 'developer', 'nick', '#AustinCrowd', 'RedLaser', 'fam', 'Points', 'Like', '#Retail', 'submit', 'gala', 'thurs', 'Brains', 'Jones', '#scvngr', 'Badger', \"Who's\", '#6thstreet', 'Z33', 'T-shirts', 'Clipboard', 'Demand', 'firm', 'YES', 'Application', 'wtf', 'www.skylines.net', 'gowalla', '#iphoneography', 'Yorkers', '#Mint', 'Liking', 'journey', 'Snowflake', 'film', 'Missed', 'Gmail', 'Launched', 'pink', 'bloody', 'Turing', \"Austin's\", 'care', '1,000', 'Tomorrow', 'gps', 'Team', 'pass', 'Tuh', 'V1', 'finger', 'advance', 'Cut', 'shoulder', '#experiential', \"We'll\", 'basecamp', 'FC', 'Takes', '<Amen!>', '#SmartCover', 'ping', 'nije', '#fastball', 'stow', 'regret', 'Engine', 'voluntary', 'Quite', '#thankyoueconomy', 'wayfarer', 'Plans', '#cbs', 'loud', 'offline', 'READ', 'exciting', 'Mars', 'Gatorade', 'degrade', 'Event', 'dine', 'Stupid', 'sketch', 'EVER', 'trustworthiness', 'Dongle', 'crop', 'Sigh', 'Promote', 'PagerDuty', '100,000', 'question', '#airplane', 'recharge', 'Doesnt', 'Mist', '2011', \"Bricklin's\", 'nuance', 'hee', 'discontinue', 'AllThingsD', '#firstworldproblems', 'unload', 'bottom', '_and_', 'clear', 'express', '#instaprint', '#bnet', 'iPhones', 'spell-checking', '#nptech', 'Something', '#TCN', 'skewed', 'pubsubhubbub', '#wjchat', 'pukey', 'mission', \"Icenhaur's\", 'responses', '90999', 'Overheard', 'addiction', 'Alert', 'Microsoft', 'Shut', 'Correction', 'bonus', 'till', '#ouch', 'queue', '#wordpress', 'geek', 'fortuitous', 'Push', 'Shout', 'form', '#JK', '#courtyard', 'galore', 'tat', 'take', 'broken', '8am', 'digit', 'MacbookPro', 'moody', 'general', 'future', 'zbowling', 'Adi', 'c', 'search', 'tsotchke', 'JL', 'gauntlet', 'gesture', 'Alas', 'moderate', '#OldSko0l', 'Esp', '#evaporation', 'Meyer', 'conserve', '#peoplefinder', 'frontwoman', '#socialemedia', 'Wireless', 'Glenda', 'bit.ly/pushsxsw11', '#letschangetheworld', 'heating', 'sea', 'Leopard', 'slip', 'papyrus', 'traction', 'housing', 'Amy', 'Super', 'Art', 'Mix', 'Michael', 'difficult', 'MUCH', 'upload', 'Gogo', ':D', \"Finger's\", 'PM', 'invisible', '#rhapsody', \"yesterday's\", '30K', 'sucessful', '#igottagetit', \"Man's\", 'Handle', '75', 'Fell', '4:59', 'Apptastic', 'phew', 'out-markets', 'APPLAUDS', 'Put', 'Woke', 'stoked', 'Pork', 'Connect', '#Hurricane', 'interface', 'MT', 'Flight', 'crowdsource', '#tiesto', 'carry', 'technology', 'Took', '#nowplaying', 'Booth', 'Cheese', '#TT', 'Selling', 'favorites', 'oppty', 'total', '#100tc', 'lax', 'Enchantment', 'WithMe', 'Kool-Aid-drinking', '#novideo', 'Kaiju', 'Spearheads', '#tweetaholic', 'rid', 'appropriately', 'chime', 'Pack', 'Front', 'Wondering', '#il', 'plus', 'Reminder', 'MacBook', 'insanely', 'consignment', 'Critical', 'behind', 'Posted', 'Coming', 'Simple', 'cool', 'Snack', 'awe', '#fastcompanygrille', 'stripe', 'trick', '#itwillbemine', 'insidious', 'penalty', 'adopter', 'Box', 'Nest', 'FullSteam', 'fond', 'Snoring', '#RewardsWagon', 'Still', 'Taking', 'hail', 'NCF', 'rumor', 'BigD', 'microsoft', '#awkward', '#fail', 'Saying', 'jqTouch', 'Staying', 'Job', 'mechanic', 'direct', 'heavy', 'FLAR', 'Jazz', 'ingenious', 'Austinjs', 'little', 'Hacker', 'tiny', 'rat', 'Ad', 'walk', 'BEYOND', 'relatable', 'Hackers', 'jamin', 'gadgetzilla', 'Election', 'controller', '#STPATRICK', 'long-time', 'Agents', 'Actually', 'Signing', 'Terminal', 'awesome', 'beautiful', 'disk', 'autonomous', 'participate', 'cheapen', 'terrible', 'pity', 'bookbook', 'qualify', 'skepticism', 'fleet', 'compact', 'House', 'Beechwood', 'plague', 'Keynote', '#virgin', 'mix', 'lots', 'dark', 'Important', 'Use', '#mVideo', 'hairy', 'Likability', 'plz', 'blast', 'screen-scrape', 'DuranDuran', '#seenocreepy', 'Monday', 'Stopping', 'plugin', 'mention', 'undoubtedly', 'scream', 'Plucked', 'comfort', 'aid', 'invite', '#FTW', 'fast', 'finder', '3/20', 'Recommendations', 'STILL', 'sluggish', 'attendee', 'gmail', 'Successful', 'Ghost', 'meet', 'thwart', 'smilebooth', 'Checkin', 'iPad.Look', 'middle', 'Kiip', 'collectively', 'Startups', 'drummer', 'Prince', 'Huzzah', 'yay', 'among', 'expectation', 'half', 'serv', 'September', 'ehPhone', 'CC', 'next-gen', 'Davis', 'hill', '7,000', '#Startup', '24/7', 'Wrote', 'Umm', '#RebelTV', '#help_sxsw', 'simply', 'line', '5th', '#Camera', 'importance', 'idiot', 'popoup', 'russian', 'unveils', 'gold', 'Tuxedo', 'text', 'cuteeeeee', 'cite', 'Written', 'in-app', 'assert', 'tron', '#drupalcon', 'Symbian', 'lurk', 'Fad', 'Demons', '3pm', 'matter', 'Isis', 'peeked', 'Previews', 'Route', 'wooooo', '#beer', 'stress', 'discriminate', '#constantcontact', 'C', 'IPhone', \"Monday's\", 'lamewad', '#samsunggalaxys', 'Lost', 'WCAG', 'supposedly', '#moreknowledge', 'Exclusive', 'google-zation', '#imaconf', 'completely', 'Chimpit', '#jealous', 'management', 'FourSquare', 'easier', 'hashtags', 'leader', 'N2', 'trade', '@', 'quote', \"ain't\", 'fascinate', \"google's\", 'Rumoured', 'trailer', 'mock', 'Unboxing', 'Thoms', 'GDC', 'social', 'con', 'conceptual', 'Australian', 'Flawless', 'WTF', 'snarky', 'crunch', 'puzzle', 'Final', 'Discovered', 'Ginger', 'Myopia', 'high-volumes', '#coupons', '#googleplaces', 'Wow', 'T-shirt', '#mac', '2010', 'final', 'K', 'insanity', '#gonnagetanipad2', '#SocialNetwork', '#getjarsxsw', '#GermanVersion', '#digitalID', 'fit', 'Bull', 'Midnight', 'sneak', 'pretty', 'BREW', 'surround', '2.4', 'buggy', 'Porn', 'Alright', 'surface', '#mingle', 'Yonkers', 'public', 'carefully', 'Beer', 'plug', 'yoroshiku', '900', 'PayPal', 'convolute', 'Rankings', 'hang', 'Pigs', 't-shirt', 'l', 'static', '#outoftheloop', 'theem', 'platform', 'Duncan', '#begger', '#youneedthis', 'relinquish', 'Eric', 'request', '64gb', 'visto', 'indie', '#swswi', 'RIM', 'lemon', 'ear', 'thx', 'sprinkle', 'tool', '#OMFG', '#nudgenudge', 'mkt', '#classicaudiencemisread', '#Wahoos', 'Finding', '#irony', 'anoth', 'SimpleGeo', 'IPAD', 'portable', 'Staff', 'horizon', 'offering', '#WINNING', 'heavyweight', 'reclaim', 'fest', 'Deathstarr', 'decency', '#spammers', 'complicate', '#Geek', 'FAST', '#hardware', 'Pauly', 'curated', 'data', 'kindle', '#scaletest', 'timesheets', 'ensue', 'AmEx', '#PiDay', 'wrap', 'disability', '#Adobe', 'Hey', 'fare', 'Qs', 'Tasks', '#SMM', 'Houston', 'Tahoe', '#NokiaConnects', 'Planning', 'Therapy', 'Finder', 'California', 'Griffin', 'release', 'oy', 'unnecessarily', 'young', '#digisxsw', '#DiscoveryTalk', 'illustrate', '#topnews', '13.6', 'Definition', '#Companies', 'lazy', 'collab', 'enterprise', '5.0', '#eatshopaustinapp', 'Hackerspace', 'Hacky', 'AGREED', 'Percent', 'airport', '#podcast', 'wed', 'rww.to/f6BCEt', 'Features', '03/12', 'Charlie', 'dictaphone', 'Drive', 'Morning', 'sharpie', 'Announces', 'Announce', '2011/03', 'scanner', 'hmm', 'Hours', '#health2dev', 'road', 'swap', 'Ep4', 'Ten', '#MarissaGoogle', 'finance', 'qualifies', '#EhTeam', 'miss', 'pixelated', 'elevator', 'NYC', 'GIVEAWAY', 'Kara', 'decision', 'amount', 'IPad', 'plunge', 'gotten', 'smartphones', '#rji', 'offchain', 'coochie', 'apple-martin-like', 'Lazers', '#taccsxsw', 'Lustre', 'Pac-Man', 'cornered', 'streamline', '206', 'hint', 'KNEW', 'Blackberr', 'Hunts', 'fanboy', 'SOO', 'Whoa', 'hose', 'Streaming', '#iphone', 'dis', '#Teamandroid', \"iPhone's\", 'Colin', 'Perfectly', 'NPR', '6-8', '#Enchantment', 'need', 'two-factor', 'talk', 'NEWS', 'cigarette', 'Craigslist', 'Charles', 'recommend', 'eric', 'Casually', 'Salon', 'Lovin', 'GLASSER', 'Hehe', 'safe', 'Excited', 'petition', 'owt', 'Blogger', 'Fill', 'Brown', 'episode', '#windowsmobile', 'explanation', '#TheStreet', 'beast', 'Steps', 'Black', 'Switching', 'Diller', 'spokewoman', 'Greaaat', 'severely', 'sched', '#cbatsxsw', 'toys', 'instruction', 'Linux', 'obsess', 'Revamped', 'leverage', 'especially', 'belong', 'indigenous', 'redirect', '48', 'Added', 'person', 'make-shift', 'Body', 'expensive', 'allow', 'WOW', 'ref', '#istache', '#iPads', 'omission', 'something', '12', 'Master', 'write', 'Link', 'encourage', 'misstatement', 'Gowalla', '#NES', 'Lady', 'GG', '#virtualoffice', 'midst', 'Lotta', '#Verizon', 'Cloud', 'cmty', 'ninjafinder', 'rocking', 'shoulda', '2009', 'Huge', 'deforestation', 'nerd', 'ability', 'favorited', 'Starry', 'den', 'Googlegays', '31', 'tried', 'speller', '#nottheipad2', 'Keyboard', 'actually', 'Diabetes', '#pushio', 'style', 'elapsed', '#frugality', 'canvas', 'Ice', 'discussion', 'Thursday', '#sxbrits', 'Cr48', 'Storm', 'Hookers', 'disuse', 'facepalmed', 'shot', 'Bandcamp', '#Android', 'Area', 'woot', 'barrier', 'mo', '6', 'Camp', '12:40', 'smell', 'enchantment', 'featured', 'PepsiCo', 'driven', 'Kweli', '#freemusic', 'must-have', '6.5', 'Happily', 'yeasayer', 'plain', 'NFL', 'China', 'RSVP', 'Journalsim', 'disgrace', 'lord', 'Heatmap', 'Marketshare', 'waste', '#musicindustry', '#Facebook', 'badge', 'participant', 'Blackberry', 'Sketchily', 'virtually', 'campus', '#sony', 'Oooh', 'ridic', '#playhopskoch', 'Filmaster', 'fricking', 'standardization', '#Blogger', 'Academy', 'refine', 'man', 'Petting', 'predict', 'meyer', '3', 'Never', 'Platform', 'audio', '#tbwasxsw', 'attitude', 'Meetchu', 'delay', 'C9', 'pedal', 'j.mp/i41H53', 'lorry', 'math', 'Belinsky', '#holytrafficjams', 'wkend', 'yummy', 'Mozilla', 'uncharged', 'influx', 'hoax', 'Health', 'hardly', 'brief', 'Going', '#News', 'batt', 'work', 'handwriting', \"Gowalla's\", 'Germ', '#15slides', 'hashtag', 'Imminent', 'add', 'south', 'Rumours', 'marketshare', '#Billings', 'Spider', '#looseorganizations', 'douchebag', 'contract', 'Bill', 'mouse', 'seemingly', 'lend', 'Pants', 'chat', 'LAME', 'auth', 'Leo', 'Bose', 'Sosososo', 'earphone', 'Lavaca', 'magnifying', '#meet', 'Avenue', 'dunno', 'Unopened', '#geo', 'reason', 'Cutts', 'First', 'currently', 'cone', 'Monitoring', 'pouch', '#paypal', 'ChopLifter', 'mktg', 'Networks', 'Shotgun', 'nonprofits', 'complete', 'Nikon', 'Kills', 'concrete', 'N37', 'BTW', '#musicviz', '#MeatXMeatWest', 'primo', 'lately', 'Utter', 'booby', '#gooddeed', 'save', '#nerdbird', 'Evolve', 'rent', 'Story', 'Facebook', '#lovemusicapi', '1991', 'liquid', 'husband', 'Educational', \"Siggi's\", 'mask', '#SM', 'Cant', '#edapps', '#GoogleMarch', 'AAPL', 'answered', 'motivator', 'Instant', '150m', 'Horrible', 'dividend', '#AppleApple', '#NYC', '#aPhone', 'hangover', 'tinfoil', 'BPM', 'looong', 'Without', 'nicely', 'bunch', 'Right', 'Star', '#sale', 'wot', '#iPad', 'teeth', 'distance', 'Arboretum', 'multiple', 'alot', 'Design', 'merrier', 'wax', '#seenatsxsw2011', 'EightBit', 'waffle', 'TTS', 'overcrowd', 'shirt', '#cartoon', 'webos', '#weekend', 'hack', '#Free', 'dinosaur', '#geogames', 'juicepack', 'Send', 'Product', 'haircut', 'Summon', \"i'll\", 'rival', 'temp', 'vortex', '#h4ckers', 'tim', 'ladies', 'Damned', 'heavens', 'extra', 'fork', 'quick', 'practice', 'Marketing', '10k', 'rude', 'set', 'Native', 'Hee', 'Americans', 'tutorial', 'Entered', '#GeeksRejoice', 'experimentation', 'Buy', 'COOL', 'Higher', 'creatures', 'Poor', \"Android's\", 'connect', '#Tokii', 'contxt', 'Officially', 'lwr', 'exquisite', '#cnnsxsw', 'brat', 'logos', 'Releases', '2h', '#CatPhysics', 'Jobs', 'htt', 'eg', 'meat-pointer', 'Touching', '#college', 'atv', 'towel', 'Ready', 'Tidbit', 'borrowing', '#SEO', '#gadgets', 'swoon', 'Fingerprint', 'Notes', 'Hitlantis', 'Choir', 'burner', 'Revenge', 'device', 'inspiration', 'concentrate', 'Rim', 'Player', 'small', 'Recap', 'error', 'exp', '#androiddev', 'pigfucker', 'impedimenta', '#channels', 'freeze', 'FlairBuilder', '#Yelp', 'belt-clip-gadget-holster', '#HNWSXSW', '#TechCrunch', 'Boomers', '1:15', 'Remember', 'Loved', 'Dynamic', 'anything', 'bank', '4sq', 'Jet', 'Group-texting', 'mirror', 'Hello', 'Cedar', 'portal', 'SUCKS', '#values', 'filling', 'may', 'vp', 'Siva', 'landmark', 'iMacs', 'THINGS', 'break', '#206', 'Shot', 'embarrass', 'awake', 'disavowal', '#Efficient', 'organize', 'Films', 'Store', '#atl', 'FREE', \"what's\", '#eventprofs', 'Post-Facebook', '9:30', 'worked', 'Chillin', 'seo', 'retreat', 'Underwire', 'categorize', 'woe', 'Bing', 'ex', 'attend', 'grab', 'Mean', 'Ugh', 'Watching', 'Launch', '#SMS', 'SxSW', 'Prototyping', 'token', 'Zip', 'subscription', 'Heh', 'bizness', 'I28', 'listing', 'Wew', 'Tan', 'impulse', \"we're\", 'Curfew', 'eyeball', 'Must', 'dollar', '##Apple_Store', '250k', 'edible', 'meetups', 'oil', 'art', '06:10', 'highly', 'artists', 'Hatch', 'p', 'creek', 'double', 'target', 'hustlin', 'Gearing', 'Naked', 'convinced', '#H4ckers', '#SMCDallas', 'hawt', 'plaid', 'damm', 'douchebaggery', 'shade', 'Brother', 'Soon', 'bastard', 'ipading', 'Solution', 'POP', 'Landed', '#Albany', 'write-off', 'Yobongoed', 'Republicans', 'Plugin', 'describe', 'HIPSTERS', 'bandwidth', '03/10', 'por', '#imrich', 'Write', 'relationship', 'GiveAway', \"who's\", 'Apple', '#AngryBirds', 'absolutely', 'hardware', 'Enough', '#project314', 'research', 'frustrate', '#FH', 'scene', 'SXXpress', 'closure', 'short', 'New', '5hrs', 'Ming.ly', 'footage', 'input', 'instagram', 'shallow', 'interoperability', 'value', 'gig', '#IPad', 'dueling', 'Knocks', 'techies', 'Adams', 'speakermeetup', 'TV', 'lion', 'pub', 'Grouped', 'Allowing', '47', 'launch', 'diller', 'Expecting', 'dow', 'non', '#HBS', 'Organic', '#spiltbeer', 'laport', '512', 'willing', 'Reviews', 'Miss', 'quite', 'Lori', 'Cesar', 'vids', '11pm', 'execs', 'transient', 'stay', 'Case', 'Obi-Wan', 'Current', 'Taken', '#lightnessofbeing', 'sayin', 'Done', 'oversized', 'Yesterday', 'grateful', 'Moose', '#goaway', 'fancy', \"where's\", 'Jurassic', 'Perfect', 'Latina', '#hollergram', 'Rhapsody', 'timely', 'Recommendation', 'Viagra', '70,000', '#cn', 'Attn', \"There's\", 'R', 'graph', 'lonelyboy', 'example', 'earplug', 'overrun', 'Bernard', 'kind', 'hunt', 'gettinng', 'cold', 'world', 'Pulse', 'all-but-confirmed', 'cancer', 'Notifications', 'Blogs', '#Tigerblood', 'hide', 'trashy', 'do-it-yourself', 'Neck', '#estyaustin', 'tom', '#geekbeat', 'giddy', 'reach', 'Fittingly', 'Asst', 'Want', 'charm', 'conveniently', 'Sketchup', 'owl', 'Temp', 'anyones', 'Gary', 'Picks', 'Level', 'secret', 'Hands', 'order', '#superlucky', 'iCrossing', 'Book', '#Ipad2', 'Brilliant', 'aim', 'LA', 'update', 'check-in', 'mp3', '8:', 'rub', 'Interested', 'semantic', 'convince', 'laser', 'relevancy', 'readable', 'Provide', '365PlusMedia', 'jaloux', '#saatchiny', 'PST', 'Law', 'drunk', '##UXDes', 'use', 'Inside', '#yam', \"#ipad2's\", 'bing', 'Nth', 'strive', 'Hot-spot', 'ie', 'darling', 'Constant', 'powermat', 'include', 'Paper', 'Arctic', '#HopeYouDidntGiveHimNone', 'iP', '#Buzz', 'AI', 'Motorola', 'item', 'non-billing', 'Hand-Held', 'unpack', '#iac', 'GO', 'Funniest', 'Come', 'Justin', 'tablet-optimized', 'GSD', 'extras', 'Japanese', 'London', '#HISXSW', 'proven', 'path', 'apply', 'sters', 'fiendishly', 'Lam', 'Cross', 'Paradigms', 'Opportunity', 'Gr8', 'xtra', 'mommy', 'Surprise', 'creatives', 'rabbit', '#randomactofkindness', '#openexhibits', 'Get', 'ago', '#badform', '3/16', 'Ask', 'Xoom', 'tug', 'Pedicab', '#CSS', 'sqft', 'tsunamis', '#NTN', 'CES', 'cierto', 'performer', 'WANTS', 'apture', 'Style', 'deliberately', 'bandwaggoners', 'effective', '#BT', 'VentureBeat', '#psych', '#sxsh', 'Ho', '#jk', 'replacement', 'Bust', 'Wanna', 'eh', '#smccolumbus', 'workin', 'smoked', 'knackered', 'wristband', 'tall-tee', 'super-fast', 'Earthquake', 'Cera', 'tenet', 'afternoon', 'makeup', '#skrugSXSW', 'Lack', 'compels', 'plutopia', 'kudos', 'in-your-face', 'host', 'desktop', 'Pop-up', '#lovesit', 'Mays', '#Texas', 'classic', 'graphing', 'hungry', 'ado', 'DOW', 'Bodyaches', 'twitter', 'Community', 'Intelligent', '#ChevySMC', 'Brings', '#4sq3', 'shit', 'Waiting', '#mccannsxsw', '#hipsters', 'voilement', 'attendence', 'Penneys', 'webmaster', '3am', 'tonight', 'Guess', 'Locals', 'escape', '157', 'Hulu', '#amismarternow', 'motherboard', '#bankinnovate', 'hand', '#bvj', '#moneytoburn', 'go', 'Fusion', 'fanboys', 'geo-hashing', 'onsite', 'th', 'Popup', 'promote', 'Away', 'debuting', 'mercy', 'Find', 'PacMan', '#twit', 'Social-type', 'solace', 'mifi', 'tsk', 'Korine', 'start-up', '#groupme', 'Respectfully', 'run', '#diet', 'indeed', 'Inline', 'Lewis', '#googlecircles', '#sxwonderful', '#CloudCamp', 'recharging', 'Sign', 'spending', 'Bundle', 'Trends', 'framework', 'ackward', 'message', 'egomaniac', 'Melodies', 'op', '81', 'sold-out', 'farm', 'transition', '#incase', 'pagemaker', 'floor', 'compose', 'homie', '#allshare', 'bc', '#brandnew', 'disturb', 'Sneak', 'Nike', 'bound', 'color', 'industry', 'autocorrect', 'Review', 'additional', 'Disney', 'latitude', 'split', 'sport', 'Personalized', 'CHART', '#WIDFY', 'malfunction', 'painfully', 'wise', 'Fuel', 'discs', 'Hhaha', 'Rumor', 'AMAZING', 'fuckin', 'truck', 'Checked', 'una', '#iPad2time', '02', 'Sheraton', 'Gettiing', 'uber', 'layout', 'approval', 'corp', 'Gold', 'ossum', 'dislike', 'Julia', 'Operation', 'Songs', 'Mealtime', '#longform', '#twitterpower', 'Madness', 'Ellen', 'Horror', 'Terror', 'student', 'innovator', 'GOOG', 'squeeze', 'DICTATORSHIP', 'excite', 'Hot', 'Surrounded', 'company', '#mobilephotography', 'bug', 'KILLING', 'Take', 'download', 'sake', 'captive', '32000', '#IPad2', 'Advice', 'Tyson', 'ex-Google', 'mis-direction', \"room's\", '#openbeta6', 'mind', 'ONLINE', 'lol', 'overcome', 'GBuzz-a', 'fell', \"y'day\", 'zone', 'replace', '#att', 'crowdsourcing', 'Arriving', 'everything', 'pray', '#suasxsw', 'delivery', '300', \"He's\", 'Serves', 'usb', '#printisnotdead', 'Showing', '#srsly', 'Soo', 'I14', '1/4', 'deshi', 'distract', 'Steve', '1st', 'woke', 'IPod', '#Mobile_Apps', 'Route-Around', 'Swiss-made', 'flatulence', '#Bettersearch', 'Thank', 'Turnstone', 'Poole', 'AppStore', 'wealthy', 'structure', 'Instead', '#unsix', \"Genius's\", '#guardian', \"Diller's\", 'Square', '#DTas', 'wnt', 'authenticator', 'consumer', 'IMPORTANT', '#GetGlue', 'reprieve', 'Relive', 'Cleanup', 'Everybody', 'Smackdown', 'Mixed', 'DPE', 'struggle', 'novit', 'code', 'Ipad', '#nonprofit', 'implode', 'spend', 'DSLR', 'Proof', 'version', '6pm', 'v2', 'Announcing', 'discoverable', '#doesdroid', 'HI', 'restful', 'donates', 'GarageBand', 'etc.All', 'marketer', \"fan's\", 'charge', '#piday', '#120', '#entrepreneur', 'Essential', 'Fret', 'preparation', '#MoJo', 'hunger', 'Realized', '#comments', 'lie', 'capital', 'redbox', 'AKQAs', 'fessing', 'victim', 'Library', '_really_', 'self-police', 'check', 'guideline', 'Africans', 'sl', 'confession', 'EaselJS', 'Argon', '#measure', 'e-mail', \"Y'all\", 'patient', '#ncbshow', 'fool', 'Fire', 'Heart', 'cuz', 'Lybian', 'MacAllan', 'KIIIIIILLING', 'Discovr', 'material', '#LatinasInTech', 'Extraordinary', 'UK', '11th', 'Also', 'exceptionally', 'marcelosomers', 'accessory', 'Zazzle', 'Tshirts', 'Incl', '#austintx', 'Downtown', 'recos', \"they'll\", 'intricate', '30mio', '#commerce', 'video', '#shared', 'skillfully', 'satanic', 'scary', 'CHEESE', 'intrestin', 'seduce', 'UberGenius', 'Except', 'pad', 'zoom', 'budget', 'Dawn', 'apparently', 'Mac', 'GTUG', 'Yo', 'lounge', 'HOORAY', 'forgot', 'Merger', 'Craps', 'iPad', 'Overload', 'Tv', 'native', 'Outlook', 'Welcome', 'wrapper', '#PopUpStore', '#craftbeer', '#iPhone4', '_r', 'law', 'asleep', '#Freespeech', 'mngr', 'suitcase', 'bb', 'headline', 'boomer', 'LIVE', 'alignd', 'everywhere', 'ticket', '#BlackBerry', '#want', 'clean', \"Skream's\", 'GREAT', 'spoil', 'delivering', 'Behind', 'Seeing', 'According', 'Edition', 'CARTEL', 'LYKWXHPYTERH', 'somehow', 'Craigs', 'login', 'dawn', 'Android', '#911tweets', 'Touch', 'Uncut', 'alert', 'shop', 'lifespan', 'Dj', 'Public', 'SoftLayer', 'smooth', 'Us', 'optimum', 'G2', 'spotlight', '#GDGTAustin', '#lego', 'cross', '#Lightbox_Photos', 'sharer', 'sh', 'effect', 'Josh', 'Geeking', 'Experiment', 'Stellar', 'zap', 'Engage', 'PERMA', '#launch', 'purchase', 'density', 'cat', '#Amex', 'TLDs', 'apptrophy', 'Test', 'm', 'Bought', 'evade', '#facebook', 'redeem', 'iOS', 'sings', 'Overdrive', 'BEAUTIFUL', 'tm', 'believe', 'professional', 'protect', 'n', 'approves', 'Crowds', 'abuzz', 'Symphony', '#saytextson', 'Deliciousness', 'scheme', 'LOVE', 'novelty', '#sem', 'pen', 'billboard', 'stuck', 'Bass', 'flavor', '#1443', \"ya'll\", 'rerouted', 'Went', 'streetview-like', '2honor', 'accompany', 'coincide', '24', 'tsunami', 'fitIng', 'Honestly', 'mister', 'users', 'Turkey', 'bookselling', 'superhappydevhouse', 'gapminder', 'forecast', 'freecreditscore', 'spambots', 'Contactless', 'Stories', 'charger', 'iriam', 'DT', 'Grounlink', 'NFB', 'league', '__', 'Lucky', '#JustMet', 'ihop', '#angrybirds', 'two', '#couchfan', 'crew', 'go2', 'YayRT', 'Ah', '#smileyparty', '#XperiaPlay', '#sxsi', 'Kicking', 'Although', 'Priorities', 'PixieEngine', 'hehe', 'Impression', 'Wohooo', 'Jumped', 'Joe', 'exact', '#newsapp', '15k', 'premier', '3/17', 'identity', 'obsolete', 'Gad', 'reserve', 're-shoot', \"kill'n\", '#peopleflowingout', 'develop', 'tingle', 'Lineup', 'IHOP', 'coder', 'tattoo', '#flummoxed', 'killer', 'Showcased', 'Apples', 'economy', 'nutshell', '2.68', '#livingthedream', 'Andriod', '/8', 'frothy', 'science', 'beer', 'check-in-thing', 'restore', 'Bryce', 'reveal', '#cnnmoneysxsw', 'Q1', 'impress', 'pop-u', '220', 'capitalist', \"app's\", 'PCM', 'emarketer', 'Syncapse', 'Pagemaker', 'Definitely', 'Ignite', 'Center', 'men', '#webos', 'Twittering', 'accelerator', '#pepsi', 'thanx', 'Location', '_Please', 'staring', '32gb', '2', 'sho', 'Sampler', '#CHAOS', '#Cnet', 'ummmm', 'OK', 'slat', 'fix', 'honesty', 'Youtube', 'hotel', 'Picked', '#retail', 'chris', 'standing', 'ridiculously', 'CIRCLES', '#GOOGLE', 'YouTube', 'carpooling', 'Woo', 'Place', 'mic', 'Valley', '#photo', 'Alto', 'EITHER', 'coolest', 'magnetic', 'alumnus', 'Deals', '#freedrinks', 'Http://bit.ly/reword_app', 'Intimate', 'view.io', 'Nothin', 'throttle', 'hoodies', '#killcommunity', 'N12', 'mania', 'map', 'N33', 'wundertablet', 'mom', 'stamp', 'lovely', 'Cohen', 'journal', 'Checkins', 'talkin', 'merge', 'addictive', 'abacus', 'Nooooooooooooooo', 'Putting', '#AT', 'Woman', 'advent', 'Mine', 'Similarily', 'base', 'Frid', 'recently', 'alguien', '#xoom', '#KeepAustinWeird', 'Announced', 'flu', 'bright', 'unreal', 'Hm', 'on-line', '#gamification', 'attempt', 'consumption', 'indoor', 'Brushes', 'PCs', 'la', 'dork', 'solo', 'im', '504', 'BORDERS', 'Lightbox', 'Usage', 'green', \"RSVP'd\", 'Intuit', '#IL', 'difference', '2g', 'Whither', 'everbody', 'flow', 'goiing', \"who'll\", '#WeLiveHere', 'goo', 'cap', 'showcased', 'ness', 'console', 'Noteshelf', 'SocialFlow', 'Pics', '#thankyou', 'THANK', '#redbull', 'swarm', 'ebay', 'Ex', '#HPSXSW', 'Navigating', 'devastate', 'Bigger', '#transitapps', 'Spazmatics', 'Clone', 'huuray', 'rightfully', 'Noh', '#Startups', 'swish', 'Racing', 'strategic', 'geo', 'bulletproof', 'Gogglers', '#socmedia', 'Capitol', 'Syked', 'Compilation', 'slowly', 'Foxconn', 'sleek', '#battledecks', 'photography', 'Contingent', 'international', 'Bah', 'play', 'pork', 'Googleto', 'extreme', 'ANZ', \"here's\", 'gr8', 'duking', 'elitebook', 'bait', 'ti', 'BEARDS', '#womenintech', 'Hahah', 'beyondbroadcast', '#videos', 'Rooftop', 'pile', 'reduce', 'call-girl', 'importantly', '#movie', 'Gaslamp', 'yesterday', 'wannabe', 'STARTING', '#goodcustomerservice', 'Krug', 'permission', 'Middle', 'NHK', 'arduino', 'prove', '4/5', 'certain', 'Corporate', 'workplace', '#FightThePaddle', 'voxpop', \"Kawasaki's\", '#features', 'Wars', 'mantra', 'studentsforcleanwater.org/rsvp', 'ZOMG', 'eod', 'snap', 'Pilhofer', '#rhizome', \"They're\", 'Stack', 'Answering', 'Sundance', 'pressure', 'male', 'Naomi', 'Megastore', 'circle', 'Moore', 'personalize', '85', 'low', 'amazingly', 'new', 'breakout', 'Quotables', 'reporter', '#vom', '12AB', 'wi-fi', 'Launching', '#PHOTOGRAPHY', 'leave', '#gamestorming', 'dry', 'Worth', 'MOMA', 'Bob', 'slower', 'popular', 'Lines', '#nerdking', '6,000', 'hockey', 'huh', 'sans', 'Tired', 'cancel', 'unconfirmed', 'downer', 'government', 'CST', 'Digging', 'perserverance', 'James', 'Crazy', 'Bahahahaha', '#mashhash', 'shipment', 'html', '#t3sxsw', 'soooooo', 'planning', 'muffin', '#geekdate', 'GameSalad', 'toward', 'Entering', 'cricket', 'in-box', 'chinese', 'AICN', '#ipad', 'duty', 'Mostly', 'iads', 'booze', '#Rance_Wilemon', '6th', 'leak', 'Alarm', 'unread', 'failure', '#fast', 'Qik', 'US', 'rooftop', 'HOBO', \"Check-In's\", 'Work', 'Kenya', 'Grand', 'Winamp', '#publicradio', 'every', 'visiting', 'Mistakes', 'intentionally', 'Digital', 'stomach', 'soul', '#Winning', 'test', 'NXDC', 'Qrafter', 'Record', 'detail', 'gaimed', 'creative', 'tchin', 'East', 'Hilarious', 'Aha', 'Protect', 'maybe', '#New', '313', '#musicconnex', 'Teases', 'important', 'Secret', '#socialnetworking', '#Mullenweg', 'mashup', '#GoogleBlog', 'spirit', 'Passed', 'WANT', '#JustSaying', 'Unite', 'overthere', '#iPAD2', 'fave', 'Opened', 'awesomeness', 'mar', 'segment', 'relies', '#YouTube', 'smentisce', '#EduAR', 'consistent', 'Java', 'organically', \"Tomorrow's\", 'mean', 'Loading', '#Atx', 'FrostWire', 'Awww', 'mojitos', '#newsapps', \"Bing's\", 'Learning', 'Childish', 'Korean', 'car', '#donthate', '#designflaws', 'make', 'tremendous', 'Balckberries', '3/11', '#elonsxsw', 'Bank', '#art', 'soll', 'PARTY', 'megastore', 'spec', 'onegai', 'QuiBids', 'EC2', 'Momento', 'Ultimate', 'Flash', 'SXSW', '03/14', 'tomato', 'Gonna', 'copia', 'departure', 'foing', 'DownLoading', 'pc', 'thumb', 'Independent', '#bum', 'Photography', '#outbrain', 'N7', 'roof', \"old's\", 'side', 'Showcase', \"Marcin's\", 'aps', 'oh', '#Austin', 'roam', '#hcinno', 'Echofon', 'Edge', 'Gambino', 'Ahem', 'Roundup', '#lockout', 'rebranded', 'current', 'laptops', 'chevy', 'msc_page', 'def', 'Streetview', 'trust', 'Cleveland', 'Hipstamatic', 'Remove', 'Livetapp', 'fake', '<-', 'bit.ly/awA50', 'Preparing', 'iPAD', 'upgrade', 'Evangelist', '#windowsphone', 'DogPatch', 'BBQ', 'ab', 'topic', \"iphone's\", 'barroom', 'handsets', '#SmileyParty', 'intimate', 'Meetup', 'Wed', '#RichPearson', 'behaviour', '#9to5', 'Gear', \"calendar's\", 'Pete', '73', 'Orly', 'dell', '#BBQ', '#CES', '#wth', 'noticed', '13,000', '2012/3', 'Verifone', 'Mobile', 'boundary', 'sun', 'corporation', 'verge', '#innovation', 'Tnooz', '#powermatteam', 'Kudos', 'Anywhere', 'TIFF', 'DFP', '4', 'probar', 'box', 'CD', '#sxtxstate', 'mustachepox', 'Whether', 'Yeaayyy', '80227', 'Direkova', 'without', 'Top', 'realty', 'whore', 'Brits', 'Mavis', 'earbud', 'cupertino', 'bumper', 'Grindr', 'give-away', 'cinemaphiles', 'HT', '3K', 'Looms', 'Hair', 'ridculous', 'Watson', 'perfect', 'thier', 'individual', 'Articulate', 'Linney', '#crashing', '54', 'Lisa', 'available', 'wakeup', \"SPIN's\", 'Pop-Up', 'examples', 'INDUSTRY', 'mountain', 'urge', 'Wonderful', 'thank', 'UX', ':/', 'Taariq', 'understand', 'Wants', '#nerdcore', 'SKORE', 'burn', '#a11y2go', '#Brian_Lam', '#sxwsi', 'Bars', '<Seriously!>', 'Tried', 'wild', '#w2p', 'separate', '#dfcbto', 'efficient', '#Sonoma', 'divide', 'dangerous', 'tune', '#what', 'Watch', 'Arthaus', 'presenting', 'collaborate', '#BrightEyes', 'I41', \"chortler's\", 'duh', '#Fuckit', 'Docomo', 'solution', '#super', 'HIPSTAMATIC', 'sitby.us', 'Flav', 'alone', 'messaging', 'Song', 'Eyewitness', '7-10', 'Consumer', 'Market', 'Applause', '#epictweets', 'Fri', 'painting', 'Answer', 'pick', '#lines', 'anybody', 'vegitarian', '9th', 'ramp', 'CNN', 'Retail', 'Gayno', 'PicCraft', '#Posterous_Events', '10pm', 'Focus', 'O_O', 'abandon', 'topicality', 'microformat', '#iTunes', 'theory', 'library', 'gent', 'Speech', 'Bringing', 'anti-social', '#numbassonfloor', 'Bluefly', 'App', 'Designing', 'number', '#papasangre', 'textbook', 'veteran', '#SMComedyFyeah', 'singularity', 'St', 'Retrollect', '#comCom', 'Four', 'stuff', 'crazy', 'spazzmatics', 'arm', '#wasteoftimeatsxsw', 'Think', 'openbeta', 'Bastards', 'Call', 'obnoxiously', 'liken', 'Restaurant', 'daily', 'shame', '#Hootsuite', 'Carr', 'feelin', 'bleed', 'dotRights', '#austin', 'detailed', 'waited', 'Within', 'rsvp', \"demo'ing\", 'Yelp-style', 'Jake', 'loving', 'FYI', 'realllllllly', 'Evelyn', 'trialled', 'thanks', 'illa-def', 'buying', '7', '#pushsnowboarding', 'Intro', 'instant', 'Patry', 'npr', 'Longest', 'Scent', 'prompt', 'spanish-speaking', 'shuffleboard', 'Gourdoughs', 'nuclear', 'HBD', 'TIX', 'investment', 'Circle', 'post', 'Austin', 'chic', 'listening', 'ninety', 'pre-order', 'disc', 'convenient', 'Popular', 'self-respecting', '150MM', 'congress', 'celebs', 'Speak', '#Silly', 'Arg', 'beyond', '..', 'Hopefully', 'android-or', 'dont', 'supply', 'bite', 'fully', 'forming', \"mom's\", '#TNW', '38', 'hall', 'Googlers', 'PowerMat', 'commandeered', 'Wallace', 'girl', 'locatn', 'dudes.Just', 'opportunity', 'massive', '#engage365', 'budge', 'Googlebook', 'notebook', 'NYT', 'Matching', 'Offering', 'Irresistible', '#vufinders', '#connectedbrands', 'Might', 'fight', 'Awwww', 'etiquette', 'birth', '#CTIA', 'C23', '#WordPress', 'automation', 'actual', '#agileagency', '20', 'Listen', 'Potentially', '#bettersearch', '24587', 'bookstore', '#fab5', '. ...', '#broadcastr', 'Antonio', 'automatic', 'Third', 'bud', '#thedaily', 'slowpoke', '65', '4Thought', '#bsidesaustin', 'Filling', 'versione', 'Stores', 'Walkin', 'traveler', 'Ze', 'estate', 'Crossing', 'Ser', 'Que', 'affair', 'Chris', 'Seesmic', 'Docs', 'Backpack', '#11ntc', 'college', 'few', '#oldschool', 'Wiebe', '#cmswire', 'toooo', 'Bereft', '#whattechnerdsdodrunk', 'Bateman', 'guest', 'GoodGuide', 'urinal', '#Redbullbpm', 'Co', 'Network', 'Lots', 'integrate', '#EMC', 'someone', 'Opens', 'Conference', 'predicts', 'syncs', 'beg', 'virginity', 'chain', 'cod', '#stumbledupon', 'camera', '#fastcompanygrill', 'attys', 'Poynter', 'move', 'USA', 'Day', 'iDevice', 'Agency', 'actionable', '#honest', \"Gary's\", '#ThankGod', 'improvement', 'breakdown', 'feel', 'DAY.from', \"Soo's\", 'Block', 'Accessibility', 'Currently', 'minivan', 'Gaga', 'Hotpot', 'chief', 'power', '#at038t', '#behance', 'avail', 'Lead', 'Trend', 'goggles', 'educational', '#sgrocks', 'anti', 'Peter', 'Else', 'Amazing', '#archives', 'Shoe', 'business', 'celebrity', 'appreciate', 'rechristening', 'genius', 'L', 'preso', 'destroy', 'Griddler', '#iTourU', 'teem', 'Gate', 'SMART', 'Pissing', 'thedomain', 'Fodder', '#boom', 'eN6P2E', 'confirm', '2.0', 'Housecat', '#getoutthevote', '#followback', 'FIONA', 'Helps', 'Austinl', \"Riley's\", '#Thanks', 'MindManager', 'bus', 'Bring', 'Mo', 'Freaking', 'checkin', 'UJ', 'Thats', '#yourmom', '#pics', 'resume', 'china', 'southpaw', 'Business', 'macbooks', '#latam', 'Psyched', 'recommends', 'nexus', 'parent', 'tank', 'naked', 'wake', 'ex-Apple', 'TM', 'Installing', 'SF', 'advocate', 'Merissa', 'mixed', '#CNN', '#Tech_News', 'stalk', 'coolness', '#ftw', 'jaw', '#contest', '#brand', 'tacos', '. .', 'Released', 'dwindle', '#lxh', '#FB', 'Auerbach', 'belt', 'unfair', 'lead', 'doubt', 'crashing', 'name', 'confuse', '#justmet', 'texting', 'Perimeter', 'headphone', 'automatically', 'lean', 'simplicity', 'Netflix-style', 'compare', 'penguin', 'guard', 'ceviche', 'implementation', 'djroe', '#mashable', 'employee', 'IRONIC', 'suicidal', 'master', 'female', 'recipes', 'lava', 'whether', 'un-Jobs-aesthetic', 'comfortable', 'mistakenly', 'Startup', '#pissedimnotgoingtosxsw', 'Forbes', 'afford', 'Burning', \"tomorrow's\", '3/18', 'swimsuit', '1.5', 'Brave', 'begin', 'DOC', 'Priceless', 'IE6', 'amalgamation', 'meier', '#quake', 'registration', 'Messina', 'Pecan', 'B_social', 'XD', 'conventionctr', 'torrent', 'malt', 'Choice', 'EVERY', 'Ipoo', 'Triple', 'Contextual', 'android', 'Skype', 'exception', 'addr', 'robot', 'entirely', 'situation', '130,000', 'Open', 'panhandle', 'ner', 'flip', 'dub', \"We've\", 'delicious', 'PDX', 'tshirt', 'window', 'dangle', 'clip', '#hpsxsw', 'Notsomuch', 'International', 'premiere', \"Woman's\", 'Carroll', '6hours', \"we've\", '64', 'Wolfenstein', '12B', '#beprepared', 'Meant', 'User', '#DontBeHatin', 'Tweetie', '#environmental', 'gym', 'a.m.they', 'Frank', 'Rainey', '#xwave', '#zlf', 'PS', 'Example', '#Glove', '58,967', '3bil', 'distinguish', 'geogames', 'minimize', '#MSUSXSW', '#addictedtotheinterwebs', '#apple', 'meant', '#AppleStore', '#microsoft', 'Rails', 'Goals', 'Valhalla', '#ui-fail', 'ProTip', 'Paolo', \"let's\", 's2', 'trouble', 'Barton', 'segway', '#tablet', 'famous', 'Post', 'chunky', 'million', 'Geek', 'scrape', '#TheGoGame', 'non-endorsement', 'lucky', 'five', 'Salicornia', 'accord', 'Sadly', 'Vietnamese', 'context', 'Heathcare', '#jwtatl', 'evernote', 'Ever', 'uber.la', '#jmc322', 'Informed', '#HowTo', '8th', 'Austin-bound', 'gaga', '#diller', 'FedEx', '#AppStore', 'creatively', 'EVERYONE', 'citeren', 'websites', \"Tyson's\", 'shoot', 'kiddie', '#mediapers', 'scan', 'already', '#CircusMash', '#technews', 'Porque', 'clumsily', 'donate', \"air's\", '#thingsthatdontgotogether', 'special', '#knowyouraudience', 'personal', '72296', 'drown', '#notenoughplaid', '#techsmith', 'tweet', 'tho', 'purse', 'goin', 'crucial', '#poetry', '#earthquake', 'not-for-profit', 'rich', 'later', 'server', '#crazy', 'restock', 'navigation', '3gs', \"company's\", 'grooving', 'monocle', 'Bryan', 'Solely', 'sticker', 'damn', 'Show', 'amaze', 'Discriminatory', '#integration', '#wack', 'purchasing', '#Recap', 'excellent', 'Security', 'Score', 'seem', '6thst', 'Tool', 'mixtape', 'retweeting', 'Doubly', 'Saving', 'onto', 'pose', 'Catherine', 'unite', 'pickup', 'ona', 'lousy', 'cute', 'Dennis', 'different', 'SWAY', 'GOING', '#crushit', '#cle', 'headset', 'crasher', 'Braille', 'debut', '5-10', 'hacking', 'Atomic', '#tools', '#app', 'uniform', 'iPhone-charger', 'trek', 'mash', 'heads-down', 'Thousands', 'Barely', '#SocialMedia', 'upgrading', 'font', 'd:', 'dynamic', 'Automated', '222', 'windows', 'gun', 'Managed', 'Guys', \"maggie's\", 'Brand', '#youareinteractive', 'retarded', '#cm48', 'unexpected', 'tower', 'Klout', 'shaker', 'mike', 'Victims', 'Tap', 'protection', 'ap', 'HD', 'Geosocial', 'Spin', 'zimride', 'Ahh', 'West', '(-:', 'render', 'b4', 'downstairs', 'Admission', '#empowered', 'summary', 'instead', 'vamos', 'galaxy', 'Verpixelungsrecht', 'Mahalo', 'sbux', 'Efficient', 'boot', 'pickup-line', 'slow', 'Jared', '#nten', 'batshit', 'profit', 'game', 'rage', 'HELP', 'govt', 'quietly', '#ooc', 'Simply', 'Sweeney', 'Git', 'hipster', 'Fogo', 'Status', '#glove', '#urbanmyth', 'Geen', 'smear', 'limp', 'abt', 'sorry', 'Earplugs', '#webvisions', 'Feedback', 'Nadja', 'arrivo', '#social', 'Amer', '#allcomfortfooddiet', '#realestate', 'Essentials', 'Map', 'disable', 'Wonder', 'Evernote', '#prayforjapan', 'conflagration', 'crashy', 'ring', 'ranking', 'setting', 'excited', '#AmplifiedLife', 'Mad', 'svcs', 'BAVC', 'improve', \"Magazine's\", '#musica', 'intro', 'distribution', 'Weather', \"can't\", 'Awards', '5:15', \"#Google's\", 'junkie', 'wireless', 'HOOOOOOOoOoOOOo', 'free', 'smugness', 'Jen', 'smart', 'UGH', 'fondle', 'prepared', 'spilled', 'surplus', 'Decision', '1/2', 'Pan-American', 'Near', '#hesayswithtwohoursleft', 'Spreadsheet', 'surprising', \"THat's\", 'wider', 'IceBreaker', 'W', 'Industry', 'quash', 'congrats', 'succeed', 'Improving', 'Strong', 'Experimenting', 'mullet', 'Zen', '#showusyouricrazy', 'padless', 'Newspapers', 'Groupon-like', 'top', 'neuf', 'Learn', 'jones', 'register', 'grill', '1m', 'Met', 'Stopped', 'niche', 'BD', 'juxtapose', 'scheduler', 'I17', '#VMware', 'reality', 'Hilton', '#QuiBidsWin', '#4hb', 'corrupt', 'teathering', '#rockstache', 'dwnld', 'Childhood', '#Brilliant', 'Variety', 'recognize', 'rely', 'RFID', 'Spark', 'join', 'Verification', 'H4ckers', '#Grrr', 'Liked', '#Droid', '100', 'whiteboard', 'iamtimbaker', 'Earns', 'Mpact', 'ther', 'Harlow', 'tax', '#groan', 'Add', 'Protocol', 'Epicurious', '#bummer', 'MONEY', \"Mae's\", 'Normal', \"Nast's\", 'connection', 'get', 'plate', 'Maurice', 'principles', 'personalized', 'sprout', 'spank', 'Id', '#Seattle', 'Launches', 'Appropriate', 'manager', 'steady', 'Oh', 'crowley', 'shameless', '#Social_Network', 'BroadFeed', '#pnid', 'visuals', 'ritas', '#Apple_Apple', 'cha', 'livestream', 'San', '#sapient', 'podcasts', 'sabotage', '03/13', 'creator', '#store', 'zombie', 'milk', 'showdown', 'Chord', 'desperate', '#TWIG', 'almost', 'leisurely', 'Catch', 'moma', 'Social', '#discovery', 'frenzy', 'analogy', '#1406-08', 'blow', 'Founder', '#WSsxsw', 'together', 'revolutionary', '#scoremore', 'myspace', 'fresh', 'artwork', 'kik', '#prodmktg', 'Balsamic', 'sink', 'Worst', 'Agreed', 'Ordinance', 'recovery', 'pillow', 'skeumorpism', 'Gets', 'Question', 'Mode', 'accessible', 'OSX', 'nerdiest', 'pro', 'opinion', 'note', '350', 'pervasive', 'Hitchery', 'iReports', 'Said', 'Zappos', 'sigh', 'iun', 'copyright', 'soft', 'CTRs', ':-/', '#applefanatic', 'Yelp', '#html5', 'detection', 'novel', 'Wikimedia', 'moment-it', 'Earlier', '#amateurhour', 'Congress', 'Daniel', '#Hotpot', '1980s', 'webfonts', 'fucking', '#touchingstories', 'tx', '#gmaps', 'America', 'Suffer', 'activity', 'yelp', 'Crescent', 'Follower', 'dj', 'internet', 'TightAssTraveler', 'extremely', 'cellular', 'Heyo', 'Pollak', 'shill', '66.7', \"I've\", 'Lego', '#MonksOfInvention', '#babyheadphones', 'yer', 'hawk', 'planner', 'load', 'Tumblr', 'apple', '#springbreak', 'Cards', 'RISE', 'wilderness', 'CrisisCommons', 'high-quality', 'Chatting', 'reward', 'Hate', 'ex-Pentagram', 'answer', 'Scarborough', 'crush', '#Welivehere', 'EFF', '#design', 'sitter', '#teamandroidsxsw', 'complex', '#cloudcomputing', 'bail', 'pre', 'dtwn', 'worker', 'recycle', 'dominance', 'Okay', 'build', 'imago', '#reword', 'hassle', '#FastSociety', 'Feckin', '#DFW', '#GWT', '#notsomuch', 'Wozniak', 'throughout', '#soundcloud', '#tempted', '#reward', 'comer', 'like', 'serendipity', '#apraxia', 'interested', 'showcase', 'diesel', '#FlashSpecial', 'Installed', 'open-minded', 'walked', 'Omaha', 'dieing', 'laugh', '#SWSurrogates', 'Danger', 'tent', '#lbsWars', '#technopriest', 'HUGE', '#geek_games', '#justinjustinjustin', 'tear', '330pm', 'geekout', 'York', '.   ...', 'McRee', '#The_Game', '16', 'CNET', 'Backlight', 'IV', 'Zeldman', 'dat', 'Tweetdeck', 'Campbell', 'overlay', 'Zeiger', 'member', 'delve', '#owllove', 'PC', 'Toy', \"C'mon\", '#dossiers', 'ifrom', 'truly', 'Matthew', 'letter', 'leg', '#TWITTER', 'Slate', 'Aug', 'Twit', '270', 'Drawing', 'evidence', 'Atari', 'non-Google', 'board', 'recall', 'hatch', '19', 'Z37', 'shift', '#web30', 'type', 'impressive', 'success', 'timing', 'print', 'salty', 'smut', 'zero', 'SOMETHING', 'bit.ly/eA1zgD', 'browser', 'TVontheradio', 'Terp', '#dyac', 'semi', 'lack', '#Taplynx', 'outdid', 'Umshini', 'SHATTER', '#Chaos', '#rhjr_ux5', '#eurorscg', 'Spy', 'schedule', 'N32', 'speech', 'Hotspot', 'Jacinto', 'proliferation', \"sync'd\", 'button', 'acerbic', 'tweegos', 'apart', 'Sq', '#allhat3', 'tyson', 'Upload', 'classy', 'V3', 'frm', 'Development', 'webmail', 'Logo', '#soccomp', 'blame', 'Tattooed', 'constuction', '#Pakistan', 'temp-apple', 'bury', 'guy', 'reader', 'overload', 'Sprint', 'Andoid', '#sloansxsw', '#Rackspace', 'tht', '#AppSavvy', 'occasion', 'rip', 'sessi', '4square', 'history', 'LOST', 'tweeps', 'jump', 'wonder', 'ipad-designing', 'stranger', 'piercings', 'www.pep.jobs/upc', 'Freak', 'Already', 'activation', '5-7', 'sister', 'legal', 'Franco', 'writin', 'Woah', 'Netscape', 'Field', 'Zinio', 'centre', 'runaround', '#telling', 'Adam', 'rumour', '#Mashable', 'co-founder', 'IE9', 'thinking', 'Shorty', 'Im', 'mapquest', '#DickBar', '#myprototype', 'circumference', 'admire', 'whole', 'east', 'SuperHappyDevHouse', '#ontologists', '#libya', '#mktg', '2rd', 'lego', 'horde', 'Holla', 'tues', 'Found', 'counter', 'I-Pad', '69', 'marissamayer', '#smartphones', 'Games', 'id', 'Hawk', 'Redcross', 'offer', \"#ARM's\", '#monopoly', 'jQuery', 'rock-steady', '3/12', 'brilliance', '#agchat', \"Dawn's\", 'COME', 'coordinate', 'Japan', 'Plancast', '#nerds', 'TDG', 'Github', 'itunes', 'smittys', '#wssxsw', 'informed', '#alwayshavingtoplugin', 'accuracy', 'Danfung', 'Boy', 'Mister', 'integrates', 'calculate', 'genetic', '49.99', 'skinny', 'richard', '#notwinning', \"Gold's\", \"u're\", 'Try', 'Brian', 'Transit', 'Fortunately', '#Want', 'Ringo', 'marrisa', 'LUV', '10:45', 'Attended', '#Social_Search', 'Z6', 'boo', 'Registration', 'Detroit', 'illegal', 'ipads', 'cam', 'Hearing', 'koblin', 'Cicles', 'lake', 'gross', 'knowing', 'exploit', 'bicycle', 'Sack', '#futuremf', 'Filmakers', 'aware', 'pay', 'smackdown', 'Postmark', 'campaign', 'better', 'central', 'as', 'Ts', 'KEK', 'shopping', 'activate', 'online', 'Ouch', 'exclusive', 'delegate', 'Dbag', 'antique', 'tire', \"everything's\", '#edtech', '#Circles', '#Thx', 'fire', 'Brush', 'hr', 'expansion', '#adam', 'onsulting', 'nerdy', 't.co/v7jWeKN', 'Web-page', 'alarm', 'luggage', 'ChromeOS', '#musik', 'Burn-B-Q', 'Labs', \"front's\", 'Destruction', '#GoogleDoodle', 'partake', 'Inevitable', 'True', 'Path', 'boil', ':-(', '#LOVE', 'Told', '#mobilefarm', 'puppy', 'Makeup', 'Cashmore', 'extraordinary', 'formerly', 'Xperia', 'bull', 'Tron', 'Gallery', 'enable', 'sight', 'diss', 'Ceder', \"they've\", 'canadian', 'Winners', 'logo', 'pearl', 'beef', 'Works', 'Retreat', 'Records', 'smack', 'txt', '#lp', 'tv', 'Birthday', 'John', 'Brought', 'practitioner', '#filmaster', 'blog', 'Install', 'States', 'Late-night', 'crowd', '#trending', 'mall', 'Porting', '#the_daily', '#vets', 'Pocket', 'fluffertraX', 'icm', 'bootleg', 'Plus', 'boost', 'keg', 'RecycleMatch', '#scrm', 'Register', '#widfy', 'WTH', 'weight', 'inbox', 'shape', 'gd', \"gold's\", 'heat', 'HipstaPaks', '#girlcrush', 'involve', ';P', 'courtside', 'Halftone', 'alcohol', 'Shouts', 'WT', '#gofigure', 'APROVEITEM', 'readership', 'Check-ins', 'Bad', 'brewing', 'dataviz', 'Despite', 'Omega', 'pant', 'haystack', 'implement', 'Spreading', 'perform', 'ultimate', 'festgoers', 'net', 'menu', 'skiiers', '#TeleTubbie', 'Picplz', \"i'm\", '#education', 'Newsbeast', 'Jon', 'SHITE', 'Au', 'acrosse', '#digitalluxury', 'anymore', 'mapper', 'bike', 'reconsider', 'designing', 'hahahahah', 'mover', 'FlyPost', 'schwag', 'ReadWriteWeb', 'Cluster', '#foursquare', 'ridicule', 'Rec', 'lighter', 'Techies', 'Warwick', 'command', 'lat', '#nisxsw', 'Sr', 'Commons', 'Built', '3.0-', 'perception', 'SONG', 'injury', 'Spending', '#samsung', 'Asddieu', '5507', 'Losers', '#BetterThingsToDo', 'Home', \"i've\", 'Historical', 'Kill', 'desire', '_pper', 'raffle', 'non-Macbook', 'ghost', \"they're\", 'Check-in', 'gummy', 'FestivalExplorer', 'allen', '#iPad2', '3323414', '33', 'Geeks', 'probably', 'Buying', 'MAC', 'Machine', '@mention', 'skull', 'earlier', 'culinary', 'Hipsters', '188', '#iear', '#Echo_Nest', 'keep', 'Thereby', 'premium', 'perfectly', 'guess', 'drip', 'Dickish', 'fade', 'mouse-click', 'pushed', 'Bummed', 'control', '9abc', 'Packed', 'pm', 'Featuring', 'monitoring', '#postPC', 'manual', 'objective', 'Strange', 'iPhone-onset', '1850', 'Mapped', '#Sonos', 'soon', '#Gogo', 'Exploring', '#PAXEast', 'Hotel', 'aka', 'Awesomeness', 'x', 'Salt', 'Mayor', '#books', 'te', 'Maps', 'doc', 'dandy', 'Egads', 'influence', 'giveaway', 'Handler', 'moleskin', 'area', 'critique', 'presentation', 'consumerization', 'meanness', 'earbuds', 'Julian', '#cupcake', '#tradeshow', 'copper', '#busy', 'Dont', 'sci-fi', 'Blinksale', \"apple's\", '#casualeducationgames', '#CBatSXSW', '98', 'BizGym', 'Roadie', '#newTwitter', 'creep', 'undr', '#SteveMartocci', 'average', 'Customization', 'Somehow', \"Someone's\", 'Week', 'Searching', 'overblown', 'Presented', 'Prods', 'Fascinating', 'P-A-R-T-Y-I-N-G', '#contentrules', 'course', 'lose', 'ready', 'quarantine', 'Sacca', 'bit.ly/i41H53', 'Rolling', 'offsite', 'periscope', 'brooklyn', 'regularly-scheduled', '#localmind', 'Filming', \"WHAT'S\", 'resurgence', '#geekout', '#groupchatapps', '#fmsignal', '#ARSXSW', 'Looking', 'groupon', 'except', 'Foodspotting', 'manufacturing', 'Furby', 'Democracy', 'Aus', 'Hero', 'Read', 'MicroSIM', 'folk', 'Leave', 'Lanyrd', 'positioning', 'Former', 'Category', 'brain', 'furniture', 'appointment', '#dotco', '#kirkus', 'Killer', 'HARD', 'Marty', '#Bizzy', 'fanbois', 'Update', 'Limited', 'Counting', 'woefully', 'prototyping', 'savvy', '12bn', 'obsession', 'sleep', 'casbah', '#socialmedia', 'Close', '#Jealous', 'paperless', 'helpful', 'wif', 'interaction', '#bing', 'Carlson', 'debate', 'NICE', '#NerdsUnite', 'Tribes', 'WSJ', 'Formula', 'twin-girls', 'popups', 'google-wannabe', '#RISE_Austin', 'system', 'jinx', 'Jules', 'Gawking', '7,200', 'Grape', '#PM', '4:00-', 'twitpic', 'land', '7:59', 'Enuf', 'Poke', 'consumer-facing', \"It'll\", 'reacquaint', '40', \"TV'ed\", 'O-FaceTime', 'find', 'programming', 'close', 'SW', 'SNAP', 'give', 'announce', 'Angel', 'downloadable', 'also', 'speaking', 'physicians', 'Unless', 'Andro', 'least', 'payin', 'darn', 'significant', 'feature', 'chalk', 'point', '65.4', '4me', 'irrelevant', 'garage', 'Outkast', 'h', '#contextclues', 'Stats', 'monitor', 'Blackout', 'tcrn.ch/eB5fjs', 'random', 'promotion', 'Ran', '#lunatik', 'ft', 'stick', 'spread', 'Would', '#ceokidschat', 'NYTimes', 'netbook', 'heading', 'digital', 'mob', '1413', 'Unveiling', 'utilize', 'AOL', 'Hanging', 'cow', '#HootSuite', 'EST', '#WebProNews', '#TWiT', \"distraction-I've\", '#popupshop', 'gotta', 'Excitement', 'word', 'argue', '78', 'GPS', 'Chilling', 'engineering', 'Exactly', 'outside', 'appeal', 'Wifi', 'nutter', 'Pair', 'Sketch', 'insatiable', 'un-RSVP', 'MP', '3GS', 'mouth', 'Promo', 'book', 'Long', 'R3CONF1GUR3D', 'DELL', 'signup', 'Ouch-hiss-hot', '#savesxsw', 'WHRRL', \"Ridley's\", 'kidney', 'Donations', 'slide', 'ftw', 'Spotify', 'sid', 'Sony', 'performance', 'carrier', 'jailbreak', 'Hungover', 'Websites', 'Indie', 'yourselves.Seems', 'Engagement', 'baller', 'Alex', '#bike', '437', '#endorsement', 'Ne', 'WORK', 'anyone', '#fsw', 'iRadar', 'cocoon', 'awesomely', 'Time', '#lmndst', 'storm', 'learned', 'horrendous', \"that's\", 'SFO', 'gear', 'Wii', '#popPOP', 'Grrr', 'Tuesday', 'geeks', '#allhat', 'Direct', 'mental', 'swag', 'movie', 'Mexican', '#LonelyPlanet', '36', '#check-in', 'Check', 'Asked', 'cry', 'Bloomberg', 'Toting', 'drag', 'Album', 'Blagojevic', 'innovative', 'linux', 'Based', 'Debating', 'Half', 'eminent', 'Early', 'cnt', 'sin', 'mayorships', 'case', 'diff', 'Geo-Social', 'Table', 'Epic', 'nur', 'breakfast', 'succinct', 'alternate', 'obviously', 'Curve', 'PengAirborne', 'Mexico', 'Rinna', 'Envisioning', 'dedication', 'freebeernear.me', 'da', '6ab', 'somewhere', '#TheGame', 'gettin', 'Controlling', '#woohoo', 'fedora', 'Moment', 'roaming', 'do', 'ate', 'hi-tops', '4pm', 'promo', 'sidewalk', 'birthday', 'MBP', 'Goal', 'Machines', 'Breakout', 'STORY', 'browse', 'Living', '5,000', '#Vegas', 'terminal', 'plush', 'Retro', 'Tech', 'demos', 'cntxt', 'wannabe-hipsters', 'personality', 'whatever', '#FACEBOOK', 'Floor', 'open', 'webex', 'Surprised', 'Gun', 'Thurs', 'compete', '#AUSXSW', 'Foosspotting', '#teamandroid', 'Speakeasy', '#thinmints', 'Nicer', 'Pan', 'Instagram', 'Trinity', 'index', 'Obs', 'Roll', 'market', 'Reason', 'scheduling', 'Showcasing', 'risk', \"we'll\", 'menia', 'press', 'partner', '#popupstores', 'UMSHINI', 'Legacy', '3.30', 'yahoo', 'Various', 'Stone', 'shpeal', 'tha', 'sprinkler', 'one-to-one', 'IMP', 'Cc', 'trigger', 'Spasmatics', 'Bounced', 'Douche', 'Hollergram', 'smoke', 'presos', 'showcases', 'mustache', 'Pictionary', 'web-browsing', '#checkin', 'Specimen', 'Z40', '#steamy', '2it', 'braille', 'Worlds', \"wife's\", 'batch', 'outa', 'glowstick', 'vicariously', 'michael', 'Truther', 'FTP', 'ver', '#hipstermuch', 'hotpot', 'water', 'k', 'CEO', 'worthy', 'Tickets', 'Expect', 'producer', 're-routed', 'by-the-pool', 'Click', '#free', 'ser', 'forget', 'Abba', 'analytics', 'Bitrate', '#shocked', '#WP7', 'Updated', 'strawberry', 'yr', 'mini-store', 'mozilla', 'I38', 'GoWalla', 'Golds', 'Npr', 'Rudy', '#reading', 'CA', 'Holler', 'iPad-ed', 'dispenses', 'user', 'Serious', '#SmallBiz', '#sundaymorning', '#socialviewing', 'LP', '#mosaicxm', '#domo', 'parse', 'ya', 'bpm', 'inflight', 'awaits', 'STEM', 'Bees', 'PLEASE', 'WWW.DIVASANDDORKS.COM', 'N', '#almedia', 'cloud', '9.50', 'weep', 'homeless', 'Jeff', 'Riots', 'iSpyArt', 'Sitting', 'Semantic', 'm4', 'meeting', 'nice', 'silver', 'cop', 'Million', 'CLIENT', 'LB', 'mat', 'canada.RT', 'Lies', 'LoL', 'Yowza', 'investor', 'Award', 'Poked', 'Amble', 'doom', 'Rule', '#networks', 'Rock', 'space', 'reputation', 'icebreaker', 'center', 'Nope', '#MEGABLAAG', 'autocorrects', 'Jackson', 'Sbux', 'AU', '#Google_Circles', 'posbly', '615AB', 'Hack', 'PLUS', 'Dark', \"Larry's\", 'Eddy', 'circular', 'guide', 'XML', 'Spring', 'dwell', 'ave', 'likely', 'keywords', '_a', 'Torrents', 'warmer', '1.75', 'hear', 'entry', 'bingo', 'Helpful', 'Force', 'cent', 'TweetDeck', 'hacker', '#TechGeek', '#mhealth', 'Fellow', 'ooing', 'flask', '5', 'relevance', 'Advertising', 'summer', 'DNSMadeEasy', 'stage', 'agent', 'healthcare', ';]', '#mobile', 'Jalen', 'dime', 'rep', 'Zelda', 'socnet', 'Merchant', 'vast', '3/22', 'Google-savvy', 'Notetaking', 'kill', '#nasatweetup', 'hive-think', 'captain', 'schema', 'crow', '7.20', 'intersection', 'prep', 'Networking', '#xperiaplay', 'addict', 'account', 'mechanical', '#Google', '#Idealab', 'Lists', 'ray', 'agree', 'Timing', 'drive-by', '#killing', 'channel', 'utter', 'Hearts', 'Plenty', 'BallroomD', '#miketyson', 'P', 'job', 'crash', '#at', 'renames', 'communication', 'absolutley', '2:30', '#archforhumanity', '#sadpanda', 'Excellent', 'overheat', 'music', '#Twitter', 'Host', 'intrvw', 'imitation', 'vs', 'Yall', 'language', 'enviro', 'Posterous', 'john', 'Relic', '2wks', 'kthxbai', 'Dexteria', 'Yes', 'brethren', 'Tell', 'stove', 'page', 'IA', '0310apple', 'audience', '#hootsuite', 'Wait', 'partytweets', 'Reported', 'stimulate', 'Potential', 'Pi', 'identify', '#asd', 'taught', 'Wi-Fi', 'float', 'iPhone-toting', 'physician', '#Netflix', 'rave', 'ever', 'party', '#PickMeUpAniPad2', 'Priests', 'store', 'consider', 'taste', 'oxymoron', 'Text', 'Join', '#electricnews', 'several', 'Arrived', 'category', \"people's\", '#Ogilvy', 'plan', 'decline', 'founder', 'cause', 'lap', 'Badges', 'real', 'draw', 'Socks', 'hahaha', 'count', 'unabashed', 'socially', 'sixth', 'robots', '#location', 'ratio', 'worth', '#Mayer', '#wedig', 'bet', 'simple', 'Battelle', 'homogeneity', 'Weinschenk', 'energy', 'Absolute', 'stand', 'adoption', '#SMX', '#keepingupwithDKM', 'Tues', 'confess', 'relation', '#jpmobilesummit', 'REI', '#CIRCLES', 'Prizes', 'FDA', 'Plant', 'mine', '#zazzlesxsw', 'venture', 'implication', 'display', 'Says', 'xD', 'luck', 'logical', 'batman', 'want', 'farmer', '#byebyemoney', 'Foursquare', 'Highly', 'beta', 'left', '4chan', 'whimsical', 'intrigue', 'RECAP', 'corner', 'Adloopz', 'Theophilus', 'brisk', 'someday', '#wantlet', '#cheatday', 'towards', 'stogy', 'Lab', 'plancast', '#VIP', 'Closer', 'Ummmm', 'banking', 'Juice', 'Xcitng', 'Gadgets', 'Success', 'duckett', 'far', 'Marketers', 'Homeless', '#osmpw', 'evil', 'destination', '#sgroars', 'facist', 'IE', 'engineer', 'N26', 'kno', 'Broadfeed', 'early', '#us', 'trans-device', 'insights', '27', \"Microsoft's\", 'naive', 'Jump', '50', 'HIG-like', 'Starbucks', '#Aquent', 'vision', '#LolShow', 'goings-ons', 'lab', 'silly', '#pvhouse', 'Hashable', 'TiPb', 'Source', 'geeky', 'program', 'Rumour', 'Maple', '#microformats', 'design', 'iPod', 'spammer', 'baseball', 'GroupMe', '#mitharvard', 'Alan', 'wow', 'SBSW', 'Jim', 'notice', \"kid's\", 'lightning', 'Netflix', 'Wiley', '#TwitPict', 'stupid', 'Integrated', 'live-tweets', 'Fruit', 'WP7', 'RIP', 'schoolgirls', '#Texting', 'hiring', 'buy', 'continual', 'weird', '#guilttrip', 'UberSoc', 'Future', 'upside', 'attracting', '#smartphone', 'h264', 'Remix', 'Zaggle', 'lightbulb', 'Fail', 'tether', 'cord', 'elevation', 'sold', 'bberry', 'Play', 'FIND', 'full', 'vaccine', 'Hub', 'headache', 'Preso', 'Vs', 'agility', '#house', 'ad', 'Screen', 'creativity', 'spy', 'extol', 'customer', 'Charity', 'selective', 'wondeR', 'Eats', 'brings', 'Comparing', 'dead', '9-15', 'drive', '#infeKTD', 'Cisco', 'gamification', 'lease', 'chart', '#thanksforthebrandedshades', 'Ohh', \"America's\", 'touch', 'bash', 'ebook', 'sharp', 'Draining', '10', 'Ya', 'smarter', 'Ooh', 'AMEX', '#VIDEO', 'glimpse', 'scrub', '#checkins', 'Farooqui', 'roll', '#notatsxsw', 'DOUCHE', 'Questions', 'Sent', '#NewsApps', 'landscape', 'H2O', 'cannot', 'Follow', 'behavior', '3323324', 'Creepily', 'waaaambulance', 'specific', 'Deleting', 'self-serve', 'LISTEN', 'Hopkins', '1985', 'eagerly', 'bargain', 'SketchUp', '#iosdevices', 'stretch', 'Thx', 'Dir', 'COM', '30min', 'Woot', '<--', 'Franken', 'marketing', 'Illustrated', 'thirsty', 'rescue', 'PLAY', 'relax', '#awesome', 'Atrix', 'greatly', 'favour', 'hadnt', 'Ahhh', 'intelligence', 'effectively', 'dawg', 'deny', 'Suck', '4this', '#Digital', 'adventure', 'outpace', 'report', 'Protip', '#media', 'Alot', '141st', 'algorithm', 'temp-to-hire', 'Mary', '12th', 'jackass', '#dumbanddumber', '#edreform', '#hmm', 'fuss', 'Desktop', 'month', 'Dense', 'Shiny', '#itunes', 'mood', 'IIe', 'Kid', 'fathom', 'japan', 'BBC', 'Experience', 'Theres', 'glue', 'pop-uitp', 'obscure', 'Magazine', 'friggen', 'character', 'Game', 'revenue', 'lone', 'Californians', 'qho', 'itashimasu', '#photos', 'Energy', \"we'd\", 'limo', 'popping', 'cerebellum', '#MarissaMeyer', 'animation', 'website', 'benieuwd', 'DoDo', 'merely', '#globalmoxie', 'SHIT', 'Selection', 'Loko', 'March', 'Robert', 'choke', 'gatorade', 'Interview', 'deficit', '03', 'woah', 'Kind', '6.30', 'Learned', '#eduar', 'VA', 'shuffle', 'blekko', 'cure', 'RT6N79XP37FF', 'Lovely', 'Akhirnya', 'N14', 'restroomi', 'sizzle', 'HDMI', 'Prevention', 'observation', '#batterykiller', 'bruise', 'minute', 'GTD', 'rare', 'career', 'Stand', '#chevysxsw', 'Especially', 'prepped', 'etc', 'Settling', 'Matter', 'Audioboo', 'Gingerman', 'WiFi', 'Kawasaki', 'tell', '#android', 'eloquently', '_gger', 'capitalism', 'geo-location', 'thus', \"How's\", 'pattern', 'major', '#vitalscom', 'goodness', '#imanidiot', 'google', '#ireport', 'toe', '#workinprogress', 'nonprofit', 'elegant', 'Zomg', 'QR', 'I11', 'lost', '#SCRM', 'home', 'lengthy', 'hometown', 'Topicality', 'zip', 'hopefully', '#Network', '#hhrs', 'embrace', 'paint', 'buzzmetrics', '#4g', 'globalbestaward', 'tag', 'Warm', 'offic', '#winwin', '#CNET', 'Convention', '#payitforward', 'devs', 'fair', '#rgv', '1986', 'Night', 'Room', 'reference', 'I18', 'W3C', 'JavaScript', 'barry', 'Faster', '39', 'Hmm', 'red', '#prettycool', 'answr', '#letsdothis', 'Coolest', 'custome', 'geek-douche', 'meetup', 'sweet', '#games', \"engine's\", 'Artists', 'recount', 'underneath', 'updates', 'Qualcomm', 'WhisperGram', 'position', 'VS', 'MUSIC', '#dairy', 'Sisyphus', 'Nah', 'Wore', 'synching', 'Gotto', 'phair', 'ATX', 'author', 'hosting', 'hotspot', 'death', 'Teo', 'sex', 'Echo', 'Bird', 'worthwhile', 'bit.ly/ajs2011', '...', 'shiny', 'Beauty', 'Code', 'hi', 'Borders', 'dije', '#thisisdare', 'Interactivity', '3X', 'franchise', 'Hungry', '#scinfluence', 'Callooh', '206k', '#API', 'SWSX', '#platform', 'deadly', '#Samsung', 'say', 'Building', '#tcs_dw', \"Where's\", '#Appletogo', 'ACT', '#memes', 'iCarRadio', '#evolvingworkplace', 'fascist', 'back', 'Foliot', '#sxprotect', '#swesxsw', \"one's\", '#smartthings', '#iphonedev', 'Looked', 'quick-contact', '1PM', '#princess', 'Burners', 'dammit', 'ixd', 'default', 'built', 'Elbow', '#bavcID', 'geo-games', 'super-friendly', '#OgilvyNotes', 'V', 'Breakfast', 'bridge', 'intermittent', 'Natural', 'document', 'goer', '#Merissa', 'shorter', 'Punchout', 'Talented', 'Several', 'thunderbolt', '#AugmentedReality', 'measure', 'saving', 'di', 'enlighten', 'grip', 'hold', 'reel', 'location-based', '#credit', 'hallway', 'crowd-curated', 'Boots', 'detect', 'see-really', 'win', 'Disgraceful', 'Please', \"Smurf's\", '#accessibility', 'Activations', 'transit', 'air', 'GB', 'taxi', 'score', 'therefore', 'eye', 'mark', ':p', '#RanceWilemon', 'Florian', 'Bat', 'hyatt', '#vb', 'pocket', '#Beluga', 'Keep', 'unboxing', 'moment', 'Typing', 'Arrow', 'super', 'fin', 'Karateka', 'ensues', 'pitch', 'Fantasy', 'Pacific', '#LongLinesBadUX', 'happen', '#LoveHer', 'fireside', 'divorce', 'System', 'PubSubHubbub', 'Shop', 'company-nice', '#galaxys4g', 'solid', 'surui', 'wanna', 'Superbia', 'OReilly', 'stunt', '#atzip', \"They'd\", 'news', 'san', 'quiet', '#LeanStartup', '#adpeopleproblems', 'Robot', 'trap', '#success', '#GeekDilemma', 'flood', 'Lame', 'Rebecca', 'Retweet', 'ambassador', '#socialfuel', '50-60', 'strength', '#disconnectedSundays', 'FB', 'ahing', 'duck', ':P', 'Wanting', 'Remaining', '#FF', 'Nextflix', 'Bit.ly/androidhig', 'mtg', 'Chrome', 'Obama', 'dongle', 'Offered', 'comment', 'juice', 'NOOK', '130,000+', '#tip10', 'passport', 'Ideally', 'impression', '<3', 'David', 'disappointingly', 'benefit', 'reminder', 'Unlisted', 'Pinball', '1.1', 'Passing', \"Firm's\", 'determine', '70', 'facts', 'nine-hour', 'confines', 'Email', 'Turning', 'original', 'v5', 'ACC', 'Mike', 'Boys', 'OOOOrkut', 'tme', 'gatekeeper', 'LBS', 'Topspin', 'damage', 'Featured', '#orly', '#HTML5', 'patent', 'rigeur', 'Drivers', 'communal', 'Wins', 'rabid', \"night's\", 'vector', 'Louisiana', 'Johnston', 'explosion', 'nosql', '#1990style', 'GPS-aware', 'TronLegacy', 'blue', '#tx', 'Productivity', 'contribution', '5.2', 'turn', '#Sencha', 'optimal', 'crazy-long', '150', 'making', '12am', 'Renders', 'techie', 'cinemagoers', \"Anna's\", 'Supposedly', 'Jetsons', 'Papa', 'pitfall', '#idontbelieve', 'Continuum', 'Seth', \"how's\", 'bread', 'xperia', 'Convience', '#sendai', 'handicap', '3/4', '25th', '#wow', '#dvsg', 'fanboy-dom', 'Muro', 'Ratio', 'wr', 'multicolor', 'layed', 'Line', 'boys', '#bitter', '#veriphone', 'Air', 'tight', 'Bulletproof', 'lovin', 'Tempted', 'mostly', 'Today', 'iTunes', 'yep', 'flatbead', 'slap', 'Second', 'Possibly', 'raise', 'HeyWire', 'income', 'Luck', 'visualization', 'Shipments', 'profile', 'Q7A', 'palette', 'dehumanizing', 'PBS', 'Payments', 'volt', 'wk', 'Religion', 'Haha', 'realm', 'Buffalo', 'long', 'carbon', 'solve', 'hum', 'profitable', '#applesxsw', 'two-week', 'passage', 'teeny', 'throw', 'JuicePack', 'breach', 'sony', 'straight', 'TWIT', 'Company', 'busy', 'tester', 'Cooler', 'vector-based', 'handle', 'seam', 'deed', 'beard', 'forgo', 'Spectacle', '#106', 'ok', '#MxM', 'austin', 'fine', 'denotes', 'akaBuzz', 'manage', 'HP', '#Chilltab', 'receive', '##tapworthy', 'Hurrah', 'sundayswagger', 'backup', 'Z7', '29', 'marker', 'sign', 'Photoes', 'Brazos', 'homies', 'Group', \"Brazil's\", 'tech-nerds', 'heatmap', 'English', 'inthat', 'disrupt', 'Reader', 'approach', 'Ives', '#mylunch', 'geekdom', 'Lifters', 'wary', 'Dell', 'heads-up', 'Presentation', 'peak', 'tcrn.ch/fcs45j', 'lame', 'Ironic', 'recommendation', 'CAB', 'Playground', 'Face', 'lineup', '#cnngrill', 'Unlike', 'cab', 'Fun', 'true-and', 'decent', 'exchange', '5-8', 'across', 'sell', 'hearing', 'Bisot', '#trollfoot', 'Cha', 'reproduce', '#browserwars', '#killerhack', 'REPORT', 'effort', 'Clark', '#maggiemaes', '#TheCivilWars', 'mag', 'sandwich', 'Many', '#sightings', 'Three', 'terrace', 'auto-corrected', 'mute', 'Refine', 'Smartphone', 'maar', 'bottle', 'refrigerator', 'hockey-fan', '#GeekSpringBreak', 'MS', '#music', 'burst', 'alt', '#LEGO', '#pepsicostage', 'alumni', 'hookup', 'sore', 'Random', 'PICS', 'dosta', '#corporatewhore', 'empty', 'Really', 'chili-cheese', 'noobs', 'inflatable', 'switch', 'Cursing', 'Zaarly', '3:39', 'w', 'maximum', '#nokiaconnects', 'nearby', 'six', 'Easily', 'Fool', 'interactive', '#payingwithdata', 'Backpacking', 'Prepares', 'portion', '#Help', 'Screw', 'artistic', 'erson', 'Folks', '165', 'dad', 'Overcome', 'Trade', 'suspense', 'Catching', 'Haralambous', '#dgtltribe', '#friends', '#up', 'chill', '#poursite', 'Respect', '7322', 'indicate', 'Nexus', 'DM', 'hip', 'Bahrain', 'ARsense', 'optional', 'intelligent', 'Page', 'Hit', '25', '#lookingforwardtothemusicfest', 'sapete', 'next', 'Back', 'Mon', 'tie', 'publisher', 'Ep3', 'previous', 'C5', 'chick', 'AOS', 'Tacos', '#stillonamacbook', 'store--great', 'sched.org', 'covered', 'makeshift', 'court', 'landlord', 'Communication', 'approve', '#NewAndroid', '#POURSite', 'hire', 'speakeasy', 'Clif', 'mail-order', 'treatment', 'Comes', 'emulate', 'pg', 'discover', 'JR', '#310409H2011', 'Photo', '3881586', 'geolocation', 'Movie', \"today's\", 'catfight', 'Frying', '#samsungsxsw', 'Aquent', 'cleaner', 'drunken', 'bird', 'Paypal', 'serious', 'verizon', 'recipe', '#Hipstamatic', 'usability', 'out-of-town', 'Texas', 'Street', 'frickin', 'Maes', 'innovation', 'bought', 'brand-new', 'en', 'regularly', 'spontaniety', 'hard-earned', 'sponsor', 'packard', 'dirty', 'Setting', 'credit', \"u've\", 'Methinks', 'howdy', 'iMac', 'bio', 'itune', 'back-up', 'Drafthouse', 'Two', 'shortly', '#LBS', '#FrostWire', 'drug', 'shark', 'acoustic', 'blind', 'Wittingly', \"We're\", 'Getting', 'Late', 'rear-facing', 'born', '#googlebread', 'hannukah', 'Beluga', '3D', 'inertia', 'lust', 'Lakers', 'imminent', 'Hunt', 'Cellbots', 'gilt', 'recap', '59p', 'Handsome', '#MadebyMany', 'arg', 'environment', 'sharing', '#pr20chat', 'Scarbrough', 'regarded', 'racetrack', 'Laporte', 'valid', 'Guy', 'NASDAQ', 'thunder', 'snoop', 'Group-Texting', 'reading', 'piss', 'Shipping', 'liberty', 'season', 'Ben', '#techi', 'Thoughts', 'hackerspace', 'Elusive', 'Lonely', 'pager', 'pride', 'basic', 'API', 'Sharing', 'cnr', 'priori', 'even', 'Twitter', 'accommodate', 'Create', 'voice', 'bay', 'proper', 'downtime', '14', 'prefer', 'kiosk', 'loyalist', 'Farmville', '#addict', 'Greenberg', 'Askd', '#Collaboration', '89', '#kids', 'closed', 'proud', 'bushwhack', 'Using', '#Music', 'Juan', 'remove', 'powerglove', 'XBox', '#Billboard', 'Biomimicry', 'mighty', 'wave', 'iTun.es/iFR3Dw', 'Gibson', 'contact', 'challenge', \"registrant's\", 'Rackspace', 'straw', 'CWebb', 'last', \"What's\", '#Tyson', 'nothin', 'Harry', 'istock', 'takeaway', '43', 'utility', 'fashion', '63MMAM7W3', 'Swing', 'Tammi', 'Finkelstein', 'Releasing', '37', 'Decided', 'organizer', 'v1', 'Context', 'phenomenal', 'technical', 'SPIN', '#tablets', '#sxsma', '#frood', 'Shaping', 'Thanks', 'cash', 'Deep', 'Demolition', '#powermat', '#BooYah', 'dl', 'whiskey', 'stampede', 'crowdbeacon', 'tee', 'homepage', 'void', 'TT66H9TJN4YE', 'wkd', 'ubiquitous', 'Cosby', 'bot', 'violin', '8A', '#discovr', 'Friday', '#hcsmeu', \"l'express\", 'Creativity', 'wander', 'TMobile', 'boycotting', 'argument', 'HOT', 'light', 'Egyptian', 'perspective', 'TWiT', 'ride', 'MAPS', 'uncatalogued', '1:59', 'Miller', 'call', 'VP', '#Filmfestepic', 'stake', 'metaphor', 'Uber', 'jet', 'q', 'GoogleMe', 'kyping', 'Saturday', 'must', 'Heard', 'ordered', 'Premium', '#mymom', 'Willing', 'consistently', 'Ok', 'dimensional', 'Spencer', 'sauce', 'Quadroid', 'guru', '#devops', 'temporary', 'incognito', 'jargon', '#hcmktg', 'official', 'attendance', 'Admob', 'earthquake', '#Socialnews', 'retailer', ';)', '#citizen_journalism', 'maes', 'Challenges', 'wishing', 'Mel', '#futurecast', 'tunage', 'rain', 'Someone', 'crisis', '#flip-board', \"#iPad2's\", 'precedent', 'Observer', 'Magazines', '11-20', 'technologist', 'le', 'jealous', '#hireme', \"McRee's\", 'bathroom', '#squarespace', 'overshadow', 'meal', 'AR', 'information', '#ATX', '#geek', 'yet', '#cnet', 'tour', 'enough', 'deadline', 'RF', 'action', 'oddly', 'dinner', 'RPG', 'miins', '#Mayor', 'Datasets', 'Marketplace', 'setup', 'sit', 'possibility', 'rating', 'Tradeshow', 'Following', 'shuttle', 'chip', '2moro', 'downloads', 'HS', 'Discover', 'Vencorps', 'AUS', '#gpp', 'Load', 'macchiato', '#wallstreet', 'WHOLE', 'rest', '#hack5', 'hello', 'surprise', '#Lean', 'Avail', 'basically', 'MARAVILHA', 'sound', 'Innovation', 'frankenstein', '#latenightgeeks', 'untrue', 'nostalgic', 'soundcloud', 'project', 'turkey', '25,000', 'Yai', 'Relief', 'Eating', 'sighting', 'upstart', 'goona', 'friendly', 'Doodles', 'Evan', 'lunch', '#USDes', 'dog', 'Duane', 'lanzamiento', 'Moot', 'joy', 'Exist', 'Amazon', 'donlot', 'sez', 'WUT', 'Bodies', 'flattery', '1100', '#tc', 'Beats', 'venn', 'Heads', 'Geekfest', 'mistake', 'Confirmed', 'purchaser', 'Reporting', '11p', 'Tons', 'Webtrends', 'directly', 'Rules', '#smc', '#ACLU', 'NIGHT', 'bear-creatures', 'potentially', 'TechDirt', '#Cleantech', '#networking', 'Build', 'removable', 'Grown', 'glut', 'pot', 'paper', 'Meet', 'ha', '#Offers', '#qabg', 'baracades', 'Safari', '#tigerblood', 'Holy', 'overnight', 'Ca', 'Makes', 'Whale', \"Map's\", 'busdev', 'photoshop', '#marissameyer', 'Iphone-appen', 'configuration', 'Fast', 'partied', 'countdown', 'CrapKit', 'Seriously', 'kid', 'feed', 'Dual', 'Egyptians', 'min', 'monetization', 'cordless', '#GoogleMe', 'size', 'Script', 'Google-sponsored', 'DMs', 'neither', 'grant', 'spectacle', 'lookin', 'Know', 'TCS', 'locate', '#industrialdesign', 'Cashmere', '#GroundLink', '#Arabspring', 'Quoi', 'Stepping', 'S2', 'ipad', 'Headaches', 'generate', 'Mega', 'Bus', '128', \"RSQ's\", '4:30', 'CPA', 'wodpress', 'plze', 'thumbs', 'Lick', 'SALE', 'Deciding', 'Instrumental', 'Mayers', 'Saber', 'Psycho', 'siren', '#privacy', 'Pure', 'alien', 'Yeay', 'NINE', 'Rocksauce', '#Tablet', 'Zeus', 'surf', 'Focused', 'Ch', '#Apple', 'possible', 'Casa', 'Googler', 'Lot', 'priestess', '#startup', 'Tom', '1223', 'Beast', 'engage', 'open-source', '#hotel', '#GSDM', '#Flex', 'live', 'startupbus', 'SoundCloud', 'hook', 'Whole', 'Anxious', 'adaptive', '#drumbeat', 'Rachael', '#mint', 'afraid', 'Volt', 'Lindsay', 'Re-tweeted', 'Called', 'intense', 'Canadian', 'see', 'SSxSW', 'Bomb', 'policy', '#bots', 'Headphones', 'natural', 'flash', 'Abt', 'I32', 'widely', '200M', 'Amid', 'strange', 'Masses', 'equity', '#raidsxsw', 'regard', '#willpay', 'bij', 'disaster', '1:30-', '#ANDROID', '#PGi', 'developers', 'dan', 'Movement', 'LOL', '#apps', 'tribe', 'Google-Bing', 'wha', 'sip', \"car'-use\", 'miracle', 'Ixd', 'Foster', 'Tune-Up', 'Grateful', 'preference', '#Hashable', 'TYP', 'Austin-Bergstrom', 'NBA', 'Chargers', '#imthetype', 'environmental', 'Feature', 'EA', 'Ears', 'step', 'Loopt', 'speak', 'Tonight', 'Cream', '#tabletwars', 'HahaRT', 'NERDS', 'cover', 'publishing', 'boi', 'visit', 'dock', 'ftrs', 'Leaf', 'que', 'scavenger', 'thru', 'Alarms', '#gdgtaustin', 'insist', 'shiner', 'popchicks', '#SpeechTherapy', 'yur', 'wacky', 'second', 'hijack', 'gutter', 'Sanders', 'next-big', 'continue', 'Tiny', 'Ventured', 'Louis', '#viral', 'park', 'Premiere', 'Options', 'MBA', 'APIs', 'pig', 'equate', 'powerhouse', 'pre-paid', 'Hope', '#Posterous', 'theft', 'availability', 'trustworhy', '#StartupTribe', 'die', 'fwd', 'livesteam', '_TIME', 'welcome', '#escape', 'Rewards', 'Prepare', '#minimalistprogramming', '#Checkin', 'high-fiving', 'transfer', 'Stacks', 'lactation', 'dress', 'since', 'entire', 'Condesa', '10.30-', 'Beyond', 'enhancement', 'Z1', 'Def', 'Questioner', 'Galaxy', 'create', '#authenticationdesign', '#genius', 'bother', 'Bump', 'protip', 'Promotion', 'desmiente', '7th', 'Kiss', 'vibrator', 'Tables', 'Pepsi', 'Battle', 'Remake', '2yrs', 'restaurant', 'Tea', '#Smile', '#marketing', '#dailydeals', 'Beautifully', 'Legal', 'click', 'Nadya', 'Smyle', 'Collection', 'incapable', 'Prob', 'Grill', '#socialnetwork', 'reddit', 'canal', 'playing', 'Details', 'slope', 'QA', 'amigos', 'flawless', 'Approved', 'Indeed', 'Syncing', 'Alice', 'Rocking', '#assistivetech', 'changer', '#SaatchiNY', 'Party', 'Set', 'Request', 'gentlemen', 'STOP', 'triple', 'footnote', 'Fuzzy', ';-)', 'glow', 'PlayStation', 'comedy', 'critically', 'retail', 'rematch', 'avoid', 'Damon', 'rig', 'please', 'MR', 'rendering', 'Soundtrckr', 'publicly', 'Antwoord', 'resist', 'Guardian', 'survey', 'Srsly', 'reassemble', 'blonde', 'child', 'blush', 'primero', 'custom', 'broadcaster', 'era', 'sm', 'Dead', 'Grt', 'log', 'Fuck', 'Developer', 'Panel', 'digitally', '#japan', 'serps', 'Layer', 'food', 'recipient', 'Streetvview', '10K', \"get'm\", '#IE9', 'realistic', '#checkinfrenzy', 'geeking', 'radio', '#techcrunch', 'hoot', '#justsayin', 'responsibility', 'station', 'select', 'btw', 'T-Mobile', 'unexpectedly', 'whattt', 'easily', 'Pedicabs', 'Valuable', 'experience', 'explode', '30', 'autistic', 'Tee', 'Sold', 'ferris', 'Weve', 'pair', 'LiveBlog', 'fetch', 'card', 'POUR', 'barton', 'Hoffman', 'Farmers', 'Cruze', 'doo', 'expert', 'parenthesis', 'Privacy', 'speed', 'dream', 'come', '#thanks', 'People', 'Geo-Fencing', '11am', 'pr', '330', 'bean', '#Sydney', '#prepared', 'mackbook', 'Browser', 'head.Give', 'Backupify', '30th', 'Verbs', 'anyways', 'overtaken', 'revelation', 'misunderstanding', 'Nevertheless', 'radical', 'Ballrm', 'Parra', '#GDGTLive', 'Soooo', 'Monetization', 'billards', 'C43', 'hysterical', '3d', 'musician', 'Geekdom', 'globally', 'referral', 'medium', 'N3', 'travelled', 'mahalo', 'Circles', 'Nerds', 'hirer', 'whirlwind', 'Www', '#OMG', 'majority', 'browsing', 'venues', 'Thrilled', 'assistant', 'brah', 'Hole', 'initiative', 'Radical', '#thankyouecon', 'Zombie', '#jobsearch', 'miami', 'Hi', 'Board', 'Andrew', 'incredible', 'lottery', 'advantage', 'constant', 'List', 'endeavor', 'attract', 'smartypants', 'de', 'Playing', '2100', 'seta', 'drinking', 'Skyr', 'multiplicity', '#PartyLikeIts1986', 'circus', 'degree', 'iPads', 'to-be-announced', '4g', 'Action', '#tye', '#dowtimebetweensessions', 'thin', 'breathtaking', 'british', 'fail', 'havin', 'rough', '#ip2', 'i.e.white', 'GoogleTV', 'tuesday', 'entrepreneur', 'Mingly', \"demo's\", 'dump', 'False', 'hierarchical', 'install', 'www.rana.co', 'evacuation', 'informal', 'pragmatic', 'BOOM', 'title', '101', 'ConventionCenter', 'Fandango', 'doofusness', 'thunk', 'Insight', 'speaks', 'ben.mcgraw', 'www', 'pubcamp', 'bracket', 'Dudes', 'Users', 'Nothing', 'Skynet', 'quantity', 'covr', 'know', 'non-text', 'Testing', 'monday', 'carpet', '#ipaddesignheadaches', 'Temperature', 'brunch', 'politics', '#crazyco', 'Ticket', 'Radioshack', 'Pic', 'hug', 'graphic', '10x2', 'Heartbreaker', 'existence', 'MyPOV', 'Climbing', 'margarita', '2.1', 'recs', '#Optimas', 'Rick', 'Spent', '#appletakingoverworld', '#stunned', 'ical', 'Pleased', 'pitchfork', '::', 'See', 'Concertgoers', 'Tessie', 'status', 'Speakers', 'saw', 'coworker', 'Windows', '#tveverywhere', 'Subscribe', 'cartridge', '#bizzy', 'officially', 'TVs', 'Shared', 'TG', 'Morphie', 'Quinn', 'Surprising', 'spin', 'Makeshift', 'Pages', 'outlandish', 'gem', 'ugh', 'Cannot', 'riff', '#dokobots', 'rate', '#classical', 'temporarily', 'bouncy', 'essential', 'Others', 'Rescuing', 'drink', 'location', 'hurt', '#snubor', 'gCal', 'endorsed', 'visualisation', 'scale', 'Nice', '2:15', 'hubby', 'sweeeeet', 'Media', 'Dangerous', 'Bart', 'Orlando', '#NNNNEEERRRRDDDDSSS', 'Around', 'blur', 'Insertion', '#mnbuzz', 'JS', 'Boggle', 'enchant', '3/5', 'clothes', 'extension', 'relate', 'Andro-nerds', 'artist', 'cloudapp', 'somebody', 'progression', 'ADD', 'lay', 'comic', 'exceeds', '#socialmuse', 'shout-out', '8p', 'award', '41', 'N31', 'Handy', '#eurosxsw', 'dual', '#TexasEvery', 'afar', 'wrong', 'Rethink', 'CAKE', '#qrcode', '#telework', '#kiosks', '#hhonorsrecharge', 'Nik', 'no-fly', 'Azure', 'LEGAL', '#youkidshavefun', 'Z9', '#bankinnovation', 'BBerry', \"Steve's\", 'Clever', 'Jesus', 'Sandwich', 'smash', 'found', 'bavc.org/impact', 'Rockaroke', 'cutsies', 'po', 'Look', '681.00', 'Maggie', 'Foam', 'Hmmmm', '#airline', 'Wave', 'Friendly', 'Zero', '#wepartyhere', '3blks', 'Gain', 'outsell', 'building', 'Whrrl', 'song', 'Moonshine', 'Part', 'Service', 'Fully', 'Skifta', 'Linkdown', '#mwrc11', 'first-generation', 'City', 'main', 'Cruisin', 'line-up', 'SCREEN', 'Visual', 'rg', 'kitchen', 'gabacustweets', 'Mascots', 'Daftary', 'Withdrawal', 'Since', 'N5', 'figure', 'lil', 'Mayer-taking', '#lamesauce', 'fleece', 'LATE', 'star', 'percentage', 'tend', '#HP', 'upstairs', 'tree', 'script', 'stroke', 'E', 'Drupal', '#chevytweethouse', 'cell', 'downloading', 'messenger', '#NFL', 'joe', '#bigmistake', '3rd', 'DK', 'Nobody', '#viztalk', '#sorry', 'pixel', 'impart', '#LIVESTRONG', 'owner', '#boston', 'Reveal', 'comparison', 'Grrrr', 'Turn', 'Africa', 'Giveaway', 'Music', 'Prepping', 'Dispatches', '#ie9', 'Carbon', 'level', 'gadget-addicted', 'IRL', \"world's\", 'tricket', 'punishment', 'relief', 'rediculous', '#wisconsin', 'dumb', '#vimeo', 'Atlantic', '3/10', 'Micro', '#ontologyshoutoutwut', 'alley', 'Goog', '#hereforwork', 'Q', '#UnderstandingHumans', 'loyalty', '#futureofed', 'Advil', '#normalpeople', 'JCPenney', 'afteward', 'Mark', 'bldg', 'XOOMs', 'shy', 'Optiscan', 'Innocent', 'Fab', '#jpquake', 'incredibly', 'empathy', 'insight', 'unoffic', 'Radian', 'seatmate', 'ouch', 'earn', 'Giant', '50.6', 'Evidence', 'translate', 'share', 'sorta', 'flipcam', 'boothe', 'Z27', '#purchase', 'grrr', 'severity', 'wil', 'hype', 'Mindstorms', 'fantastic', 'Ha', 'kit', 'H4ackers', 'rockstar', 'Aristotle'}\n"
     ]
    }
   ],
   "source": [
    "words = set(all_words)\n",
    "len(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Frequency of Top 20 Words in Text'}, xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = FreqDist()\n",
    "for word in all_words:\n",
    "    fdist[word.lower()] += 1\n",
    "    \n",
    "fdist.plot(20, title = 'Frequency of Top 20 Words in Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for excessive puntuation - no go\n",
    "def punc_count(tweet):\n",
    "    punctuations = '!$%&()*+,-./:;<=>?[\\]^_`{|}~'\n",
    "    count = 0\n",
    "    for p in punctuations:\n",
    "        count += tweet.count(p)\n",
    "    return count\n",
    "\n",
    "#Look for excessive !/?'s' - no go\n",
    "def exc_que_count(tweet):\n",
    "    punctuation = '!?'\n",
    "    count = 0\n",
    "    for p in punctuation:\n",
    "        count += tweet.count(p)\n",
    "    return count\n",
    "\n",
    "#only periods\n",
    "def period_count(tweet):\n",
    "    punctuation = '.'\n",
    "    count = 0\n",
    "    for p in punctuation:\n",
    "        count += tweet.count(p)\n",
    "    return count\n",
    "\n",
    "#Ratio capital to length tweet\n",
    "def capital_letter_ratio(tweet):\n",
    "    capital_count = 0\n",
    "    for c in tweet:\n",
    "        if c.isupper():\n",
    "            capital_count += 1\n",
    "    return capital_count / len(tweet)\n",
    "\n",
    "#Repeating words - fix to be adjacent\n",
    "def any_repeats(tweet):\n",
    "    if len(set(tweet.split())) < len(tweet.split()):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "    \n",
    "#Hashtag count\n",
    "def count_hash(tweet):\n",
    "    hashtag = re.findall(r'(#w[A-Za-z0-9]*)', tweet)\n",
    "    return len(hashtag)\n",
    "\n",
    "#Average word length\n",
    "def avg_length(tweet):\n",
    "    char = len(tweet)\n",
    "    word = len(tweet.split())\n",
    "    return char / word\n",
    "\n",
    "#Number of words\n",
    "def word_count(tweet):\n",
    "    return len(tweet.split())\n",
    "\n",
    "#Add in if tweet about Apple or Google?\n",
    "def what_company(tweet):\n",
    "    tweet_check = tweet.lower()\n",
    "    if ('iphone' in tweet_check) or ('ipad' in tweet_check) or ('apple' in tweet_check) or ('#apple' in tweet_check):\n",
    "        if ('android' in tweet_check) or ('google' in tweet_check) or ('#google' in tweet_check):\n",
    "            return 'Both'\n",
    "        return 'Apple'\n",
    "    if ('android' in tweet_check) or ('google' in tweet_check) or ('#google' in tweet_check):\n",
    "        return 'Google'\n",
    "    return 'Neither'\n",
    "\n",
    "#Add in what service/product talk about?\n",
    "def what_product(tweet):\n",
    "    tweet_check = tweet.lower()\n",
    "    if ('app' in tweet_check):\n",
    "        return 'App'\n",
    "    if ('iphone' in tweet_check) or ('phone' in tweet_check) or ('android' in tweet_check):\n",
    "        return 'Phone'\n",
    "    if ('ipad' in tweet_check):\n",
    "        return 'iPad'\n",
    "    if ('apple' in tweet_check):\n",
    "        return 'Company'\n",
    "    else:\n",
    "        return 'General'\n",
    "    \n",
    "('google' in tweet_check) or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iPad                               946\n",
       "Apple                              661\n",
       "iPad or iPhone App                 470\n",
       "Google                             430\n",
       "iPhone                             297\n",
       "Other Google product or service    293\n",
       "Android App                         81\n",
       "Android                             78\n",
       "Other Apple product or service      35\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.emotion_in_tweet_is_directed_at.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "App        3238\n",
       "Company    2251\n",
       "iPad       1488\n",
       "Phone      1266\n",
       "General     693\n",
       "Name: what_product, dtype: int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['what_product'] = tweets['tweet_text'].apply(what_product)\n",
    "tweets.what_product.value_counts()\n",
    "# no_cat = tweets[tweets['what_product'] == 'General']\n",
    "# for tweet in no_cat['tweet_text']:\n",
    "#     print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get rid of bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8936.000000\n",
       "mean       17.767793\n",
       "std         4.961420\n",
       "min         2.000000\n",
       "25%        14.000000\n",
       "50%        18.000000\n",
       "75%        21.000000\n",
       "max        33.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['word_count'] = tweets['tweet_text'].apply(word_count)\n",
    "tweets['word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>RT @mention</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_text  label  word_count\n",
       "5025  RT @mention      2           2"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[tweets['word_count'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[tweets['word_count'] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple',\n",
       " 'store',\n",
       " 'mall',\n",
       " 'Sunday',\n",
       " '10x',\n",
       " 'crowd',\n",
       " 'line',\n",
       " 'fake',\n",
       " 'need',\n",
       " 'fuck',\n",
       " 'dongle',\n",
       " 'Genius',\n",
       " 'let']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative = tweets[tweets.label == 0]\n",
    "# tweet_tokenizer(negative['tweet_text'][291])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Different Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['punc_count'] = tweets['tweet_text'].apply(punc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets.groupby('label')['exc_que_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, axes = setup_three_subplots()\n",
    "# plot_distribution_of_column_by_category(\"punc_count\", axes, \"Freqency of Posts Containing Prices for\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Each Feature With Logreg Cause Visuals Are Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test = tweets.drop('tweet_text', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>exc_que_count</th>\n",
       "      <th>punc_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7225</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5863</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7846</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7766</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3886</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8170</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6701 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_count  exc_que_count  punc_count\n",
       "7225           7              0           1\n",
       "5863          13              0           4\n",
       "3045          11              0           4\n",
       "2367          20              3           6\n",
       "7846          19              0           1\n",
       "...          ...            ...         ...\n",
       "7766          15              1           5\n",
       "3886          22              0           2\n",
       "1039          19              0           6\n",
       "8170          18              1           6\n",
       "3435          20              1           8\n",
       "\n",
       "[6701 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train test split\n",
    "y = tweets_test['label']\n",
    "X = tweets_test.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_count 0.06002538528680205\n",
      "punc_count 0.17909048047757065\n",
      "exc_que_count 0.19944959489690914\n",
      "capital_letter_ratio 0.2319134369654643\n",
      "any_repeats 0.08832240109620128\n",
      "count_hash 0.0\n",
      "avg_length 0.1428706650931737\n",
      "period_count 0.09832803618387893\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "features = imbpipeline(steps=[\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(max_depth = 5, random_state = 213))\n",
    "])\n",
    "\n",
    "features.fit(X_train, y_train)\n",
    "for name, importance in zip(X_train.columns, features['dtc'].feature_importances_):\n",
    "    print(name, importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "col_labels = list(X_train.columns)\n",
    "col_labels.remove('tweet_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Vectorize the words\n",
    "cv = CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer)\n",
    "X_train_cv = cv.fit_transform(X_train['tweet_text'])\n",
    "\n",
    "#Scales the non-word columns\n",
    "X_train_nowords = X_train[col_labels]\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(ss.fit_transform(X_train_nowords),columns = X_train_nowords.columns, index = X_train.index)\n",
    "\n",
    "#Combines the scaled and vectorized data together\n",
    "X_train_cv_df = pd.DataFrame(X_train_cv.toarray(), columns=cv.get_feature_names(), index = X_train.index)\n",
    "X_train_final = pd.concat([X_train_cv_df, X_train_scaled], axis=1)\n",
    "\n",
    "#SMOTE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train['tweet_text'])\n",
    "\n",
    "#Scales the non-word columns\n",
    "X_train_nowords = X_train[col_labels]\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(ss.fit_transform(X_train_nowords),columns = X_train_nowords.columns, index = X_train.index)\n",
    "\n",
    "#Combines vectorized and scaled data together\n",
    "X_train_tfidf_df = pd.DataFrame(X_train_tfidf.toarray(), columns = tfidf.get_feature_names(), index = X_train.index)\n",
    "X_train_final = pd.concat([X_train_tfidf_df, X_train_scaled], axis=1)\n",
    "\n",
    "#SMOTE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Bayes - ONLY TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8327115355917027\n",
      "Validation Score:0.6515432902601088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Train test split\n",
    "y = tweets['label']\n",
    "X = tweets['tweet_text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "#Vectorize the words\n",
    "cv = CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer)\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "\n",
    "#Fit and print model scores\n",
    "first_pass = MultinomialNB()\n",
    "first_pass.fit(X_train_cv, y_train)\n",
    "print(\"Training Score:\", first_pass.score(X_train_cv, y_train))\n",
    "scores = np.mean(cross_val_score(first_pass, X_train_cv, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models - can add other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE/TESTING AREA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IF WANT TRY PIPELINE/ADD FEATURE THINGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "count_vec = FeatureUnion([\n",
    "        ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer))\n",
    "        #, add any feature creation things here\n",
    "    ])\n",
    "\n",
    "tfidf_vec = FeatureUnion([\n",
    "        ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer))\n",
    "        #, add any feature creation things here\n",
    "    ])\n",
    "\n",
    "first_pass = Pipeline(steps=[\n",
    "    ('vec', count_vec),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc_count(tweet):\n",
    "    punctuations = '!$%&()*+,-./:;<=>?[\\]^_`{|}~'\n",
    "    count = 0\n",
    "    for p in punctuations:\n",
    "        count += tweet.count(p)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class PuncCount(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return pd.Series(X).apply(punc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tweets['label']\n",
    "X = tweets['tweet_text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 6701, expected 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-b8a1610a2539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m ])\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mfirst_pass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_pass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[1;32m    329\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    332\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    293\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m             \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    586\u001b[0m                                                     \u001b[0mexp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbrow_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                                                     got=A.shape[0]))\n\u001b[0;32m--> 588\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbcol_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 6701, expected 1."
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "\n",
    "count_vec = FeatureUnion([\n",
    "        ('punc', PuncCount()),\n",
    "        ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer))\n",
    "    ])\n",
    "\n",
    "first_pass = Pipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    #('ss', StandardSclaer()) - only if added features on diff scale\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "first_pass.fit(X_train, y_train)\n",
    "print(\"Training Score:\", first_pass.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(first_pass, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x6701 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6479 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc = PuncCount()\n",
    "pc.fit(X_train)\n",
    "test = pc.transform(X_train)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.47157140725264884\n",
      "Validation Score:0.4588858837802041\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "y = tweets['label']\n",
    "X = tweets['tweet_text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "baseline = imbpipeline(steps=[\n",
    "    ('preproc', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213, max_depth = 5))\n",
    "])\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "print(\"Training Score:\", baseline.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(baseline, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "      <th>capital_letter_ratio</th>\n",
       "      <th>exc_que_count</th>\n",
       "      <th>what_company</th>\n",
       "      <th>what_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.118110</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071942</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "      <td>App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>iPad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0</td>\n",
       "      <td>Google</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  label  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...      0   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...      1   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...      1   \n",
       "3  @sxsw I hope this year's festival isn't as cra...      0   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...      1   \n",
       "\n",
       "   capital_letter_ratio  exc_que_count what_company what_product  \n",
       "0              0.118110              1        Apple        Phone  \n",
       "1              0.071942              1        Apple          App  \n",
       "2              0.088608              0        Apple         iPad  \n",
       "3              0.024390              0        Apple          App  \n",
       "4              0.106870              0       Google      Company  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making final dataset\n",
    "tweets['capital_letter_ratio'] = tweets['tweet_text'].apply(capital_letter_ratio)\n",
    "tweets['exc_que_count'] = tweets['tweet_text'].apply(exc_que_count)\n",
    "tweets['what_company'] = tweets['tweet_text'].apply(what_company)\n",
    "tweets['what_product'] = tweets['tweet_text'].apply(what_product)\n",
    "tweets = tweets.drop('word_count', axis = 1)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "col_labels = list(X_train.columns)\n",
    "col_labels.remove('tweet_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB - No Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8097582811101164\n",
      "Validation Score:0.6727805046356583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = True, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_company', 'what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "first = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "first.fit(X_train, y_train)\n",
    "print(\"Training Score:\", first.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(first, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7643986869591167\n",
      "Validation Score:0.66114728371565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = True, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_company', 'what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "first = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    #('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "first.fit(X_train, y_train)\n",
    "print(\"Training Score:\", first.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(first, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preproc__cv__max_features': 8000,\n",
       " 'preproc__cv__min_df': 1,\n",
       " 'preproc__cv__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__cv__min_df': [1, .1, 0],\n",
    "    'preproc__cv__max_df': [.8, .9, 0],\n",
    "    'preproc__cv__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(first, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7684273351238436\n",
      "Validation Score:0.6671137600587665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = True, tokenizer = tweet_tokenizer,\n",
    "                          min_df = 1, max_features = 8000, ngram_range = (1,2)), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_company', 'what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "first = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    #('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "first.fit(X_train, y_train)\n",
    "print(\"Training Score:\", first.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(first, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - No Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9292642889121027\n",
      "Validation Score:0.66288668514252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "second = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('logreg', LogisticRegression(random_state = 213, max_iter = 1000))\n",
    "])\n",
    "\n",
    "second.fit(X_train, y_train)\n",
    "print(\"Training Score:\", second.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(second, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logreg - gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logreg__C': 1.0, 'logreg__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Creates parameters to test\n",
    "params = {\n",
    "    'logreg__C': [1.0, 1e3, 1e6],\n",
    "    'logreg__solver': ['lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(second, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This would be where our tuned logreg went - if we had one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unknown tuned (?) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB - Gridsearch (lowercase/min max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/imblearn/pipeline.py\", line 277, in fit\n",
      "    Xt, yt, fit_params = self._fit(X, y, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/imblearn/pipeline.py\", line 229, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\", line 531, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\", line 458, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\", line 866, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\", line 784, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1213, in fit_transform\n",
      "    raise ValueError(\n",
      "ValueError: max_df corresponds to < documents than min_df\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/imblearn/pipeline.py\", line 277, in fit\n",
      "    Xt, yt, fit_params = self._fit(X, y, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/imblearn/pipeline.py\", line 229, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\", line 531, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\", line 458, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\", line 866, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\", line 784, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1213, in fit_transform\n",
      "    raise ValueError(\n",
      "ValueError: max_df corresponds to < documents than min_df\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'preproc__cv__lowercase': False,\n",
       " 'preproc__cv__max_df': 0.95,\n",
       " 'preproc__cv__min_df': 0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__cv__lowercase': [True, False],\n",
    "    'preproc__cv__min_df': [0, .05, .1],\n",
    "    'preproc__cv__max_df': [1, .95, .9]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(first, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.835397701835547\n",
      "Validation Score:0.6596012109474997\n"
     ]
    }
   ],
   "source": [
    "#Tuned MNB\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          max_df = .95), 'tweet_text')],\n",
    "    #('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "tuned_mnb = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "tuned_mnb.fit(X_train, y_train)\n",
    "print(\"Training Score:\", tuned_mnb.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(tuned_mnb, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB - gridsearch (ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preproc__cv__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__cv__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(tuned_mnb, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9186688553947172\n",
      "Validation Score:0.6800486382405645\n"
     ]
    }
   ],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          max_df = .95, ngram_range = (1,2)), 'tweet_text')],\n",
    "    #('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "ngram = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "ngram.fit(X_train, y_train)\n",
    "print(\"Training Score:\", ngram.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(ngram, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB - gridsearch ngram max features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preproc__cv__max_df': 0.5,\n",
       " 'preproc__cv__max_features': 8000,\n",
       " 'preproc__cv__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__cv__max_features': [None, 4000, 8000],\n",
    "    'preproc__cv__max_df': [.3, .5, .8],\n",
    "    'preproc__cv__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(ngram, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8212207133263691\n",
      "Validation Score:0.6597506872794862\n"
     ]
    }
   ],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          ngram_range = (1,1), max_df = .7, max_features = 8000), 'tweet_text')],\n",
    "    #('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "test = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "test.fit(X_train, y_train)\n",
    "print(\"Training Score:\", test.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(test, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9989553797940606\n",
      "Validation Score:0.6212522399189734\n"
     ]
    }
   ],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text')],\n",
    "    #('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "tree = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213))\n",
    "])\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Training Score:\", tree.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(tree, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtc__max_depth': 30, 'dtc__min_samples_split': 2}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'dtc__max_depth': [10, 20, 30],\n",
    "    'dtc__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(tree, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7780928219668706\n",
      "Validation Score:0.6375170011241332\n"
     ]
    }
   ],
   "source": [
    "#min sample split = 5/max_depth = 20 has less overfitting\n",
    "tree = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213, min_samples_split = 2, max_depth = 30))\n",
    "])\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Training Score:\", tree.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(tree, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTC w/ bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9992538427100432\n",
      "Validation Score:0.6325916279898048\n"
     ]
    }
   ],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          ngram_range = (1,2)), 'tweet_text')],\n",
    "    #('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "dtc_test = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213))\n",
    "])\n",
    "\n",
    "dtc_test.fit(X_train, y_train)\n",
    "print(\"Training Score:\", dtc_test.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(dtc_test, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtc__criterion': 'gini', 'dtc__max_depth': 30}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'dtc__max_depth': [20, 30, 40],\n",
    "    'dtc__criterion': ['entropy', 'gini']\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(dtc_test, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7130279062826443\n",
      "Validation Score:0.6245295891905128\n"
     ]
    }
   ],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          ngram_range = (1,2)), 'tweet_text')],\n",
    "    #('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "dtc_test = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213, max_depth = 30, max_features = 3000))\n",
    "])\n",
    "\n",
    "dtc_test.fit(X_train, y_train)\n",
    "print(\"Training Score:\", dtc_test.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(dtc_test, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logreg with bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7667512311595285\n",
      "Validation Score:0.5964791256246731\n"
     ]
    }
   ],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          ngram_range = (1,2), max_features = 1500), 'tweet_text'),\n",
    "    ('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "logreg_worse = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state = 213)),\n",
    "    ('logreg', LogisticRegression(random_state = 213, max_iter = 1000))\n",
    "])\n",
    "\n",
    "logreg_worse.fit(X_train, y_train)\n",
    "print(\"Training Score:\", logreg_worse.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(logreg_worse, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9737352633935233\n",
      "Validation Score:0.6773597337696307\n"
     ]
    }
   ],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          ngram_range = (1,2)), 'tweet_text'),\n",
    "    ('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "logreg_bi = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state = 213)),\n",
    "    ('logreg', LogisticRegression(random_state = 213, max_iter = 1000))\n",
    "])\n",
    "\n",
    "logreg_bi.fit(X_train, y_train)\n",
    "print(\"Training Score:\", logreg_bi.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(logreg_bi, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preproc__cv__max_features': None, 'preproc__cv__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__cv__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "    'preproc__cv__max_features': [None, 1000, 2000]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(logreg_bi, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "all_words = []\n",
    "for tweet in tweets['tweet_text']:\n",
    "    all_words.extend(tweet_tokenizer(tweet))\n",
    "    \n",
    "\n",
    "vocab_mix = set(all_words)\n",
    "test = []\n",
    "for word in vocab_mix:\n",
    "    test.append(word.lower())\n",
    "vocab = set(test)\n",
    "\n",
    "glove = {}\n",
    "with open('glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in vocab:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.6203548866778577),\n",
       " ('Support Vector Machine', 0.6028950725953165),\n",
       " ('Logistic Regression', 0.6178179295056399)]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "y = tweets['label']\n",
    "X = tweets['tweet_text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "#Pick a Pipeline to try\n",
    "w2v_rfc = Pipeline([\n",
    "    ('Word2Vec', W2vVectorizer(glove)),\n",
    "    ('rf', RandomForestClassifier(random_state = 42))\n",
    "])\n",
    "\n",
    "w2v_svc = Pipeline([\n",
    "    ('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "    ('Support Vector Machine', SVC())\n",
    "])\n",
    "\n",
    "w2v_logreg = Pipeline([\n",
    "    ('Word2Vec', W2vVectorizer(glove)),\n",
    "    ('lr', LogisticRegression(max_iter = 1000, random_state = 42))\n",
    "])\n",
    "\n",
    "models = [('Random Forest', w2v_rfc),\n",
    "          ('Support Vector Machine', w2v_svc),\n",
    "          ('Logistic Regression', w2v_logreg)]\n",
    "\n",
    "scores = [(name, cross_val_score(model, X_train, y_train, cv = 3).mean()) for name, model, in models]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__criterion': 'gini', 'rf__min_samples_split': 2, 'rf__n_estimators': 300}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'rf__n_estimators': [100, 300, 500],\n",
    "    'rf__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(w2v_rfc, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.995672287718251\n",
      "Validation Score:0.6236379623137112\n"
     ]
    }
   ],
   "source": [
    "w2v_tuned = Pipeline([\n",
    "    ('Word2Vec', W2vVectorizer(glove)),\n",
    "    ('rf', RandomForestClassifier(n_estimators = 300, random_state = 42))\n",
    "])\n",
    "\n",
    "w2v_tuned.fit(X_train, y_train)\n",
    "print(\"Training Score:\", w2v_tuned.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(w2v_tuned, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.5834954722059961),\n",
       " ('Support Vector Machine', 0.39083807455060504),\n",
       " ('Logistic Regression', 0.4484387693723044)]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_rfc = imbpipeline([\n",
    "    ('Word2Vec', W2vVectorizer(glove)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('rf', RandomForestClassifier(random_state = 42))\n",
    "])\n",
    "\n",
    "w2v_svc = imbpipeline([\n",
    "    ('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('Support Vector Machine', SVC())\n",
    "])\n",
    "\n",
    "w2v_logreg = imbpipeline([\n",
    "    ('Word2Vec', W2vVectorizer(glove)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('lr', LogisticRegression(max_iter = 1000, random_state = 42))\n",
    "])\n",
    "\n",
    "models = [('Random Forest', w2v_rfc),\n",
    "          ('Support Vector Machine', w2v_svc),\n",
    "          ('Logistic Regression', w2v_logreg)]\n",
    "\n",
    "scores = [(name, cross_val_score(model, X_train, y_train, cv = 3).mean()) for name, model, in models]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__min_samples_split': 2, 'rf__n_estimators': 800}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'rf__n_estimators': [500, 800, 1000],\n",
    "    'rf__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(w2v_rfc, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.995672287718251\n",
      "Validation Score:0.6279663205226663\n"
     ]
    }
   ],
   "source": [
    "w2v_tuned = Pipeline([\n",
    "    ('Word2Vec', W2vVectorizer(glove)),\n",
    "    ('rf', RandomForestClassifier(n_estimators = 800, min_samples_split = 2, random_state = 42))\n",
    "])\n",
    "\n",
    "w2v_tuned.fit(X_train, y_train)\n",
    "print(\"Training Score:\", w2v_tuned.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(w2v_tuned, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting only Apple Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iPad                               946\n",
       "Apple                              661\n",
       "iPad or iPhone App                 470\n",
       "Google                             430\n",
       "iPhone                             297\n",
       "Other Google product or service    293\n",
       "Android App                         81\n",
       "Android                             78\n",
       "Other Apple product or service      35\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emotion_in_tweet_is_directed_at'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_labels = df[df['emotion_in_tweet_is_directed_at'].isna()]\n",
    "no_labels = no_labels.dropna(subset = ['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = []\n",
    "for tweet in no_labels['tweet_text']:\n",
    "    tweet_check = tweet.lower()\n",
    "    if ('iphone' in tweet_check) or ('ipad' in tweet_check) or ('apple' in tweet_check):\n",
    "        tweet_list.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Need to buy an iPad2 while I'm in Austin at #s...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RT @LaurieShook: I'm looking forward to the #S...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The best!  RT @mention Ha! First in line for #...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{link} RT @mention 1st stop on the #SXSW #Chao...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text  \\\n",
       "2   @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "20  Need to buy an iPad2 while I'm in Austin at #s...   \n",
       "25  RT @LaurieShook: I'm looking forward to the #S...   \n",
       "36  The best!  RT @mention Ha! First in line for #...   \n",
       "57  {link} RT @mention 1st stop on the #SXSW #Chao...   \n",
       "\n",
       "   emotion_in_tweet_is_directed_at  \\\n",
       "2                             iPad   \n",
       "20                            iPad   \n",
       "25                            iPad   \n",
       "36                            iPad   \n",
       "57                            iPad   \n",
       "\n",
       "   is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "2                                    Positive emotion  \n",
       "20                                   Positive emotion  \n",
       "25                                   Positive emotion  \n",
       "36                                   Positive emotion  \n",
       "57                                   Positive emotion  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipad = df[df['emotion_in_tweet_is_directed_at'] == 'iPad']\n",
    "apple = df[df['emotion_in_tweet_is_directed_at'] == 'Apple']\n",
    "mix = df[df['emotion_in_tweet_is_directed_at'] == 'iPad or iPhone App']\n",
    "iphone = df[df['emotion_in_tweet_is_directed_at'] == 'iPhone']\n",
    "apps = df[df['emotion_in_tweet_is_directed_at'] == 'Other Apple product or service']\n",
    "\n",
    "unlabeled_apple = df[df['tweet_text'].isin(tweet_list)]\n",
    "unlabeled_apple = unlabeled_apple.drop_duplicates(subset = 'tweet_text')\n",
    "\n",
    "final_df = pd.concat([ipad, apple, mix, iphone, apps, unlabeled_apple], axis = 0)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    0.519993\n",
       "Positive emotion                      0.386466\n",
       "Negative emotion                      0.075629\n",
       "I can't tell                          0.017912\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: float64"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.529477\n",
       "1    0.393515\n",
       "0    0.077008\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = final_df.drop('emotion_in_tweet_is_directed_at', axis = 1)\n",
    "tweets = tweets[tweets['is_there_an_emotion_directed_at_a_brand_or_product'] != 'I can\\'t tell']\n",
    "tweets['label'] = tweets['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "tweets = tweets.drop('is_there_an_emotion_directed_at_a_brand_or_product', axis = 1)\n",
    "tweets = tweets.dropna()\n",
    "tweets.label = tweets.label.map({'Negative emotion' : 0, 'Positive emotion': 1, \n",
    "                                 'No emotion toward brand or product': 2})\n",
    "\n",
    "tweets.label.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.3630557602554655\n",
      "Validation Score:0.3684662576687117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "y = tweets['label']\n",
    "X = tweets['tweet_text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "baseline = imbpipeline(steps=[\n",
    "    ('preproc', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213, max_depth = 5))\n",
    "])\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "print(\"Training Score:\", baseline.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(baseline, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "      <th>capital_letter_ratio</th>\n",
       "      <th>exc_que_count</th>\n",
       "      <th>what_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0</td>\n",
       "      <td>iPad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Need to buy an iPad2 while I'm in Austin at #s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084906</td>\n",
       "      <td>1</td>\n",
       "      <td>App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RT @LaurieShook: I'm looking forward to the #S...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0</td>\n",
       "      <td>iPad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The best!  RT @mention Ha! First in line for #...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>2</td>\n",
       "      <td>App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{link} RT @mention 1st stop on the #SXSW #Chao...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086614</td>\n",
       "      <td>1</td>\n",
       "      <td>iPad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text  label  \\\n",
       "2   @swonderlin Can not wait for #iPad 2 also. The...      1   \n",
       "20  Need to buy an iPad2 while I'm in Austin at #s...      1   \n",
       "25  RT @LaurieShook: I'm looking forward to the #S...      1   \n",
       "36  The best!  RT @mention Ha! First in line for #...      1   \n",
       "57  {link} RT @mention 1st stop on the #SXSW #Chao...      1   \n",
       "\n",
       "    capital_letter_ratio  exc_que_count what_product  \n",
       "2               0.088608              0         iPad  \n",
       "20              0.084906              1          App  \n",
       "25              0.129032              0         iPad  \n",
       "36              0.042553              2          App  \n",
       "57              0.086614              1         iPad  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making final dataset\n",
    "tweets['capital_letter_ratio'] = tweets['tweet_text'].apply(capital_letter_ratio)\n",
    "tweets['exc_que_count'] = tweets['tweet_text'].apply(exc_que_count)\n",
    "tweets['what_product'] = tweets['tweet_text'].apply(what_product)\n",
    "#tweets = tweets.drop('word_count', axis = 1)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "col_labels = list(X_train.columns)\n",
    "col_labels.remove('tweet_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>capital_letter_ratio</th>\n",
       "      <th>exc_que_count</th>\n",
       "      <th>what_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>A free heat map app of 4sq check-ins at #SXSW ...</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0</td>\n",
       "      <td>App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>Dawn's iPad App of the Week is up early to hel...</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0</td>\n",
       "      <td>App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>picked up an ipad 2 at the #sxsw pop up  apple...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7651</th>\n",
       "      <td>Another great review of Super Kaiju Hero Force...</td>\n",
       "      <td>0.075342</td>\n",
       "      <td>1</td>\n",
       "      <td>Phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4864</th>\n",
       "      <td>Awesome to meet @mention @mention and @mention...</td>\n",
       "      <td>0.037383</td>\n",
       "      <td>0</td>\n",
       "      <td>App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4324</th>\n",
       "      <td>Hehe RTÛÏ@mention March 11. Austin, TX. Will ...</td>\n",
       "      <td>0.137097</td>\n",
       "      <td>1</td>\n",
       "      <td>App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>At #SXSW?? MindTouchers @mention @mention @men...</td>\n",
       "      <td>0.112903</td>\n",
       "      <td>4</td>\n",
       "      <td>iPad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>.@mention Love it. @mention @mention #sxsw: &amp;q...</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>1</td>\n",
       "      <td>App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8860</th>\n",
       "      <td>The line was too long the other day... but I'm...</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0</td>\n",
       "      <td>App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>Why is there a line waiting for temp Apple sto...</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>2</td>\n",
       "      <td>App</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4071 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  capital_letter_ratio  \\\n",
       "2141  A free heat map app of 4sq check-ins at #SXSW ...              0.131579   \n",
       "1884  Dawn's iPad App of the Week is up early to hel...              0.108108   \n",
       "244   picked up an ipad 2 at the #sxsw pop up  apple...              0.000000   \n",
       "7651  Another great review of Super Kaiju Hero Force...              0.075342   \n",
       "4864  Awesome to meet @mention @mention and @mention...              0.037383   \n",
       "...                                                 ...                   ...   \n",
       "4324  Hehe RTÛÏ@mention March 11. Austin, TX. Will ...              0.137097   \n",
       "1467  At #SXSW?? MindTouchers @mention @mention @men...              0.112903   \n",
       "861   .@mention Love it. @mention @mention #sxsw: &q...              0.006494   \n",
       "8860  The line was too long the other day... but I'm...              0.073171   \n",
       "3007  Why is there a line waiting for temp Apple sto...              0.042105   \n",
       "\n",
       "      exc_que_count what_product  \n",
       "2141              0          App  \n",
       "1884              0          App  \n",
       "244               2          App  \n",
       "7651              1        Phone  \n",
       "4864              0          App  \n",
       "...             ...          ...  \n",
       "4324              1          App  \n",
       "1467              4         iPad  \n",
       "861               1          App  \n",
       "8860              0          App  \n",
       "3007              2          App  \n",
       "\n",
       "[4071 rows x 4 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB w/ cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8224023581429624\n",
      "Validation Score:0.6145906754495712\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = True, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "apple_cv = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "apple_cv.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_cv.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_cv, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB w/ TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7720461802996806\n",
      "Validation Score:0.6288400838094089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "apple = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    #('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "apple.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preproc__tfidf__max_df': 0.4,\n",
       " 'preproc__tfidf__max_features': None,\n",
       " 'preproc__tfidf__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__tfidf__max_features': [None, 4000, 8000, 10000],\n",
    "    'preproc__tfidf__max_df': [.4, .6, .8],\n",
    "    'preproc__tfidf__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(apple, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.7720461802996806\n",
      "Validation Score:0.6288400838094089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "apple_tuned = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    #('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "apple_tuned.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_tuned.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_tuned, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9992630803242447\n",
      "Validation Score:0.5671862046095175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "apple_dt = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213))\n",
    "])\n",
    "\n",
    "apple_dt.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_dt.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_dt, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtc__criterion': 'gini', 'dtc__max_depth': None, 'dtc__min_samples_split': 5}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Creates parameters to test\n",
    "params = {\n",
    "    'dtc__max_depth': [None, 5, 20],\n",
    "    'dtc__criterion': ['gini', 'entropy'],\n",
    "    'dtc__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(apple_dt, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9606976172930484\n",
      "Validation Score:0.5659595122171808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "apple_dt = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(min_samples_split = 5, random_state = 213))\n",
    "])\n",
    "\n",
    "apple_dt.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_dt.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_dt, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
