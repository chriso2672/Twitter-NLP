{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stakeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apple as stakeholder and compare sentiment of its release vs. Google's at SXSW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('../data/tweets.csv', encoding = 'iso-8859-1')\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets rid of company column\n",
    "tweets = df.drop('emotion_in_tweet_is_directed_at', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops rows with unknown sentiment\n",
    "tweets = tweets[tweets['is_there_an_emotion_directed_at_a_brand_or_product'] != 'I can\\'t tell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renames column with sentiments as label and drops single nan row\n",
    "tweets['label'] = tweets['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "tweets = tweets.drop('is_there_an_emotion_directed_at_a_brand_or_product', axis = 1)\n",
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.label.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reassigns label\n",
    "tweets.label = tweets.label.map({'Negative emotion' : 0, 'Positive emotion': 1, \n",
    "                                 'No emotion toward brand or product': 2})\n",
    "\n",
    "tweets.label.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to tokenize text\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Replaces pos tags with lemmatize compatable tags\n",
    "def pos_replace(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "#Makes list of punctuation to exclude, keeps certain symbols\n",
    "punct = list(string.punctuation)\n",
    "keep_punct = ['#', '?', '!', '@']\n",
    "punct = [p for p in punct if p not in keep_punct]\n",
    "\n",
    "#Used to filter out words leftover by twitter scrape\n",
    "common_tweet_words = ['rt']\n",
    "\n",
    "#Removes non-ASCII characters (aka emojis that cant be converted to original symbol)\n",
    "def remove_junk(tweet):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in tweet])\n",
    "    \n",
    "#TRY LOWERING/NOT LEMMATIZING AND SEE WHAT CHANGES\n",
    "def tweet_tokenizer(doc, stop_words = sw):\n",
    "    #Gets rid of links\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    doc = re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', '', doc)\n",
    "    doc = re.sub(r'(?i)(#sxsw)\\w*', '', doc)\n",
    "    #Gets rid of conversions made during scrapping\n",
    "    doc = re.sub(r'{link}', '', doc)\n",
    "    doc = re.sub(r'\\[video\\]', '', doc)\n",
    "    #Gets rid of weird characters\n",
    "    doc = remove_junk(doc)\n",
    "    #Tokenizes using NLTK Twitter Tokenizer\n",
    "    tweet_token = TweetTokenizer(strip_handles = True)\n",
    "    doc = tweet_token.tokenize(doc)\n",
    "    #Gets rid of numbers\n",
    "#     doc_2 = []\n",
    "#     for w in doc:\n",
    "#         if any([c.isdigit() for c in w]):\n",
    "#             pass\n",
    "#         else:\n",
    "#             doc_2.append(w)\n",
    "    #Gets rid of leftover stopwords/punctuation/twitter meta-info\n",
    "    doc = [w for w in doc if w.lower() not in sw]\n",
    "    doc = [w for w in doc if w.lower() not in common_tweet_words]\n",
    "    doc = [w for w in doc if w not in punct]\n",
    "    #Stemmer\n",
    "#     snows = SnowballStemmer('english')\n",
    "#     doc = [snows.stem(w) for w in doc]\n",
    "    #Lemmatizes tokens\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(w[0], pos_replace(w[1])) for w in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tweets.loc[38, 'tweet_text']\n",
    "test = tweet_tokenizer(t)\n",
    "print(t)\n",
    "print('\\n')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Get rid of #sxsw #SXSW #Sxsw #SXSWedu don\\'t delete this'\n",
    "test = re.sub(r'(?i)(#sxsw)\\w*', '', test)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Features with Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def setup_three_subplots():\n",
    "    \"\"\"\n",
    "    It's hard to make an odd number of graphs pretty with just nrows\n",
    "    and ncols, so we make a custom grid. See example for more details:\n",
    "    https://matplotlib.org/stable/gallery/subplots_axes_and_figures/gridspec_multicolumn.html\n",
    "\n",
    "    We want the graphs to look like this:\n",
    "     [ ] [ ] [ ]\n",
    "       [ ] [ ]\n",
    "\n",
    "    So we make a 2x6 grid with 5 graphs arranged on it. 3 in the\n",
    "    top row, 2 in the second row\n",
    "\n",
    "      0 1 2 3 4 5\n",
    "    0|[|]|[|]|[|]|\n",
    "    1| |[|]|[|]| |\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(15,9))\n",
    "    fig.set_tight_layout(True)\n",
    "    gs = fig.add_gridspec(2, 4)\n",
    "    ax1 = fig.add_subplot(gs[0, :2]) # row 0, cols 0-1\n",
    "    ax2 = fig.add_subplot(gs[0, 2:4])# row 0, cols 2-3\n",
    "    #ax3 = fig.add_subplot(gs[0, 4:]) # row 0, cols 4-5\n",
    "    ax4 = fig.add_subplot(gs[1, 1:3])# row 1, cols 1-2\n",
    "    #ax5 = fig.add_subplot(gs[1, 3:5])# row 1, cols 3-4\n",
    "    return fig, [ax1, ax2, ax4]\n",
    "\n",
    "def plot_distribution_of_column_by_category(column, axes, title=\"Word Frequency for\"):\n",
    "    for index, category in enumerate([0, 1, 2]):\n",
    "        # Calculate frequency distribution for this subset\n",
    "        all_words = tweets[tweets[\"label\"] == index][column].explode()\n",
    "        freq_dist = FreqDist(all_words)\n",
    "        top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "        tokens = top_10[0]\n",
    "        counts = top_10[1]\n",
    "\n",
    "        # Set up plot\n",
    "        ax = axes[index]\n",
    "        ax.hist(tokens, counts)\n",
    "\n",
    "        # Customize plot appearance\n",
    "        ax.set_title(f\"{title} {category}\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.tick_params(axis=\"x\", rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "all_words = []\n",
    "for tweet in tweets['tweet_text']:\n",
    "    all_words.extend(tweet_tokenizer(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = set(all_words)\n",
    "len(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist()\n",
    "for word in all_words:\n",
    "    fdist[word.lower()] += 1\n",
    "    \n",
    "fdist.plot(20, title = 'Frequency of Top 20 Words in Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for excessive puntuation - no go\n",
    "def punc_count(tweet):\n",
    "    punctuations = '!$%&()*+,-./:;<=>?[\\]^_`{|}~'\n",
    "    count = 0\n",
    "    for p in punctuations:\n",
    "        count += tweet.count(p)\n",
    "    return count\n",
    "\n",
    "#Look for excessive !/?'s' - no go\n",
    "def exc_que_count(tweet):\n",
    "    punctuation = '!?'\n",
    "    count = 0\n",
    "    for p in punctuation:\n",
    "        count += tweet.count(p)\n",
    "    return count\n",
    "\n",
    "#only periods\n",
    "def period_count(tweet):\n",
    "    punctuation = '.'\n",
    "    count = 0\n",
    "    for p in punctuation:\n",
    "        count += tweet.count(p)\n",
    "    return count\n",
    "\n",
    "#Ratio capital to length tweet\n",
    "def capital_letter_ratio(tweet):\n",
    "    capital_count = 0\n",
    "    for c in tweet:\n",
    "        if c.isupper():\n",
    "            capital_count += 1\n",
    "    return capital_count / len(tweet)\n",
    "\n",
    "#Repeating words - fix to be adjacent\n",
    "def any_repeats(tweet):\n",
    "    if len(set(tweet.split())) < len(tweet.split()):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "    \n",
    "#Hashtag count\n",
    "def count_hash(tweet):\n",
    "    hashtag = re.findall(r'(#w[A-Za-z0-9]*)', tweet)\n",
    "    return len(hashtag)\n",
    "\n",
    "#Average word length\n",
    "def avg_length(tweet):\n",
    "    char = len(tweet)\n",
    "    word = len(tweet.split())\n",
    "    return char / word\n",
    "\n",
    "#Number of words\n",
    "def word_count(tweet):\n",
    "    return len(tweet.split())\n",
    "\n",
    "#Add in if tweet about Apple or Google?\n",
    "def what_company(tweet):\n",
    "    tweet_check = tweet.lower()\n",
    "    if ('iphone' in tweet_check) or ('ipad' in tweet_check) or ('apple' in tweet_check) or ('#apple' in tweet_check):\n",
    "        if ('android' in tweet_check) or ('google' in tweet_check) or ('#google' in tweet_check):\n",
    "            return 'Both'\n",
    "        return 'Apple'\n",
    "    if ('android' in tweet_check) or ('google' in tweet_check) or ('#google' in tweet_check):\n",
    "        return 'Google'\n",
    "    return 'Neither'\n",
    "\n",
    "#Add in what service/product talk about?\n",
    "def what_product(tweet):\n",
    "    tweet_check = tweet.lower()\n",
    "    if ('app' in tweet_check):\n",
    "        return 'App'\n",
    "    if ('iphone' in tweet_check) or ('phone' in tweet_check) or ('android' in tweet_check):\n",
    "        return 'Phone'\n",
    "    if ('ipad' in tweet_check):\n",
    "        return 'iPad'\n",
    "    if ('apple' in tweet_check):\n",
    "        return 'Company'\n",
    "    else:\n",
    "        return 'General'\n",
    "    \n",
    "('google' in tweet_check) or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.emotion_in_tweet_is_directed_at.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['what_product'] = tweets['tweet_text'].apply(what_product)\n",
    "tweets.what_product.value_counts()\n",
    "# no_cat = tweets[tweets['what_product'] == 'General']\n",
    "# for tweet in no_cat['tweet_text']:\n",
    "#     print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get rid of bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['word_count'] = tweets['tweet_text'].apply(word_count)\n",
    "tweets['word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets['word_count'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[tweets['word_count'] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative = tweets[tweets.label == 0]\n",
    "# tweet_tokenizer(negative['tweet_text'][291])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Different Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['punc_count'] = tweets['tweet_text'].apply(punc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets.groupby('label')['exc_que_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, axes = setup_three_subplots()\n",
    "# plot_distribution_of_column_by_category(\"punc_count\", axes, \"Freqency of Posts Containing Prices for\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Each Feature With Logreg Cause Visuals Are Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test = tweets.drop('tweet_text', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "y = tweets_test['label']\n",
    "X = tweets_test.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "features = imbpipeline(steps=[\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(max_depth = 5, random_state = 213))\n",
    "])\n",
    "\n",
    "features.fit(X_train, y_train)\n",
    "for name, importance in zip(X_train.columns, features['dtc'].feature_importances_):\n",
    "    print(name, importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "col_labels = list(X_train.columns)\n",
    "col_labels.remove('tweet_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Vectorize the words\n",
    "cv = CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer)\n",
    "X_train_cv = cv.fit_transform(X_train['tweet_text'])\n",
    "\n",
    "#Scales the non-word columns\n",
    "X_train_nowords = X_train[col_labels]\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(ss.fit_transform(X_train_nowords),columns = X_train_nowords.columns, index = X_train.index)\n",
    "\n",
    "#Combines the scaled and vectorized data together\n",
    "X_train_cv_df = pd.DataFrame(X_train_cv.toarray(), columns=cv.get_feature_names(), index = X_train.index)\n",
    "X_train_final = pd.concat([X_train_cv_df, X_train_scaled], axis=1)\n",
    "\n",
    "#SMOTE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train['tweet_text'])\n",
    "\n",
    "#Scales the non-word columns\n",
    "X_train_nowords = X_train[col_labels]\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(ss.fit_transform(X_train_nowords),columns = X_train_nowords.columns, index = X_train.index)\n",
    "\n",
    "#Combines vectorized and scaled data together\n",
    "X_train_tfidf_df = pd.DataFrame(X_train_tfidf.toarray(), columns = tfidf.get_feature_names(), index = X_train.index)\n",
    "X_train_final = pd.concat([X_train_tfidf_df, X_train_scaled], axis=1)\n",
    "\n",
    "#SMOTE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Bayes - ONLY TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Train test split\n",
    "y = tweets['label']\n",
    "X = tweets['tweet_text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "#Vectorize the words\n",
    "cv = CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer)\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "\n",
    "#Fit and print model scores\n",
    "first_pass = MultinomialNB()\n",
    "first_pass.fit(X_train_cv, y_train)\n",
    "print(\"Training Score:\", first_pass.score(X_train_cv, y_train))\n",
    "scores = np.mean(cross_val_score(first_pass, X_train_cv, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models - can add other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE/TESTING AREA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IF WANT TRY PIPELINE/ADD FEATURE THINGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "count_vec = FeatureUnion([\n",
    "        ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer))\n",
    "        #, add any feature creation things here\n",
    "    ])\n",
    "\n",
    "tfidf_vec = FeatureUnion([\n",
    "        ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer))\n",
    "        #, add any feature creation things here\n",
    "    ])\n",
    "\n",
    "first_pass = Pipeline(steps=[\n",
    "    ('vec', count_vec),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc_count(tweet):\n",
    "    punctuations = '!$%&()*+,-./:;<=>?[\\]^_`{|}~'\n",
    "    count = 0\n",
    "    for p in punctuations:\n",
    "        count += tweet.count(p)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class PuncCount(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return pd.Series(X).apply(punc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tweets['label']\n",
    "X = tweets['tweet_text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "\n",
    "count_vec = FeatureUnion([\n",
    "        ('punc', PuncCount()),\n",
    "        ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer))\n",
    "    ])\n",
    "\n",
    "first_pass = Pipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    #('ss', StandardSclaer()) - only if added features on diff scale\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "first_pass.fit(X_train, y_train)\n",
    "print(\"Training Score:\", first_pass.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(first_pass, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PuncCount()\n",
    "pc.fit(X_train)\n",
    "test = pc.transform(X_train)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "y = tweets['label']\n",
    "X = tweets['tweet_text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "baseline = imbpipeline(steps=[\n",
    "    ('preproc', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213, max_depth = 5))\n",
    "])\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "print(\"Training Score:\", baseline.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(baseline, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making final dataset\n",
    "tweets['capital_letter_ratio'] = tweets['tweet_text'].apply(capital_letter_ratio)\n",
    "tweets['exc_que_count'] = tweets['tweet_text'].apply(exc_que_count)\n",
    "tweets['what_company'] = tweets['tweet_text'].apply(what_company)\n",
    "tweets['what_product'] = tweets['tweet_text'].apply(what_product)\n",
    "tweets = tweets.drop('word_count', axis = 1)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "col_labels = list(X_train.columns)\n",
    "col_labels.remove('tweet_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB - No Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = True, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_company', 'what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "first = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "first.fit(X_train, y_train)\n",
    "print(\"Training Score:\", first.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(first, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = True, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_company', 'what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "first = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    #('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "first.fit(X_train, y_train)\n",
    "print(\"Training Score:\", first.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(first, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__cv__min_df': [1, .1, 0],\n",
    "    'preproc__cv__max_df': [.8, .9, 0],\n",
    "    'preproc__cv__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(first, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = True, tokenizer = tweet_tokenizer,\n",
    "                          min_df = 1, max_features = 8000, ngram_range = (1,2)), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_company', 'what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "first = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    #('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "first.fit(X_train, y_train)\n",
    "print(\"Training Score:\", first.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(first, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - No Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "second = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('logreg', LogisticRegression(random_state = 213, max_iter = 1000))\n",
    "])\n",
    "\n",
    "second.fit(X_train, y_train)\n",
    "print(\"Training Score:\", second.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(second, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logreg - gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Creates parameters to test\n",
    "params = {\n",
    "    'logreg__C': [1.0, 1e3, 1e6],\n",
    "    'logreg__solver': ['lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(second, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This would be where our tuned logreg went - if we had one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unknown tuned (?) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB - Gridsearch (lowercase/min max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__cv__lowercase': [True, False],\n",
    "    'preproc__cv__min_df': [0, .05, .1],\n",
    "    'preproc__cv__max_df': [1, .95, .9]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(first, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned MNB\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          max_df = .95), 'tweet_text')],\n",
    "    #('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "tuned_mnb = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "tuned_mnb.fit(X_train, y_train)\n",
    "print(\"Training Score:\", tuned_mnb.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(tuned_mnb, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB - gridsearch (ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__cv__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(tuned_mnb, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          max_df = .95, ngram_range = (1,2)), 'tweet_text')],\n",
    "    #('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "ngram = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "ngram.fit(X_train, y_train)\n",
    "print(\"Training Score:\", ngram.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(ngram, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB - gridsearch ngram max features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__cv__max_features': [None, 4000, 8000],\n",
    "    'preproc__cv__max_df': [.3, .5, .8],\n",
    "    'preproc__cv__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(ngram, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          ngram_range = (1,1), max_df = .7, max_features = 8000), 'tweet_text')],\n",
    "    #('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "test = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "test.fit(X_train, y_train)\n",
    "print(\"Training Score:\", test.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(test, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text')],\n",
    "    #('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "tree = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213))\n",
    "])\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Training Score:\", tree.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(tree, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'dtc__max_depth': [10, 20, 30],\n",
    "    'dtc__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(tree, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min sample split = 5/max_depth = 20 has less overfitting\n",
    "tree = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213, min_samples_split = 2, max_depth = 30))\n",
    "])\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Training Score:\", tree.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(tree, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTC w/ bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          ngram_range = (1,2)), 'tweet_text')],\n",
    "    #('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "dtc_test = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213))\n",
    "])\n",
    "\n",
    "dtc_test.fit(X_train, y_train)\n",
    "print(\"Training Score:\", dtc_test.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(dtc_test, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'dtc__max_depth': [20, 30, 40],\n",
    "    'dtc__criterion': ['entropy', 'gini']\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(dtc_test, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          ngram_range = (1,2)), 'tweet_text')],\n",
    "    #('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "dtc_test = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213, max_depth = 30, max_features = 3000))\n",
    "])\n",
    "\n",
    "dtc_test.fit(X_train, y_train)\n",
    "print(\"Training Score:\", dtc_test.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(dtc_test, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logreg with bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          ngram_range = (1,2), max_features = 1500), 'tweet_text'),\n",
    "    ('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "logreg_worse = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state = 213)),\n",
    "    ('logreg', LogisticRegression(random_state = 213, max_iter = 1000))\n",
    "])\n",
    "\n",
    "logreg_worse.fit(X_train, y_train)\n",
    "print(\"Training Score:\", logreg_worse.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(logreg_worse, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer,\n",
    "                          ngram_range = (1,2)), 'tweet_text'),\n",
    "    ('scale', StandardScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "logreg_bi = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state = 213)),\n",
    "    ('logreg', LogisticRegression(random_state = 213, max_iter = 1000))\n",
    "])\n",
    "\n",
    "logreg_bi.fit(X_train, y_train)\n",
    "print(\"Training Score:\", logreg_bi.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(logreg_bi, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__cv__ngram_range': [(1,1), (1,2), (2,2)],\n",
    "    'preproc__cv__max_features': [None, 1000, 2000]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(logreg_bi, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "all_words = []\n",
    "for tweet in tweets['tweet_text']:\n",
    "    all_words.extend(tweet_tokenizer(tweet))\n",
    "    \n",
    "\n",
    "vocab_mix = set(all_words)\n",
    "test = []\n",
    "for word in vocab_mix:\n",
    "    test.append(word.lower())\n",
    "vocab = set(test)\n",
    "\n",
    "glove = {}\n",
    "with open('glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in vocab:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "y = tweets['label']\n",
    "X = tweets['tweet_text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "#Pick a Pipeline to try\n",
    "w2v_rfc = Pipeline([\n",
    "    ('Word2Vec', W2vVectorizer(glove)),\n",
    "    ('rf', RandomForestClassifier(random_state = 42))\n",
    "])\n",
    "\n",
    "w2v_svc = Pipeline([\n",
    "    ('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "    ('Support Vector Machine', SVC())\n",
    "])\n",
    "\n",
    "w2v_logreg = Pipeline([\n",
    "    ('Word2Vec', W2vVectorizer(glove)),\n",
    "    ('lr', LogisticRegression(max_iter = 1000, random_state = 42))\n",
    "])\n",
    "\n",
    "models = [('Random Forest', w2v_rfc),\n",
    "          ('Support Vector Machine', w2v_svc),\n",
    "          ('Logistic Regression', w2v_logreg)]\n",
    "\n",
    "scores = [(name, cross_val_score(model, X_train, y_train, cv = 3).mean()) for name, model, in models]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'rf__n_estimators': [100, 300, 500],\n",
    "    'rf__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(w2v_rfc, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tuned = Pipeline([\n",
    "    ('Word2Vec', W2vVectorizer(glove)),\n",
    "    ('rf', RandomForestClassifier(n_estimators = 300, random_state = 42))\n",
    "])\n",
    "\n",
    "w2v_tuned.fit(X_train, y_train)\n",
    "print(\"Training Score:\", w2v_tuned.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(w2v_tuned, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_rfc = imbpipeline([\n",
    "    ('Word2Vec', W2vVectorizer(glove)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('rf', RandomForestClassifier(random_state = 42))\n",
    "])\n",
    "\n",
    "w2v_svc = imbpipeline([\n",
    "    ('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('Support Vector Machine', SVC())\n",
    "])\n",
    "\n",
    "w2v_logreg = imbpipeline([\n",
    "    ('Word2Vec', W2vVectorizer(glove)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('lr', LogisticRegression(max_iter = 1000, random_state = 42))\n",
    "])\n",
    "\n",
    "models = [('Random Forest', w2v_rfc),\n",
    "          ('Support Vector Machine', w2v_svc),\n",
    "          ('Logistic Regression', w2v_logreg)]\n",
    "\n",
    "scores = [(name, cross_val_score(model, X_train, y_train, cv = 3).mean()) for name, model, in models]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates parameters to test\n",
    "params = {\n",
    "    'rf__n_estimators': [500, 800, 1000],\n",
    "    'rf__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(w2v_rfc, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tuned = Pipeline([\n",
    "    ('Word2Vec', W2vVectorizer(glove)),\n",
    "    ('rf', RandomForestClassifier(n_estimators = 800, min_samples_split = 2, random_state = 42))\n",
    "])\n",
    "\n",
    "w2v_tuned.fit(X_train, y_train)\n",
    "print(\"Training Score:\", w2v_tuned.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(w2v_tuned, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting only Apple Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iPad                               946\n",
       "Apple                              661\n",
       "iPad or iPhone App                 470\n",
       "Google                             430\n",
       "iPhone                             297\n",
       "Other Google product or service    293\n",
       "Android App                         81\n",
       "Android                             78\n",
       "Other Apple product or service      35\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emotion_in_tweet_is_directed_at'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_labels = df[df['emotion_in_tweet_is_directed_at'].isna()]\n",
    "no_labels = no_labels.dropna(subset = ['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = []\n",
    "for tweet in no_labels['tweet_text']:\n",
    "    tweet_check = tweet.lower()\n",
    "    if ('iphone' in tweet_check) or ('ipad' in tweet_check) or ('apple' in tweet_check):\n",
    "        tweet_list.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Need to buy an iPad2 while I'm in Austin at #s...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RT @LaurieShook: I'm looking forward to the #S...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The best!  RT @mention Ha! First in line for #...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{link} RT @mention 1st stop on the #SXSW #Chao...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text  \\\n",
       "2   @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "20  Need to buy an iPad2 while I'm in Austin at #s...   \n",
       "25  RT @LaurieShook: I'm looking forward to the #S...   \n",
       "36  The best!  RT @mention Ha! First in line for #...   \n",
       "57  {link} RT @mention 1st stop on the #SXSW #Chao...   \n",
       "\n",
       "   emotion_in_tweet_is_directed_at  \\\n",
       "2                             iPad   \n",
       "20                            iPad   \n",
       "25                            iPad   \n",
       "36                            iPad   \n",
       "57                            iPad   \n",
       "\n",
       "   is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "2                                    Positive emotion  \n",
       "20                                   Positive emotion  \n",
       "25                                   Positive emotion  \n",
       "36                                   Positive emotion  \n",
       "57                                   Positive emotion  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipad = df[df['emotion_in_tweet_is_directed_at'] == 'iPad']\n",
    "apple = df[df['emotion_in_tweet_is_directed_at'] == 'Apple']\n",
    "mix = df[df['emotion_in_tweet_is_directed_at'] == 'iPad or iPhone App']\n",
    "iphone = df[df['emotion_in_tweet_is_directed_at'] == 'iPhone']\n",
    "apps = df[df['emotion_in_tweet_is_directed_at'] == 'Other Apple product or service']\n",
    "\n",
    "unlabeled_apple = df[df['tweet_text'].isin(tweet_list)]\n",
    "unlabeled_apple = unlabeled_apple.drop_duplicates(subset = 'tweet_text')\n",
    "\n",
    "final_df = pd.concat([ipad, apple, mix, iphone, apps, unlabeled_apple], axis = 0)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    0.519993\n",
       "Positive emotion                      0.386466\n",
       "Negative emotion                      0.075629\n",
       "I can't tell                          0.017912\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.529477\n",
       "1    0.393515\n",
       "0    0.077008\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = final_df.drop('emotion_in_tweet_is_directed_at', axis = 1)\n",
    "tweets = tweets[tweets['is_there_an_emotion_directed_at_a_brand_or_product'] != 'I can\\'t tell']\n",
    "tweets['label'] = tweets['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "tweets = tweets.drop('is_there_an_emotion_directed_at_a_brand_or_product', axis = 1)\n",
    "tweets = tweets.dropna()\n",
    "tweets.label = tweets.label.map({'Negative emotion' : 0, 'Positive emotion': 1, \n",
    "                                 'No emotion toward brand or product': 2})\n",
    "\n",
    "tweets.label.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin can not wait for #ipad 2 also. the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>need to buy an ipad2 while i'm in austin at #s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rt @laurieshook: i'm looking forward to the #s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>the best!  rt @mention ha! first in line for #...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{link} rt @mention 1st stop on the #sxsw #chao...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text  label\n",
       "2   @swonderlin can not wait for #ipad 2 also. the...      1\n",
       "20  need to buy an ipad2 while i'm in austin at #s...      1\n",
       "25  rt @laurieshook: i'm looking forward to the #s...      1\n",
       "36  the best!  rt @mention ha! first in line for #...      1\n",
       "57  {link} rt @mention 1st stop on the #sxsw #chao...      1"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweets = tweets.copy()\n",
    "retweets['tweet_text'] = retweets['tweet_text'].str.lower()\n",
    "retweets['tweet_text'].value_counts()\n",
    "retweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    3939\n",
       "1    1489\n",
       "Name: retweet, dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is making a new column indicating if a line is a retweet or not\n",
    "# tweets are scanned for \"rt\" to identify them as retweets\n",
    "\n",
    "counter=0\n",
    "retweet_list = []\n",
    "\n",
    "for i in retweets['tweet_text']:\n",
    "    rt = False\n",
    "    if i.startswith('rt ') or ' rt ' in i or (i.endswith(' rt')):\n",
    "        rt = True\n",
    "    for pos in [' rt.',' rt, ']:\n",
    "        if pos in i:\n",
    "            rt = True\n",
    "    if rt:\n",
    "        retweet_list.append(1)\n",
    "        counter += 1\n",
    "    else:\n",
    "        retweet_list.append(0)\n",
    "\n",
    "retweets['retweet'] = retweet_list\n",
    "print(counter)\n",
    "retweets['retweet'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tweets_ct = len([0 for i in retweets['retweet'] if i == 0])\n",
    "retweets_ct = len([1 for i in retweets['retweet'] if i == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEVCAYAAAAM3jVmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAApkElEQVR4nO3defwWZb3/8ddbcCGX1ESPAorHQyVaURJatnjSkqzESk+YppUdyqOVv+qUeuxEC+lpszylZWVoi0anRTOtyMIyt74WLmgm5QKCggsKZS74+f1xfb4x3Nzbd+ELMu/n43E/7rmvuWbmmu1zz1wzc40iAjMzq4eN1nUBzMxs6Djom5nViIO+mVmNOOibmdWIg76ZWY046JuZ1YiD/iCQdISkn6/rcvSSNELSjyU9JOl7/Rh+haR/HoRyvFXSFQMdj61/JN0h6YB1XQ7ru/Uq6Et6s6SeDDqLJV0q6SXrulydRMS3I+JV67ocFYcCOwDPiIjDGntK2lrSOZLukbRc0p8kfai3f0RsERF/GcoCN5RvXm4DKyStlPT3yu+T1/K050h6R4c8m0iaLuk2SX/NAHiOpLFruWz7SVq4NqcxGCTNlPRYrq8HJM2W9Owuh50u6Vtru4wN0xwrKSQN75DvmZK+J+m+PKC6QdL7JA1by+WbKekTgzW+9SboS3of8Hngk5SAtTNwJjBlHRaro04byjqyC/CniHiiRf/TgS2A3YGnAwcDfx6isnUUEXvkH88WwG+A43t/R8Qn13X5gP+jLLM3U5bf84DrgP3XZaHWM5/K9TcKuBv4+jouz4BI2g24BlgAPCcing4cBkwEtlyXZeuziFjnH8qOswI4rE2eTSl/Covy83lg0+y3H7AQ+CCwBFgMHAIcBPwJeAA4uTKu6ZQd97vAcuD3wPMq/U+kBMHlwM3A6yv93gr8lhI4HwA+kWlXZH9lvyXAQ8ANwJ6V+TwPWArcCZwCbFQZ7xXAZ4AHgduBV7dZHrsDc4BlwDzg4Ez/KPAY8Hgu02OaDHsTcEibcQfwL9k9E/gS8JNcHtcAu1Xyvgq4Nef1TOBy4B3VearkfTYwO5fbrcC/dbFtzKmM705gr+w+Mss5Pn+/A/hRdm9UWYf3A7OAbSvj3Ae4Mpfd9cB+mT4DWAn8PZfdF5uU5wDgEWBMmzLvBFyU8zkf+PdKv5nAJyq/9wMWVn7fAXwgt5uHKNvoZsDmOd0ns2wrcjqTgB7gYeBe4HMtyrQNcDFl23swu0c3LOePU7bt5cDPge0q/d+Sy/9+4L+ynAe0mFbjPB4E/LVh+Xw/y3I78J5Mn8zq2+71wL8CN1aG/QVwbeX3FeS23Gq8nbYJ4K7clnqX64uazNO3gJ902FYPpuyLy3J57t5sn2pcRqyKX+9nVfx6W/ablsvjsSzbjzP9Q5Q/0+WUfWn/ruNttxnX5idX9hPA8DZ5PgZcDWwPjKTstB+vLLQngP8GNgb+PVf8dyj/wntQduR/zvzTc0Eemvk/kBvJxtn/sNyANgLeBPwV2DH7vTWn9W5gODCC1YP+gZSjvq0pfwC7V4Y9D7gwyzSW8od0TGW8j2fZhwHHUv7c1GRZbEwJJicDmwCvyJX/rMr8favNsvxabpxvA8Y16d8Y9B+gBJfhwLeBC7LfdpRg84bs996chzWCPiVoLchpDgdeANwH7NFh25hTGd95wPuz+2zKDnxspd//y+4TKNvKaMrBwleA87PfKMpOf1Cu31fm75GN02tRntOAyzuU+XLKH+BmwATKtrh/485e3eErv+8ArqVsf9sCtwDvapY3064C3pLdWwD7tCjTM4A3Ak+jbH/fI/8kK/P9Z+CZlG16DnBa9htPCTgvy+X5Oco+0DHo53r/JnB9/t6Isn/8N2Xb/WfgL8CBzbbdXIaPULa14cA9lP1iyyznIzlvncbbbpsYS9nm28Wfe8hA3KL/Mylx4pWU/fODlH10k8Z9qsky2i+X58dy2IOAvwHbtNhmnkXZl3aqlH+3VmVbo6zdZlybH+AI4J4Oef4MHFT5fSBwR2WhPQIMy99b5kLeu5L/OlYdEUwHrq7024jy7/rSFtOeC0zJ7rcCdzX0fyurgtsrKMF8H/IoPtOHAY+SR6aZ9k5gTmUc8yv9npbz8E9NyvPS3Air4z8fmN5sx2ky/AjKH8Z1lCA9n8pZBWsG/a9V+h0E/DG7jwKuqvRTbozNgv6bgN80lOMrwEc6rPc5lfEdA1yU3bdQju57/4DuBF5Q6bd/ZRw75nwOpxwhfbNhGj8Djm6cXovyfLV3mi36j6GcLWxZSTsVmFlZnp2C/pGV358Cvtwsb6b9mnJ2t12rMrUo5wTgwYblfErl938AP83u/67OMyWQP0b7oP93yhHvk5QDqudmv71Zc/85CfhGq22XUsX3Bso+9XPKUfpkylnADV2Ot902MZbOQf9xYHKb/h8GZlV+b0Q5Et+vcZ9q3A5YFb+GV/ovIf/Am2wz/5L9DyAPVPvyWV/q9O8HtutQP74TZcfudWem/WMcEbEyux/J73sr/R+hHAn1WtDbERFPUk6vdgKQdJSkuZKWSVoG7Ek50lhj2EYR8Uvgi5QqkXslnS1pqxx+kybzMKry+57KeP6WndUy99oJWJDlbjWuliLikYj4ZETsRTlKmgV8T9K2LQa5p9L9t0qZdmL15RiU5djMLsDevcs0l+sRwD91U+Z0OfBSSf9E+RP9LrBvXkB9OuXPuXdaP6xM5xZKIN4h+x3WUI6XUIJAN+7vkHcn4IGIWF5J63rdpFbLu5ljKEeZf5T0O0mvbZZJ0tMkfUXSnZIepvxZbN1wEbLb9fxXynJo5zMRsTUloD5COTqFsvx3alj+J1PWTSuXUwLjy7J7DvDy/Fze5XjbbRPd6Ga9/2Pfzn1zAd2v9/tj9WtwLdd7RMynnLlMB5ZIukDSTs3yNrO+BP2rKEcGh7TJs4iy4nrtnGn9Naa3Q9JGlNO+RZJ2oRzNHU+5+2VrSh24KsNGuxFHxBkZUPeg7JD/SanKeLzJPNzdj7IvAsZkuQc0roh4mHLxfHNg1z4Ovpiy3ACQpOrvBgso1SJbVz5bRMSxfSjrfMrO8B7g1xlY76HUe15R+RNcQDlzqU5rs4i4O/t9s6Hf5hFxWu9kOhTjF8AkSa3mcxGwraTqxb3quvkr5SyuV1/+9NYoW0TcFhGHU6o9/wf4P0mbNxn2/ZTAu3dEbEUJoLD6dt3KYlbfX55GOVjoXOCIuyjVfl+QNIKy/G9vWP5bRsRBreaRNYP+5awZ9DuNt9020WmdQ1nvb2zTf7X4lPvCGFat978xuOv9OxHxkpxmUNZ9V9aLoB8RD1FOIb8k6ZA8KtlY0qslfSqznQ+cImmkpO0y/0Bu7dpL0hvy7OIEStXL1ZTgF5R6WCS9jXKk3xVJL5S0t6SNKTv434GVeRYyC5ghacv8c3lfP+fhmhz3B3M57Qe8DrigyzJ+OMu5iaTNKDvlMsoFob74CfCcXGfDgeNovTFfDDxT0luyzBtnGXbv4zQvp/wh9+7scxp+A3yZspx3AchtZkr2+xbwOkkHShomabO8FbI3iN9LqQ9uKiJ+QbkY/UNJe0kanuvzXZLeHhELKNebTs1xP5dyNP7tHMVc4CBJ2+YZywl9mPd7gWdIenpvgqQjJY3MP7xlmbyyybBbUo64l+UZ3Uf6MN3/A14r6SWSNqHUPXcdOyJiNiUoTqNcr3hY0odUnicZJmlPSS+szOPYhgOaKyl/WJMoF3HnkWeOlDMWuhhvu21iKaUaqt2zKR8BXizp07nekPQvkr4laWvKvv0aSfvnvv9+Sky5MoefC7w5yzWZ8ofVrdW2SUnPkvQKSZtS4ssjNF/nTa0XQR8gIj5HCYKnUFbCAsrO/KPM8gnKXQo3ADdS7rgZyL2rF1LqmR+k3Jnwhoh4PCJuBj5LOfu4F3gO5Y6Gbm1FOVN4kFV3O3wm+72bEqz/Qrnr4DvAOX0teEQ8RrlT4NWUM4gzgaMi4o/djgL4Rg67iHLx6TURsaKP5biPctH7U5T5HE9ZR482ybuccqfP1JzmPZSjk037Mk1KcN+SVTt742+AL1Dunvm5pOWUP/O9sxwLKLcBn8yq7ew/WbUvfAE4VNKDks5oUYZDgUso1UsPUc4EJ1KOBgEOp1RrLAJ+SLluMTv7fZNyV8odlPrp73Y747l+zwf+ktUUO1HqtudJWpFlnxoRf28y+Ocp13Luy+Xx0z5Mdx7lD/07lKP+B2ldjdfKpykXN4dTDlAmUOr676PcWND7R9b7MOH9kn6f0/8rZX+fl9s+lP3zzohYknlWdhhvu23ib5Q7t36by3WfJsvgz8CLKOt1nqSHKHcK9QDLI+JWyh1l/5vTfh3wukp535tpyyjVmj/qftHxdWB8lu1HlH3mtJzOPZSzvK6fX1FeGKgVSdMpF1WOXNdl2ZDk0dlC4IiI+NW6Lo+ZrWm9OdK3p6asJtk6TzVPptQRX72Oi2VmLTjo20C9iHI7be8p7SER8Uj7QcxsXdnggr6kUyWd0C5PREzvtmpH0pclfbhN/5Mlfa2PxawO/5RuuCqX5TPyTom9I+KaoZy+pJdK6usF6Gbj6djmTpth+91eTLthB2veWox7Xt4AMNDxPCXaA3oqk3SwpK5u0ujGBhX0JY2kPDD0lfy9n6Q52R2VfF3v4BHxroj4eGV8Cxv6fzIi3pH9u2q4qU35p+fnH+VukmdA0+ivgQTFAU637Z9iRPwmIp7Vqv9T2UDmLe/M+qykhSoNn90u6fTKuPeIiDmDVti+l+9SrWpE73GtaqBthaQvr+Vpd2zATMV7JN2k0qjeQpXG1p6zlsu2xv4dERcBe+adYAO2PjYWNhBvBS5x9YIZJ1HuKJpEueNmF1bdm7/ORcSre7slzaQ8aXzKuivRGr4AvIbSLMpvKQ8Dvj7TblwH5Tmfcsvr8QMd0QZ1pE+5hfHyjrkqeo/eJb1f0hKVJp3fVuk/U9InVB54uZTy1F/vEclODafnvbcNLsv+L5K0m6RfSrpfpUnWb+d9vf3VbBp3Story3tkHimMz9/vyNu8kLSRpBMl/TnLM0uVp3Al7SPpyrw17Pre039JMyhNP3wxp/nFPBI6PZfZQyrNzDZ9niHPEj4u6bcqTTn/XOVZi97+B2d1w7LMu3umf5PyYNOPc7ofbDLu1c6+8szgA1mehyR9V+VZhN7+U1Setn44l8PkJuNcrcql8ehL0q6SLs95mc3qT2u3XI7dDDuQeWvwQuCHEbEoijsi4ryGcR1Qmd9Zks7Lcs2TNLGS9wWS/pD9vpfTbXqknPvE9yUtVTm7eE+r+Wsx/OWS3pjdL8nlflD+PkDS3Eret0u6ReX22p8p78HPfs9WadL5AUm3Svq3TJ9GuWXyg7lN/bhJGcZRblE9PCJ+GRGPRsTfojShflrmeXour6Uq+98pymcLuth+2u0Pa+zf+XsO5Q9n4KKP7Taszx/Kfdcv7CLfHFa157IfXTZ2RPO2T6aTbYXQpA0PSjsZr6TcWzsyV+rnK/3voEUbJi3K3mwaQ94QGW0almuxvFs15tWpoaq2y6dxndC+wbJJlPvqX5nzOAp4dpNt4h/rtNkyp9wj/rlchi+jNHb3rS6XY8thBzJvTYY9hdJ65H9QnjVRQ/9/LNec379nmYdR2gq6Ovv1Nh3y3lw/b6C0u7PGPkGHRs/arMOZlfF9DPjf7D6Zst38T6XfF7L7kNxOdqfUWJwCXJn92jbuR0NbNk3K8y7KMwDtytyu8cRO288cWu8Pq+WtjGPbTN+qv/Gx97OhHelvTdmJ+upx4GNRHs66hNKi4KDUE0fE/IiYHeVoYSllh+/L03jd6H0sHcoR+amV39VH1d8J/FdELIyIRykb56F5BHIkpWrskoh4MsrDRD2UQNDM45QN/tmUgHJLRCxuU8ZvRMSfolS9zaI8RAPlAbmf5DJ6nPIg2wjgxd3P/hrOiHKE+wDw48q0jgHOyWk9GRF3R/cPtAEgaWfKUfSHc53+OqfRq+Vy7GLYgcxbo1MpD78dkdO/W9LRbcZ7RZZ5JeUBsudl+j6UwHlG7h8/oPzxNPNCyp/bxyLisSgv4vkq5YG8blW35ZfRfls+Nbe7JyhNiUzIo/3XUhpj/EZEPBERv6c8SHVol2V4BqVKrCmV9oreBJwUEcsj4g7KA51v6XL80Hp/aKU3rm3dh2k0taEF/Qfp3wsNum7sqK8kba/SINLdKg1dfYs2p/T9NOQNkUXrhuVaadeY10AaqurLtMYw8JfF7ERpnfKvlbRqI3rtlmOnYbvRVWNsEbEyIr4UEftSAsUM4By1bvaicbyb5cHATsDdkYebqVWDg/1pTK3RVZTmOnagBMLzKO1MbUc5U+ut/tiF0p5P73QeoJxxjmLgjft1alytm8YTO+lLo3qwKq4t68M0mtrQgv4NlFOmtaXT48vN+p+a6c+N0tDVkXTXyFXX04h11BBZNG9Yrq86NVQ1mI+MLwB26yJfu0bRFgPbaPVGzXZumEar5dhp2LUiSquqX6IcFI3v4+CLgVG5XnqNaZG3U6Nn3ZT1b5QqovcCN0VpxuBKShMtf47S9EfvtN7ZMK0REXElnRv367RNXQaMrl7XaNCp8cRBbVQv7U45e3m4D+NqakML+pcw+FUnVWs0eNWgWcNNW1Kqi5ZJGkX/AmOnacAQN0SmFg3L9WN+OjVU1bYBtD76OvC2nNZGkkap+btb5wIvk7RzruuTentExJ2U6pKPqtwW+RLKQ2m9Wi7HLoYdNJJOyOmOUGkU7mjKtviHPo7qKsp6PT7HM4VyxN1Mp0bPutXttnySpD3gHxdWe98H3alxv06N6t1Gac/q/FyGm+R6nCrpxOjceOJcWmw/XWi1f7+cciPJgG1oQf88St3piLUx8mje4FW1f7OGmz5KuZD0EKVVyh8MsAytGoca6obI2jUs15f56dRQ1amU1lWXSfpAX8ffMK1rKRf3Tqesj8tZ/WitN99sShXZDZSjzosbsryZstweoLS+eF5l2E7LseWwg+wRSj3zPZTlehzwxujjC+9zPbyBcj1kGWVdXUzzRvU6NXrWrY7bckT8kHLN4oKsNr2Jcvce0blxv8YGzJp5D6uqL5dRqgVfz6prMC0bT+xi+2mpzf59OPn80UBtcA2uSfoksCQiPr+uy2K2IZJ0DeVtXt9Y12WpA0mvo7wS898GZXwbWtA3s8El6eWUdy3cR7kg+mXK+6bb3a1l66kN7YlcMxt8z6LUYW9BqeY41AH/qctH+mZmNbKhXcg1M7M21vvqne222y7Gjh27rothZvaUct11190XESMb09f7oD927Fh6enrWdTHMzJ5SJDV92tvVO2ZmNeKgb2ZWI10H/Xyk+g+SLs7f26q0V31bfm9TyXuSpPkq7VgfWEnfS9KN2e+MhvY8zMxsLevLkf57Ka0y9joRuCwixlEaKDoRQOXlHVMpjXBNBs5UaYoU4CxKI2Dj8rPGCyzMzGzt6SroZ6Nbr6G0o9FrCnBudp9LealBb/oF2V747ZQXHUyStCPlBQBXZTOt51WGMTOzIdDtkf7nKW80erKStkPvU3n5vX2mj2L19rYXZtqo7G5MNzOzIdIx6Et6LaUBs+u6HGezevpok95smtMk9UjqWbp0aZeTNTOzTro50t8XOFjSHcAFwCtUXvp7b1bZkN9LMv9CVn/JwmhK86YLs7sxfQ0RcXZETIyIiSNHrvFsgZmZ9VPHoB8RJ0XE6IgYS7lA+8uIOJLSLnvvOzePprwkmEyfKmlTSbtSLthem1VAyyXtk3ftHFUZxszMhsBAnsg9DZgl6RjgLuAwgIiYJ2kWcDPwBHBcvlwB4FjKm+hHUN4CMyhvgmlF339wbY7e+iHeuE3nTGa21vQp6EfEHMqry4iI+4H9W+SbQXn7S2N6D7BnXwtpZmaDw0/kmpnViIO+mVmNOOibmdWIg76ZWY046JuZ1YiDvplZjTjom5nViIO+mVmNOOibmdWIg76ZWY046JuZ1YiDvplZjTjom5nViIO+mVmNOOibmdWIg76ZWY046JuZ1UjHoC9pM0nXSrpe0jxJH8306ZLuljQ3PwdVhjlJ0nxJt0o6sJK+l6Qbs98Z+a5cMzMbIt28LvFR4BURsULSxsAVknrfbXt6RHymmlnSeMoL1PcAdgJ+IemZ+Z7cs4BpwNXAJcBk1vJ7cs3MbJWOR/pRrMifG+cn2gwyBbggIh6NiNuB+cAkSTsCW0XEVRERwHnAIQMqvZmZ9UlXdfqShkmaCywBZkfENdnreEk3SDpH0jaZNgpYUBl8YaaNyu7GdDMzGyJdBf2IWBkRE4DRlKP2PSlVNbsBE4DFwGcze7N6+miTvgZJ0yT1SOpZunRpN0U0M7Mu9OnunYhYBswBJkfEvfln8CTwVWBSZlsIjKkMNhpYlOmjm6Q3m87ZETExIiaOHDmyL0U0M7M2url7Z6SkrbN7BHAA8Meso+/1euCm7L4ImCppU0m7AuOAayNiMbBc0j55185RwIWDNytmZtZJN3fv7AicK2kY5U9iVkRcLOmbkiZQqmjuAN4JEBHzJM0CbgaeAI7LO3cAjgVmAiMod+34zh0zsyHUMehHxA3A85ukv6XNMDOAGU3Se4A9+1hGMzMbJH4i18ysRhz0zcxqxEHfzKxGHPTNzGrEQd/MrEYc9M3MasRB38ysRhz0zcxqxEHfzKxGHPTNzGrEQd/MrEYc9M3MasRB38ysRhz0zcxqxEHfzKxGHPTNzGrEQd/MrEYc9M3MaqSbF6NvJulaSddLmifpo5m+raTZkm7L720qw5wkab6kWyUdWEnfS9KN2e+MfEG6mZkNkW6O9B8FXhERzwMmAJMl7QOcCFwWEeOAy/I3ksYDU4E9gMnAmflSdYCzgGnAuPxMHrxZMTOzTjoG/ShW5M+N8xPAFODcTD8XOCS7pwAXRMSjEXE7MB+YJGlHYKuIuCoiAjivMoyZmQ2Brur0JQ2TNBdYAsyOiGuAHSJiMUB+b5/ZRwELKoMvzLRR2d2Y3mx60yT1SOpZunRpH2bHzMza6SroR8TKiJgAjKYcte/ZJnuzevpok95semdHxMSImDhy5MhuimhmZl3o0907EbEMmEOpi783q2zI7yWZbSEwpjLYaGBRpo9ukm5mZkOkm7t3RkraOrtHAAcAfwQuAo7ObEcDF2b3RcBUSZtK2pVywfbarAJaLmmfvGvnqMowZmY2BIZ3kWdH4Ny8A2cjYFZEXCzpKmCWpGOAu4DDACJinqRZwM3AE8BxEbEyx3UsMBMYAVyaHzMzGyIdg35E3AA8v0n6/cD+LYaZAcxokt4DtLseYGZma5GfyDUzqxEHfTOzGnHQNzOrEQd9M7MacdA3M6sRB30zsxpx0DczqxEHfTOzGnHQNzOrEQd9M7MacdA3M6sRB30zsxpx0DczqxEHfTOzGnHQNzOrEQd9M7MacdA3M6uRbt6RO0bSryTdImmepPdm+nRJd0uam5+DKsOcJGm+pFslHVhJ30vSjdnvjHxXrpmZDZFu3pH7BPD+iPi9pC2B6yTNzn6nR8RnqpkljQemAnsAOwG/kPTMfE/uWcA04GrgEmAyfk+umdmQ6XikHxGLI+L32b0cuAUY1WaQKcAFEfFoRNwOzAcmSdoR2CoiroqIAM4DDhnoDJiZWff6VKcvaSzlJenXZNLxkm6QdI6kbTJtFLCgMtjCTBuV3Y3pzaYzTVKPpJ6lS5f2pYhmZtZG10Ff0hbA94ETIuJhSlXNbsAEYDHw2d6sTQaPNulrJkacHRETI2LiyJEjuy2imZl10FXQl7QxJeB/OyJ+ABAR90bEyoh4EvgqMCmzLwTGVAYfDSzK9NFN0s3MbIh0c/eOgK8Dt0TE5yrpO1ayvR64KbsvAqZK2lTSrsA44NqIWAwsl7RPjvMo4MJBmg8zM+tCN3fv7Au8BbhR0txMOxk4XNIEShXNHcA7ASJinqRZwM2UO3+Oyzt3AI4FZgIjKHft+M4dM7Mh1DHoR8QVNK+Pv6TNMDOAGU3Se4A9+1JAMzMbPH4i18ysRhz0zcxqxEHfzKxGHPTNzGrEQd/MrEYc9M3MasRB38ysRhz0zcxqxEHfzKxGHPTNzGrEQd/MrEYc9M3MasRB38ysRhz0zcxqxEHfzKxGHPTNzGrEQd/MrEa6eUfuGEm/knSLpHmS3pvp20qaLem2/N6mMsxJkuZLulXSgZX0vSTdmP3OyHflmpnZEOnmSP8J4P0RsTuwD3CcpPHAicBlETEOuCx/k/2mAnsAk4EzJQ3LcZ0FTKO8LH1c9jczsyHSMehHxOKI+H12LwduAUYBU4BzM9u5wCHZPQW4ICIejYjbgfnAJEk7AltFxFUREcB5lWHMzGwI9KlOX9JY4PnANcAOEbEYyh8DsH1mGwUsqAy2MNNGZXdjupmZDZGug76kLYDvAydExMPtsjZJizbpzaY1TVKPpJ6lS5d2W0QzM+ugq6AvaWNKwP92RPwgk+/NKhvye0mmLwTGVAYfDSzK9NFN0tcQEWdHxMSImDhy5Mhu58XMzDro5u4dAV8HbomIz1V6XQQcnd1HAxdW0qdK2lTSrpQLttdmFdBySfvkOI+qDGNmZkNgeBd59gXeAtwoaW6mnQycBsySdAxwF3AYQETMkzQLuJly589xEbEyhzsWmAmMAC7Nj5mZDZGOQT8irqB5fTzA/i2GmQHMaJLeA+zZlwKamdng8RO5ZmY14qBvZlYjDvpmZjXioG9mViMO+mZmNeKgb2ZWIw76ZmY14qBvZlYjDvpmZjXioG9mViMO+mZmNeKgb2ZWIw76ZmY14qBvZlYjDvpmZjXioG9mViMO+mZmNeKgb2ZWI928GP0cSUsk3VRJmy7pbklz83NQpd9JkuZLulXSgZX0vSTdmP3OyJejm5nZEOrmSH8mMLlJ+ukRMSE/lwBIGg9MBfbIYc6UNCzznwVMA8blp9k4zcxsLeoY9CPi18ADXY5vCnBBRDwaEbcD84FJknYEtoqIqyIigPOAQ/pZZjMz66eB1OkfL+mGrP7ZJtNGAQsqeRZm2qjsbkxvStI0ST2SepYuXTqAIpqZWdXwfg53FvBxIPL7s8DbgWb19NEmvamIOBs4G2DixIkt85nZIPmOL7Gtd968dkJfv470I+LeiFgZEU8CXwUmZa+FwJhK1tHAokwf3STdzMyGUL+CftbR93o90Htnz0XAVEmbStqVcsH22ohYDCyXtE/etXMUcOEAym1mZv3QsXpH0vnAfsB2khYCHwH2kzSBUkVzB/BOgIiYJ2kWcDPwBHBcRKzMUR1LuRNoBHBpfszMbAh1DPoRcXiT5K+3yT8DmNEkvQfYs0+lMzOzQeUncs3MasRB38ysRhz0zcxqxEHfzKxGHPTNzGrEQd/MrEYc9M3MasRB38ysRhz0zcxqxEHfzKxGHPTNzGrEQd/MrEYc9M3MasRB38ysRhz0zcxqxEHfzKxGHPTNzGqkY9CXdI6kJZJuqqRtK2m2pNvye5tKv5MkzZd0q6QDK+l7Sbox+52R78o1M7Mh1M2R/kxgckPaicBlETEOuCx/I2k8MBXYI4c5U9KwHOYsYBrlZenjmozTzMzWso5BPyJ+DTzQkDwFODe7zwUOqaRfEBGPRsTtwHxgkqQdga0i4qqICOC8yjBmZjZE+lunv0NELAbI7+0zfRSwoJJvYaaNyu7G9KYkTZPUI6ln6dKl/SyimZk1GuwLuc3q6aNNelMRcXZETIyIiSNHjhy0wpmZ1V1/g/69WWVDfi/J9IXAmEq+0cCiTB/dJN3MzIZQf4P+RcDR2X00cGElfaqkTSXtSrlge21WAS2XtE/etXNUZRgzMxsiwztlkHQ+sB+wnaSFwEeA04BZko4B7gIOA4iIeZJmATcDTwDHRcTKHNWxlDuBRgCX5sfMzIZQx6AfEYe36LV/i/wzgBlN0nuAPftUOjMzG1R+ItfMrEYc9M3MasRB38ysRhz0zcxqxEHfzKxGHPTNzGrEQd/MrEYc9M3MasRB38ysRhz0zcxqxEHfzKxGHPTNzGrEQd/MrEYc9M3MasRB38ysRhz0zcxqxEHfzKxGBhT0Jd0h6UZJcyX1ZNq2kmZLui2/t6nkP0nSfEm3SjpwoIU3M7O+GYwj/X+NiAkRMTF/nwhcFhHjgMvyN5LGA1OBPYDJwJmShg3C9M3MrEtro3pnCnBudp8LHFJJvyAiHo2I24H5wKS1MH0zM2thoEE/gJ9Luk7StEzbISIWA+T39pk+ClhQGXZhppmZ2RAZPsDh942IRZK2B2ZL+mObvGqSFk0zlj+QaQA777zzAItoZma9BnSkHxGL8nsJ8ENKdc29knYEyO8lmX0hMKYy+GhgUYvxnh0REyNi4siRIwdSRDMzq+h30Je0uaQte7uBVwE3ARcBR2e2o4ELs/siYKqkTSXtCowDru3v9M3MrO8GUr2zA/BDSb3j+U5E/FTS74BZko4B7gIOA4iIeZJmATcDTwDHRcTKAZXezMz6pN9BPyL+AjyvSfr9wP4thpkBzOjvNM3MbGD8RK6ZWY046JuZ1YiDvplZjTjom5nViIO+mVmNOOibmdWIg76ZWY046JuZ1YiDvplZjTjom5nViIO+mVmNOOibmdWIg76ZWY046JuZ1YiDvplZjTjom5nViIO+mVmNDHnQlzRZ0q2S5ks6cainb2ZWZ0Ma9CUNA74EvBoYDxwuafxQlsHMrM6G+kh/EjA/Iv4SEY8BFwBThrgMZma1NdRBfxSwoPJ7YaaZmdkQGD7E01OTtFgjkzQNmJY/V0i6da2W6qlhO+C+dV2IgWq2AZgNsg1iX+GIAe8tuzRLHOqgvxAYU/k9GljUmCkizgbOHqpCPRVI6omIieu6HGbrO+8r7Q119c7vgHGSdpW0CTAVuGiIy2BmVltDeqQfEU9IOh74GTAMOCci5g1lGczM6myoq3eIiEuAS4Z6uhsAV3eZdcf7ShuKWOM6qpmZbaDcDIOZWY046HdB0n9JmifpBklzJe2d6V/r7xPFksZKuqnLvM/I6c6VdI+kuyu/N+nP9FtMZ4KkgwZrfGbtSFqZ2/BNkn4saesO+Q9Zm0/w12X7d9DvQNKLgNcCL4iI5wIHkA+YRcQ7IuLmtV2GiLg/IiZExATgy8Dpvb/zyebBMgHY4Dd6W288ktvwnsADwHEd8h9Cab5lbZlADbZ/B/3OdgTui4hHASLivohYBCBpjqSJ2b1C0gxJ10u6WtIOmb5b/v6dpI9JWtE4AUnDJH0689wg6Z2dCiVpe0nXZffzJIWknfP3nyU9TdJISd/P8f5O0r7Zf3NJ52TaHyRNyTOGjwFvyqOvN0l6eeWM4g+SthyUJWq2pqvIp/Nzn/mppOsk/UbSsyW9GDgY+HTv2ba3/36KCH/afIAtgLnAn4AzgZdX+s0BJmZ3AK/L7k8Bp2T3xcDh2f0uYEV2jwVuyu5plfybAj3Ari3KMx34QHbPA7YCjqc8A3EE5Sm8q7L/d4CXZPfOwC3Z/UngyOzeOudtc+CtwBcr0/oxsG9lOQxf1+vDnw3nU9kXhgHfAybn78uAcdm9N/DL7J4JHFoZ3tt/Pz5DfsvmU01ErJC0F/BS4F+B70o6MSJmNmR9jBLgAa4DXpndL6KclkLZCD/TZDKvAp4r6dD8/XRgHHB7h+JdCewLvIyyIU+mtHTwm+x/ADBe+sfj3Fvl0cqrgIMlfSDTN6PsFI1+C3xO0reBH0TEwg7lMeuLEZLmUg6ArgNmS9oCeDHwvcp2u2mL4b3994ODfhciYiXlqH6OpBuBoylHHVWPRx4SACvp27IV8O6I+Fkfi/Ybyp/RLsCFwIcoZxy9fz4bAS+KiEdWm1jZC94YEbc2pO9d/R0Rp0n6CaWe82pJB0TEH/tYRrNWHomICZKeTtlmj6PsV8uiXL/qxNt/P7hOvwNJz5I0rpI0AbizD6O4Gnhjdk9tkednwLGSNs5pPlPS5l2M+9fAkcBtEfEk5WLYQZQjFICfU059yfFOqEzv3bnxI+n5mb4c2LKSf7eIuDEi/odS5fTsLspk1icR8RDwHuADwCPA7ZIOgxKgJT0vs662feLtv18c9DvbAjhX0s2SbqDcPTC9D8OfALxP0rWUi8IPNcnzNeBm4Pcqt3F+hS7OFCLijuz8dX5fQTlKejB/vweYmBeHb6ZcUwD4OLAxcENO7+OZ/ivK6fBcSW8CTlC5ne56ys54aZfzbNYnEfEH4HrKgdERwDG53c1j1Ts3LgD+My+q7ubtv3/8RO5aJulplNPYkDSVclHXL44xs3XCdfpr317AF/NUchnw9nVbHDOrMx/pm5nViOv0zcxqxEHfzKxGHPTNzGrEQd/MrEYc9M3MasRB38ysRv4/MwyPC2MXUh8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "retweets_ratio = retweets_ct / (retweets_ct + single_tweets_ct)\n",
    "tweets_ratio_plot = [single_tweets_ct, retweets_ct]\n",
    "x_labels = ['Single Tweets', 'Retweets']\n",
    "plt.bar(x=x_labels, height=tweets_ratio_plot, width = 0.5, color=['#00ACEE','orange'])\n",
    "plt.suptitle('Comparison of Single Tweet Counts and Retweet Counts')\n",
    "plt.title('(\"Initital\" tweets not included in Single Tweet Count)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label</th>\n",
       "      <th>retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin can not wait for #ipad 2 also. the...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>need to buy an ipad2 while i'm in austin at #s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rt @laurieshook: i'm looking forward to the #s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>the best!  rt @mention ha! first in line for #...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{link} rt @mention 1st stop on the #sxsw #chao...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text  label  retweet\n",
       "2   @swonderlin can not wait for #ipad 2 also. the...      1        0\n",
       "20  need to buy an ipad2 while i'm in austin at #s...      1        0\n",
       "25  rt @laurieshook: i'm looking forward to the #s...      1        1\n",
       "36  the best!  rt @mention ha! first in line for #...      1        1\n",
       "57  {link} rt @mention 1st stop on the #sxsw #chao...      1        1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    2874\n",
       "1    2136\n",
       "0     418\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweets['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "y = tweets['label']\n",
    "X = tweets['tweet_text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "baseline = imbpipeline(steps=[\n",
    "    ('preproc', CountVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer)),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213, max_depth = 5))\n",
    "])\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "print(\"Training Score:\", baseline.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(baseline, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making final dataset\n",
    "tweets['capital_letter_ratio'] = tweets['tweet_text'].apply(capital_letter_ratio)\n",
    "tweets['exc_que_count'] = tweets['tweet_text'].apply(exc_que_count)\n",
    "tweets['what_product'] = tweets['tweet_text'].apply(what_product)\n",
    "#tweets = tweets.drop('word_count', axis = 1)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "y = tweets['label']\n",
    "X = tweets.drop('label', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 213)\n",
    "\n",
    "col_labels = list(X_train.columns)\n",
    "col_labels.remove('tweet_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB w/ cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', CountVectorizer(encoding = 'iso-8859-1', lowercase = True, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "apple_cv = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "apple_cv.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_cv.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_cv, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB w/ TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "apple = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    #('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "apple.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Creates parameters to test\n",
    "params = {\n",
    "    'preproc__tfidf__max_features': [None, 4000, 8000, 10000],\n",
    "    'preproc__tfidf__max_df': [.4, .6, .8],\n",
    "    'preproc__tfidf__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(apple, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "apple_tuned = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    #('smote', SMOTE(sampling_strategy = 'not majority', random_state=11)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "apple_tuned.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_tuned.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_tuned, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "apple_dt = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(random_state = 213))\n",
    "])\n",
    "\n",
    "apple_dt.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_dt.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_dt, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Creates parameters to test\n",
    "params = {\n",
    "    'dtc__max_depth': [None, 5, 20],\n",
    "    'dtc__criterion': ['gini', 'entropy'],\n",
    "    'dtc__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Fits gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(apple_dt, param_grid = params, scoring = 'accuracy', cv = 3)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vec = ColumnTransformer([\n",
    "    ('cv', TfidfVectorizer(encoding = 'iso-8859-1', lowercase = False, tokenizer = tweet_tokenizer), 'tweet_text'),\n",
    "    ('ohe', OneHotEncoder(), ['what_product'])],\n",
    "    #('scale', MinMaxScaler(), col_labels)],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "apple_dt = imbpipeline(steps=[\n",
    "    ('preproc', count_vec),\n",
    "    ('smote', SMOTE(sampling_strategy = 'minority', random_state=11)),\n",
    "    ('dtc', DecisionTreeClassifier(min_samples_split = 5, random_state = 213))\n",
    "])\n",
    "\n",
    "apple_dt.fit(X_train, y_train)\n",
    "print(\"Training Score:\", apple_dt.score(X_train, y_train))\n",
    "scores = np.mean(cross_val_score(apple_dt, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
